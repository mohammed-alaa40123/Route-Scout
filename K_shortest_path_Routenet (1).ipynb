{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkBVUFXx482w",
        "outputId": "a70c142a-e5d7-4031-d9a4-04b457d78d7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxUrrCSsf-k0"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4377fbef",
        "outputId": "c4ccb8c9-d25f-45da-ed2f-95efe7a85de8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-12-17 22:54:41--  https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
            "Resolving repo.anaconda.com (repo.anaconda.com)... 104.16.32.241, 104.16.191.158, 2606:4700::6810:bf9e, ...\n",
            "Connecting to repo.anaconda.com (repo.anaconda.com)|104.16.32.241|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 156772981 (150M) [application/octet-stream]\n",
            "Saving to: ‘miniconda.sh’\n",
            "\n",
            "miniconda.sh        100%[===================>] 149.51M   137MB/s    in 1.1s    \n",
            "\n",
            "2025-12-17 22:54:42 (137 MB/s) - ‘miniconda.sh’ saved [156772981/156772981]\n",
            "\n",
            "PREFIX=/opt/conda\n",
            "Unpacking bootstrapper...\n",
            "Unpacking payload...\n",
            "\n",
            "Installing base environment...\n",
            "\n",
            "Preparing transaction: ...working... done\n",
            "Executing transaction: ...working... done\n",
            "installation finished.\n",
            "WARNING:\n",
            "    You currently have a PYTHONPATH environment variable set. This may cause\n",
            "    unexpected behavior when running the Python interpreter in Miniconda3.\n",
            "    For best results, please verify that your PYTHONPATH only points to\n",
            "    directories of packages that are compatible with the Python interpreter\n",
            "    in Miniconda3: /opt/conda\n"
          ]
        }
      ],
      "source": [
        "!wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda.sh\n",
        "!bash miniconda.sh -b -p /opt/conda\n",
        "!rm miniconda.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "7f35aebe",
        "outputId": "618e84ec-1446-4b56-dbfc-399094e0efc5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accepted Terms of Service for \u001b[4;94mhttps://repo.anaconda.com/pkgs/main\u001b[0m\n",
            "accepted Terms of Service for \u001b[4;94mhttps://repo.anaconda.com/pkgs/r\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!source /opt/conda/etc/profile.d/conda.sh && conda tos accept --override-channels --channel https://repo.anaconda.com/pkgs/main\n",
        "!source /opt/conda/etc/profile.d/conda.sh && conda tos accept --override-channels --channel https://repo.anaconda.com/pkgs/r"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "537eea68",
        "outputId": "213491d8-4192-4f15-ab99-824bdaa9b2ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;33mJupyter detected\u001b[0m\u001b[1;33m...\u001b[0m\n",
            "\u001b[1;32m2\u001b[0m\u001b[1;32m channel Terms of Service accepted\u001b[0m\n",
            "Retrieving notices: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "Channels:\n",
            " - defaults\n",
            "Platform: linux-64\n",
            "Collecting package metadata (repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Solving environment: / \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /opt/conda/envs/tf_py37\n",
            "\n",
            "  added / updated specs:\n",
            "    - python=3.9\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    libnsl-2.0.0               |       h5eee18b_0          31 KB\n",
            "    python-3.9.25              |       h0dcde21_1        23.0 MB\n",
            "    setuptools-80.9.0          |   py39h06a4308_0         1.4 MB\n",
            "    wheel-0.45.1               |   py39h06a4308_0         114 KB\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:        24.6 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  _libgcc_mutex      pkgs/main/linux-64::_libgcc_mutex-0.1-main \n",
            "  _openmp_mutex      pkgs/main/linux-64::_openmp_mutex-5.1-1_gnu \n",
            "  bzip2              pkgs/main/linux-64::bzip2-1.0.8-h5eee18b_6 \n",
            "  ca-certificates    pkgs/main/linux-64::ca-certificates-2025.12.2-h06a4308_0 \n",
            "  expat              pkgs/main/linux-64::expat-2.7.3-h7354ed3_4 \n",
            "  ld_impl_linux-64   pkgs/main/linux-64::ld_impl_linux-64-2.44-h153f514_2 \n",
            "  libexpat           pkgs/main/linux-64::libexpat-2.7.3-h7354ed3_4 \n",
            "  libffi             pkgs/main/linux-64::libffi-3.4.4-h6a678d5_1 \n",
            "  libgcc             pkgs/main/linux-64::libgcc-15.2.0-h69a1729_7 \n",
            "  libgcc-ng          pkgs/main/linux-64::libgcc-ng-15.2.0-h166f726_7 \n",
            "  libgomp            pkgs/main/linux-64::libgomp-15.2.0-h4751f2c_7 \n",
            "  libnsl             pkgs/main/linux-64::libnsl-2.0.0-h5eee18b_0 \n",
            "  libstdcxx          pkgs/main/linux-64::libstdcxx-15.2.0-h39759b7_7 \n",
            "  libstdcxx-ng       pkgs/main/linux-64::libstdcxx-ng-15.2.0-hc03a8fd_7 \n",
            "  libuuid            pkgs/main/linux-64::libuuid-1.41.5-h5eee18b_0 \n",
            "  libxcb             pkgs/main/linux-64::libxcb-1.17.0-h9b100fa_0 \n",
            "  libzlib            pkgs/main/linux-64::libzlib-1.3.1-hb25bd0a_0 \n",
            "  ncurses            pkgs/main/linux-64::ncurses-6.5-h7934f7d_0 \n",
            "  openssl            pkgs/main/linux-64::openssl-3.0.18-hd6dcaed_0 \n",
            "  pip                pkgs/main/noarch::pip-25.3-pyhc872135_0 \n",
            "  pthread-stubs      pkgs/main/linux-64::pthread-stubs-0.3-h0ce48e5_1 \n",
            "  python             pkgs/main/linux-64::python-3.9.25-h0dcde21_1 \n",
            "  readline           pkgs/main/linux-64::readline-8.3-hc2a1206_0 \n",
            "  setuptools         pkgs/main/linux-64::setuptools-80.9.0-py39h06a4308_0 \n",
            "  sqlite             pkgs/main/linux-64::sqlite-3.51.0-h2a70700_0 \n",
            "  tk                 pkgs/main/linux-64::tk-8.6.15-h54e0aa7_0 \n",
            "  tzdata             pkgs/main/noarch::tzdata-2025b-h04d1e81_0 \n",
            "  wheel              pkgs/main/linux-64::wheel-0.45.1-py39h06a4308_0 \n",
            "  xorg-libx11        pkgs/main/linux-64::xorg-libx11-1.8.12-h9b100fa_1 \n",
            "  xorg-libxau        pkgs/main/linux-64::xorg-libxau-1.0.12-h9b100fa_0 \n",
            "  xorg-libxdmcp      pkgs/main/linux-64::xorg-libxdmcp-1.1.5-h9b100fa_0 \n",
            "  xorg-xorgproto     pkgs/main/linux-64::xorg-xorgproto-2024.1-h5eee18b_1 \n",
            "  xz                 pkgs/main/linux-64::xz-5.6.4-h5eee18b_1 \n",
            "  zlib               pkgs/main/linux-64::zlib-1.3.1-hb25bd0a_0 \n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages:\n",
            "python-3.9.25        | 23.0 MB   | :   0% 0/1 [00:00<?, ?it/s]\n",
            "setuptools-80.9.0    | 1.4 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "wheel-0.45.1         | 114 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "libnsl-2.0.0         | 31 KB     | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "python-3.9.25        | 23.0 MB   | :   0% 0.0006784110506800052/1 [00:00<04:51, 291.30s/it]\n",
            "setuptools-80.9.0    | 1.4 MB    | :   1% 0.011036459926145175/1 [00:00<00:18, 19.08s/it]\u001b[A\n",
            "\n",
            "\n",
            "libnsl-2.0.0         | 31 KB     | :  52% 0.515966492410405/1 [00:00<00:00,  2.39it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "wheel-0.45.1         | 114 KB    | : 100% 1.0/1 [00:00<00:00,  1.33s/it]                \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "libnsl-2.0.0         | 31 KB     | : 100% 1.0/1 [00:00<00:00,  2.39it/s]              \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "libnsl-2.0.0         | 31 KB     | : 100% 1.0/1 [00:00<00:00,  2.39it/s]\u001b[A\u001b[A\u001b[A\n",
            "setuptools-80.9.0    | 1.4 MB    | : 100% 1.0/1 [00:00<00:00, 19.08s/it]                 \u001b[A\n",
            "\n",
            "wheel-0.45.1         | 114 KB    | : 100% 1.0/1 [00:00<00:00,  4.00it/s]\u001b[A\u001b[A\n",
            "\n",
            "python-3.9.25        | 23.0 MB   | : 100% 1.0/1 [00:00<00:00,  2.00it/s]               \n",
            "setuptools-80.9.0    | 1.4 MB    | : 100% 1.0/1 [00:00<00:00,  1.51it/s]\u001b[A\n",
            "                                                                        \n",
            "                                                                        \u001b[A\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\n",
            "Preparing transaction: \\ \b\b| \b\bdone\n",
            "Verifying transaction: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Executing transaction: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "#\n",
            "# To activate this environment, use\n",
            "#\n",
            "#     $ conda activate tf_py37\n",
            "#\n",
            "# To deactivate an active environment, use\n",
            "#\n",
            "#     $ conda deactivate\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!source /opt/conda/etc/profile.d/conda.sh && conda create -n tf_py37 python=3.9 -y\n",
        "!source /opt/conda/etc/profile.d/conda.sh && conda activate tf_py37"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9HsSzf_t4FnN",
        "outputId": "cd30e9e9-470f-46a8-a29a-e0f1ed0e10ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==2.6.*\n",
            "  Downloading tensorflow-2.6.5-cp39-cp39-manylinux2010_x86_64.whl.metadata (2.8 kB)\n",
            "Collecting numpy~=1.19.2 (from tensorflow==2.6.*)\n",
            "  Downloading numpy-1.19.5-cp39-cp39-manylinux2010_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting absl-py~=0.10 (from tensorflow==2.6.*)\n",
            "  Downloading absl_py-0.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting astunparse~=1.6.3 (from tensorflow==2.6.*)\n",
            "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting clang~=5.0 (from tensorflow==2.6.*)\n",
            "  Downloading clang-5.0.tar.gz (30 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting flatbuffers~=1.12.0 (from tensorflow==2.6.*)\n",
            "  Downloading flatbuffers-1.12-py2.py3-none-any.whl.metadata (872 bytes)\n",
            "Collecting google-pasta~=0.2 (from tensorflow==2.6.*)\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
            "Collecting h5py~=3.1.0 (from tensorflow==2.6.*)\n",
            "  Downloading h5py-3.1.0-cp39-cp39-manylinux1_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting keras-preprocessing~=1.1.2 (from tensorflow==2.6.*)\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting opt-einsum~=3.3.0 (from tensorflow==2.6.*)\n",
            "  Downloading opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting protobuf<3.20,>=3.9.2 (from tensorflow==2.6.*)\n",
            "  Downloading protobuf-3.19.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (787 bytes)\n",
            "Collecting six~=1.15.0 (from tensorflow==2.6.*)\n",
            "  Downloading six-1.15.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting termcolor~=1.1.0 (from tensorflow==2.6.*)\n",
            "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting typing-extensions<3.11,>=3.7 (from tensorflow==2.6.*)\n",
            "  Downloading typing_extensions-3.10.0.2-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: wheel~=0.35 in /opt/conda/envs/tf_py37/lib/python3.9/site-packages (from tensorflow==2.6.*) (0.45.1)\n",
            "Collecting wrapt~=1.12.1 (from tensorflow==2.6.*)\n",
            "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gast==0.4.0 (from tensorflow==2.6.*)\n",
            "  Downloading gast-0.4.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting tensorboard<2.7,>=2.6.0 (from tensorflow==2.6.*)\n",
            "  Downloading tensorboard-2.6.0-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting tensorflow-estimator<2.7,>=2.6.0 (from tensorflow==2.6.*)\n",
            "  Downloading tensorflow_estimator-2.6.0-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting keras<2.7,>=2.6.0 (from tensorflow==2.6.*)\n",
            "  Downloading keras-2.6.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting grpcio<2.0,>=1.37.0 (from tensorflow==2.6.*)\n",
            "  Downloading grpcio-1.76.0-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.7 kB)\n",
            "INFO: pip is looking at multiple versions of grpcio to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading grpcio-1.75.1-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.7 kB)\n",
            "  Downloading grpcio-1.75.0-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.7 kB)\n",
            "  Downloading grpcio-1.74.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting google-auth<2,>=1.6.3 (from tensorboard<2.7,>=2.6.0->tensorflow==2.6.*)\n",
            "  Downloading google_auth-1.35.0-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.7,>=2.6.0->tensorflow==2.6.*)\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting markdown>=2.6.8 (from tensorboard<2.7,>=2.6.0->tensorflow==2.6.*)\n",
            "  Downloading markdown-3.9-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting requests<3,>=2.21.0 (from tensorboard<2.7,>=2.6.0->tensorflow==2.6.*)\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/envs/tf_py37/lib/python3.9/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow==2.6.*) (80.9.0)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.7,>=2.6.0->tensorflow==2.6.*)\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.7,>=2.6.0->tensorflow==2.6.*)\n",
            "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl.metadata (873 bytes)\n",
            "Collecting werkzeug>=0.11.15 (from tensorboard<2.7,>=2.6.0->tensorflow==2.6.*)\n",
            "  Downloading werkzeug-3.1.4-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow==2.6.*)\n",
            "  Downloading cachetools-4.2.4-py3-none-any.whl.metadata (4.8 kB)\n",
            "Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow==2.6.*)\n",
            "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting rsa<5,>=3.1.4 (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow==2.6.*)\n",
            "  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow==2.6.*)\n",
            "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
            "Collecting charset_normalizer<4,>=2 (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow==2.6.*)\n",
            "  Downloading charset_normalizer-3.4.4-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (37 kB)\n",
            "Collecting idna<4,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow==2.6.*)\n",
            "  Downloading idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow==2.6.*)\n",
            "  Downloading urllib3-2.6.2-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow==2.6.*)\n",
            "  Downloading certifi-2025.11.12-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting pyasn1>=0.1.3 (from rsa<5,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow==2.6.*)\n",
            "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting importlib-metadata>=4.4 (from markdown>=2.6.8->tensorboard<2.7,>=2.6.0->tensorflow==2.6.*)\n",
            "  Downloading importlib_metadata-8.7.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Collecting zipp>=3.20 (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.7,>=2.6.0->tensorflow==2.6.*)\n",
            "  Downloading zipp-3.23.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow==2.6.*)\n",
            "  Downloading oauthlib-3.3.1-py3-none-any.whl.metadata (7.9 kB)\n",
            "Collecting markupsafe>=2.1.1 (from werkzeug>=0.11.15->tensorboard<2.7,>=2.6.0->tensorflow==2.6.*)\n",
            "  Downloading markupsafe-3.0.3-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.7 kB)\n",
            "Downloading tensorflow-2.6.5-cp39-cp39-manylinux2010_x86_64.whl (464.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m464.3/464.3 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m  \u001b[33m0:00:26\u001b[0m\n",
            "\u001b[?25hDownloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Downloading absl_py-0.15.0-py3-none-any.whl (132 kB)\n",
            "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
            "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "Downloading grpcio-1.74.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m106.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h5py-3.1.0-cp39-cp39-manylinux1_x86_64.whl (4.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m104.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-2.6.0-py2.py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "Downloading numpy-1.19.5-cp39-cp39-manylinux2010_x86_64.whl (14.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.9/14.9 MB\u001b[0m \u001b[31m146.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
            "Downloading protobuf-3.19.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading tensorboard-2.6.0-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m84.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n",
            "Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
            "Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "Downloading charset_normalizer-3.4.4-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (153 kB)\n",
            "Downloading idna-3.11-py3-none-any.whl (71 kB)\n",
            "Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
            "Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m117.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.6.0-py2.py3-none-any.whl (462 kB)\n",
            "Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\n",
            "Downloading urllib3-2.6.2-py3-none-any.whl (131 kB)\n",
            "Downloading certifi-2025.11.12-py3-none-any.whl (159 kB)\n",
            "Downloading markdown-3.9-py3-none-any.whl (107 kB)\n",
            "Downloading importlib_metadata-8.7.0-py3-none-any.whl (27 kB)\n",
            "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
            "Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
            "Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
            "Downloading oauthlib-3.3.1-py3-none-any.whl (160 kB)\n",
            "Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading werkzeug-3.1.4-py3-none-any.whl (224 kB)\n",
            "Downloading markupsafe-3.0.3-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (20 kB)\n",
            "Downloading zipp-3.23.0-py3-none-any.whl (10 kB)\n",
            "Building wheels for collected packages: clang, termcolor, wrapt\n",
            "  Building wheel for clang (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clang: filename=clang-5.0-py3-none-any.whl size=30732 sha256=b06ba39b296bbda798d1c2f8c5f1db5770026a079ee43ea7130f4e38decab3bc\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/ce/7a/27094f689461801c934296d07078773603663dfcaca63bb064\n",
            "  Building wheel for termcolor (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4902 sha256=090b9c968df220447cba75be0b98b41dffbf697c07d677dd277c3f5ddcc6bbab\n",
            "  Stored in directory: /root/.cache/pip/wheels/b6/0d/90/0d1bbd99855f99cb2f6c2e5ff96f8023fad8ec367695f7d72d\n",
            "  Building wheel for wrapt (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wrapt: filename=wrapt-1.12.1-cp39-cp39-linux_x86_64.whl size=36920 sha256=2cf0c08f5d53e43bfad2fe162a027c672706e01a05bd28ab878c3c2fa2a0409c\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/23/68/efe259aaca055e93b08e74fbe512819c69a2155c11ba3c0f10\n",
            "Successfully built clang termcolor wrapt\n",
            "Installing collected packages: wrapt, typing-extensions, termcolor, tensorflow-estimator, tensorboard-plugin-wit, keras, flatbuffers, clang, zipp, urllib3, tensorboard-data-server, six, pyasn1, protobuf, oauthlib, numpy, markupsafe, idna, grpcio, gast, charset_normalizer, certifi, cachetools, werkzeug, rsa, requests, pyasn1-modules, opt-einsum, keras-preprocessing, importlib-metadata, h5py, google-pasta, astunparse, absl-py, requests-oauthlib, markdown, google-auth, google-auth-oauthlib, tensorboard, tensorflow\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40/40\u001b[0m [tensorflow]\n",
            "\u001b[1A\u001b[2KSuccessfully installed absl-py-0.15.0 astunparse-1.6.3 cachetools-4.2.4 certifi-2025.11.12 charset_normalizer-3.4.4 clang-5.0 flatbuffers-1.12 gast-0.4.0 google-auth-1.35.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.74.0 h5py-3.1.0 idna-3.11 importlib-metadata-8.7.0 keras-2.6.0 keras-preprocessing-1.1.2 markdown-3.9 markupsafe-3.0.3 numpy-1.19.5 oauthlib-3.3.1 opt-einsum-3.3.0 protobuf-3.19.6 pyasn1-0.6.1 pyasn1-modules-0.4.2 requests-2.32.5 requests-oauthlib-2.0.0 rsa-4.9.1 six-1.15.0 tensorboard-2.6.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.6.5 tensorflow-estimator-2.6.0 termcolor-1.1.0 typing-extensions-3.10.0.2 urllib3-2.6.2 werkzeug-3.1.4 wrapt-1.12.1 zipp-3.23.0\n"
          ]
        }
      ],
      "source": [
        "!/opt/conda/envs/tf_py37/bin/pip install tensorflow==2.6.*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amhPc-Pb2g0n",
        "outputId": "911f7211-bc90-4dec-e361-af77242129bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Routenet\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/Routenet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2K2RjULGc_EW"
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/BNN-UPC/RouteNet-Fermi.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8MqB62a-7eT"
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/BNN-UPC/RouteNet-Erlang.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "No-umyOfdBv8",
        "outputId": "ff9af6de-2364-4db5-c253-e4e5186e8477"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Routenet/RouteNet-Fermi\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/Routenet/RouteNet-Fermi/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "En2I5nCqd8ZI",
        "outputId": "0f29b2ed-f287-4706-e76a-c5d5a507ed74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==2.6.* in /opt/conda/envs/tf_py37/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (2.6.5)\n",
            "Collecting networkx==2.6.* (from -r requirements.txt (line 2))\n",
            "  Downloading networkx-2.6.3-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting pandas==1.1.* (from -r requirements.txt (line 3))\n",
            "  Downloading pandas-1.1.5-cp39-cp39-manylinux1_x86_64.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /opt/conda/envs/tf_py37/lib/python3.9/site-packages (from tensorflow==2.6.*->-r requirements.txt (line 1)) (1.19.5)\n",
            "Requirement already satisfied: absl-py~=0.10 in /opt/conda/envs/tf_py37/lib/python3.9/site-packages (from tensorflow==2.6.*->-r requirements.txt (line 1)) (0.15.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /opt/conda/envs/tf_py37/lib/python3.9/site-packages (from tensorflow==2.6.*->-r requirements.txt (line 1)) (1.6.3)\n",
            "Requirement already satisfied: clang~=5.0 in /opt/conda/envs/tf_py37/lib/python3.9/site-packages (from tensorflow==2.6.*->-r requirements.txt (line 1)) (5.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /opt/conda/envs/tf_py37/lib/python3.9/site-packages (from tensorflow==2.6.*->-r requirements.txt (line 1)) (1.12)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /opt/conda/envs/tf_py37/lib/python3.9/site-packages (from tensorflow==2.6.*->-r requirements.txt (line 1)) (0.2.0)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /opt/conda/envs/tf_py37/lib/python3.9/site-packages (from tensorflow==2.6.*->-r requirements.txt (line 1)) (3.1.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /opt/conda/envs/tf_py37/lib/python3.9/site-packages (from tensorflow==2.6.*->-r requirements.txt (line 1)) (1.1.2)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /opt/conda/envs/tf_py37/lib/python3.9/site-packages (from tensorflow==2.6.*->-r requirements.txt (line 1)) (3.3.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /opt/conda/envs/tf_py37/lib/python3.9/site-packages (from tensorflow==2.6.*->-r requirements.txt (line 1)) (3.19.6)\n",
            "Requirement already satisfied: six~=1.15.0 in /opt/conda/envs/tf_py37/lib/python3.9/site-packages (from tensorflow==2.6.*->-r requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /opt/conda/envs/tf_py37/lib/python3.9/site-packages (from tensorflow==2.6.*->-r requirements.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: typing-extensions<3.11,>=3.7 in /opt/conda/envs/tf_py37/lib/python3.9/site-packages (from tensorflow==2.6.*->-r requirements.txt (line 1)) (3.10.0.2)\n",
            "Requirement already satisfied: wheel~=0.35 in /opt/conda/envs/tf_py37/lib/python3.9/site-packages (from tensorflow==2.6.*->-r requirements.txt (line 1)) (0.45.1)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /opt/conda/envs/tf_py37/lib/python3.9/site-packages (from tensorflow==2.6.*->-r requirements.txt (line 1)) (1.12.1)\n",
            "Requirement already satisfied: gast==0.4.0 in /opt/conda/envs/tf_py37/lib/python3.9/site-packages (from tensorflow==2.6.*->-r requirements.txt (line 1)) (0.4.0)\n",
            "Requirement already satisfied: tensorboard<2.7,>=2.6.0 in /opt/conda/envs/tf_py37/lib/python3.9/site-packages (from tensorflow==2.6.*->-r requirements.txt (line 1)) (2.6.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.7,>=2.6.0 in /opt/conda/envs/tf_py37/lib/python3.9/site-packages (from tensorflow==2.6.*->-r requirements.txt (line 1)) (2.6.0)\n",
            "Requirement already satisfied: keras<2.7,>=2.6.0 in /opt/conda/envs/tf_py37/lib/python3.9/site-packages (from tensorflow==2.6.*->-r requirements.txt (line 1)) (2.6.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.37.0 in /opt/conda/envs/tf_py37/lib/python3.9/site-packages (from tensorflow==2.6.*->-r requirements.txt (line 1)) (1.74.0)\n",
            "Collecting python-dateutil>=2.7.3 (from pandas==1.1.*->-r requirements.txt (line 3))\n",
            "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting pytz>=2017.2 (from pandas==1.1.*->-r requirements.txt (line 3))\n",
            "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/envs/tf_py37/lib/python3.9/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow==2.6.*->-r requirements.txt (line 1)) (1.35.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/envs/tf_py37/lib/python3.9/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow==2.6.*->-r requirements.txt (line 1)) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/envs/tf_py37/lib/python3.9/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow==2.6.*->-r requirements.txt (line 1)) (3.9)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/envs/tf_py37/lib/python3.9/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow==2.6.*->-r requirements.txt (line 1)) (2.32.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/envs/tf_py37/lib/python3.9/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow==2.6.*->-r requirements.txt (line 1)) (80.9.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/envs/tf_py37/lib/python3.9/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow==2.6.*->-r requirements.txt (line 1)) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/envs/tf_py37/lib/python3.9/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow==2.6.*->-r requirements.txt (line 1)) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/envs/tf_py37/lib/python3.9/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow==2.6.*->-r requirements.txt (line 1)) (3.1.4)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/envs/tf_py37/lib/python3.9/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow==2.6.*->-r requirements.txt (line 1)) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/envs/tf_py37/lib/python3.9/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow==2.6.*->-r requirements.txt (line 1)) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/envs/tf_py37/lib/python3.9/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow==2.6.*->-r requirements.txt (line 1)) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/envs/tf_py37/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow==2.6.*->-r requirements.txt (line 1)) (2.0.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/envs/tf_py37/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow==2.6.*->-r requirements.txt (line 1)) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/tf_py37/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow==2.6.*->-r requirements.txt (line 1)) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/tf_py37/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow==2.6.*->-r requirements.txt (line 1)) (2.6.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/tf_py37/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow==2.6.*->-r requirements.txt (line 1)) (2025.11.12)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /opt/conda/envs/tf_py37/lib/python3.9/site-packages (from rsa<5,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow==2.6.*->-r requirements.txt (line 1)) (0.6.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/envs/tf_py37/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.7,>=2.6.0->tensorflow==2.6.*->-r requirements.txt (line 1)) (8.7.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /opt/conda/envs/tf_py37/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.7,>=2.6.0->tensorflow==2.6.*->-r requirements.txt (line 1)) (3.23.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/envs/tf_py37/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow==2.6.*->-r requirements.txt (line 1)) (3.3.1)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /opt/conda/envs/tf_py37/lib/python3.9/site-packages (from werkzeug>=0.11.15->tensorboard<2.7,>=2.6.0->tensorflow==2.6.*->-r requirements.txt (line 1)) (3.0.3)\n",
            "Downloading networkx-2.6.3-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-1.1.5-cp39-cp39-manylinux1_x86_64.whl (9.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
            "Installing collected packages: pytz, python-dateutil, networkx, pandas\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [pandas]\n",
            "\u001b[1A\u001b[2KSuccessfully installed networkx-2.6.3 pandas-1.1.5 python-dateutil-2.9.0.post0 pytz-2025.2\n"
          ]
        }
      ],
      "source": [
        "!/opt/conda/envs/tf_py37/bin/pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3vrnP5N6Dhg",
        "outputId": "4a42ede2-83a3-4a54-b78a-bc7a5e7c0bce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Routenet\n"
          ]
        }
      ],
      "source": [
        "cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ccnf4zSvd-CN"
      },
      "outputs": [],
      "source": [
        "# !wget -O traffic_models.zip https://bnn.upc.edu/download/dataset-v6-traffic-models/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfNYOHzyVsSX"
      },
      "outputs": [],
      "source": [
        "# !rm -rf /content/traffic_models.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sArZHCHg6L3S"
      },
      "outputs": [],
      "source": [
        "# !wget -O scheduling.zip https://bnn.upc.edu/download/dataset-v6-scheduling/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "RDBvLVEBjgUg"
      },
      "outputs": [],
      "source": [
        "# !unzip traffic_models.zip -d data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "rtMSP1UhVy2b"
      },
      "outputs": [],
      "source": [
        "# !unzip scheduling.zip -d data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VkGz4g6M33_z"
      },
      "outputs": [],
      "source": [
        "# mv data RouteNet-Fermi/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBqnYwWDEhO5"
      },
      "source": [
        "## write files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-dFXvGwgg1o",
        "outputId": "841c3f69-ad59-4749-c8e1-709ca60f6d4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Routenet\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive/Routenet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IA6SwjZftAMD",
        "outputId": "0fc9775a-ae2c-4384-b00d-d4aab8f55479"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mFailed to start the Kernel. \n",
            "\u001b[1;31mTypeError: __init__() got an unexpected keyword argument 'default'. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# @title\n",
        "%%writefile predict_unified_with_candidate_routes.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmlj2KuJb6FH",
        "outputId": "00d6fd96-4b35-4632-d068-e93ef98cbe7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting predict_unified_with_candidate_routes.py\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "%%writefile predict_unified_with_candidate_routes.py\n",
        "import os\n",
        "import sys\n",
        "import re\n",
        "import argparse\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import networkx as nx\n",
        "\n",
        "# Disable GPU\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
        "\n",
        "# This script lives ABOVE both repos\n",
        "TOP_BASE_DIR = os.path.dirname(os.path.abspath(__file__))\n",
        "ERLANG_DIR = os.path.join(TOP_BASE_DIR, \"RouteNet-Erlang\")\n",
        "FERMI_DIR = os.path.join(TOP_BASE_DIR, \"RouteNet-Fermi\")\n",
        "\n",
        "# =============================================================================\n",
        "# Shared helpers\n",
        "# =============================================================================\n",
        "\n",
        "def parse_candidate_routes(routes_file):\n",
        "    \"\"\"\n",
        "    Expect lines like:\n",
        "        1 (hops=4): 0->2->4->8->5\n",
        "        2 (hops=6): 0->2->4->8->7->6->5\n",
        "\n",
        "    Returns: [{'id': '1', 'src': 0, 'dst': 5, 'nodes': [0,2,4,8,5]}, ...]\n",
        "    \"\"\"\n",
        "    routes = []\n",
        "    with open(routes_file, \"r\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line or line.startswith(\"#\"):\n",
        "                continue\n",
        "            if \":\" not in line:\n",
        "                continue\n",
        "            left, right = line.split(\":\", 1)\n",
        "            left = left.strip()\n",
        "            right = right.strip()\n",
        "\n",
        "            cand_id = left.split()[0]\n",
        "\n",
        "            node_strs = [s.strip() for s in right.split(\"->\")]\n",
        "            nodes = [int(s) for s in node_strs if s != \"\"]\n",
        "            if len(nodes) < 2:\n",
        "                continue\n",
        "            src = nodes[0]\n",
        "            dst = nodes[-1]\n",
        "            routes.append({\"id\": cand_id, \"src\": src, \"dst\": dst, \"nodes\": nodes})\n",
        "    return routes\n",
        "\n",
        "\n",
        "def resolve_dataset_dir(framework, traffic_mode, split, topo):\n",
        "    \"\"\"\n",
        "    Map (framework, traffic_mode, split, topo) -> directory with tar.gz + graphs + routings.\n",
        "    Matches the actual folder names under data/traffic_models.\n",
        "    \"\"\"\n",
        "    if framework == \"erlang\":\n",
        "        root = os.path.join(ERLANG_DIR, \"data\", \"traffic_models\", traffic_mode)\n",
        "    else:\n",
        "        root = os.path.join(FERMI_DIR, \"data\", \"traffic_models\", traffic_mode)\n",
        "\n",
        "    split = split.lower()\n",
        "    topo = topo.lower()\n",
        "    traffic_mode = traffic_mode.lower()\n",
        "\n",
        "    # Map traffic_mode to suffix used in folder names\n",
        "    suffix_map = {\n",
        "        \"all_multiplexed\": \"multiplexed\",\n",
        "        \"autocorrelated\": \"autocorrelated\",\n",
        "        \"constant_bitrate\": \"constant\",\n",
        "        \"modulated\": \"modulated\",\n",
        "        \"onoff\": \"onoff\",\n",
        "    }\n",
        "\n",
        "    if traffic_mode not in suffix_map:\n",
        "        raise SystemExit(f\"Unknown traffic_mode '{traffic_mode}' in resolve_dataset_dir().\")\n",
        "\n",
        "    suffix = suffix_map[traffic_mode]\n",
        "\n",
        "    if split == \"test\":\n",
        "        # Test sets are always GBN\n",
        "        if topo != \"gbn\":\n",
        "            raise SystemExit(\"For split='test', topology must be 'gbn'.\")\n",
        "        # e.g. gbn-multiplexed, gbn-constant, gbn-autocorrelated, ...\n",
        "        return os.path.join(root, \"test\", f\"gbn-{suffix}\")\n",
        "    else:\n",
        "        # Train: geant2 / nsfnet\n",
        "        if topo not in (\"geant2\", \"nsfnet\"):\n",
        "            raise SystemExit(\"For split='train', topology must be 'geant2' or 'nsfnet'.\")\n",
        "        # e.g. geant2-multiplexed, geant2-constant, nsfnet-autocorrelated, ...\n",
        "        return os.path.join(root, \"train\", f\"{topo}-{suffix}\")\n",
        "\n",
        "\n",
        "\n",
        "def validate_candidate_path_for_sample(sample, cand_id, path_nodes, framework_tag, args):\n",
        "    \"\"\"\n",
        "    Checks that every hop (u,v) in path_nodes is a valid directed edge\n",
        "    in this sample's topology. If not, exit with a clear message.\n",
        "    \"\"\"\n",
        "    G_topo = sample.get_topology_object()\n",
        "    for u, v in zip(path_nodes, path_nodes[1:]):\n",
        "        if not G_topo.has_edge(u, v):\n",
        "            raise SystemExit(\n",
        "                f\"[{framework_tag}] Candidate {cand_id} has invalid hop {u}->{v} \"\n",
        "                f\"for this topology.\\n\"\n",
        "                f\"  dataset_split = {args.dataset_split}\\n\"\n",
        "                f\"  topology      = {args.topology}\\n\"\n",
        "                f\"  traffic_mode  = {args.traffic_mode}\\n\"\n",
        "                f\"  routes_file   = {args.routes_file}\\n\"\n",
        "                f\"Make sure the routes_file was generated for THIS topology.\"\n",
        "            )\n",
        "\n",
        "# =============================================================================\n",
        "# ERLANG BRANCH\n",
        "# =============================================================================\n",
        "\n",
        "def erlang_import_modules():\n",
        "    \"\"\"\n",
        "    Add RouteNet-Erlang/TrafficModels to sys.path and import Erlang modules.\n",
        "    \"\"\"\n",
        "    tm_dir = os.path.join(ERLANG_DIR, \"TrafficModels\")\n",
        "    if tm_dir not in sys.path:\n",
        "        sys.path.insert(0, tm_dir)\n",
        "\n",
        "    from datanetAPI import DatanetAPI\n",
        "    from read_dataset import network_to_hypergraph\n",
        "    from model import GNN_Model\n",
        "\n",
        "    return DatanetAPI, network_to_hypergraph, GNN_Model\n",
        "\n",
        "\n",
        "def erlang_build_graph_and_features(sample, label, network_to_hypergraph):\n",
        "    \"\"\"\n",
        "    Same logic as your working Erlang scripts: build hypergraph representation and features.\n",
        "    \"\"\"\n",
        "    HG = network_to_hypergraph(sample=sample)\n",
        "\n",
        "    n_p = 0\n",
        "    n_l = 0\n",
        "    mapping = {}\n",
        "    for entity in list(HG.nodes()):\n",
        "        if entity.startswith(\"p\"):\n",
        "            mapping[entity] = f\"p_{n_p}\"\n",
        "            n_p += 1\n",
        "        elif entity.startswith(\"l\"):\n",
        "            mapping[entity] = f\"l_{n_l}\"\n",
        "            n_l += 1\n",
        "\n",
        "    D_G = nx.relabel_nodes(HG, mapping)\n",
        "\n",
        "    # Incidence arrays\n",
        "    link_to_path = []\n",
        "    path_ids = []\n",
        "    sequence_path = []\n",
        "    for i in range(n_p):\n",
        "        node_name = f\"p_{i}\"\n",
        "        seq_len = 0\n",
        "        for elem in D_G[node_name]:\n",
        "            link_to_path.append(int(elem.replace(\"l_\", \"\")))\n",
        "            seq_len += 1\n",
        "        path_ids.extend(np.full(seq_len, i, dtype=np.int32))\n",
        "        sequence_path.extend(range(seq_len))\n",
        "\n",
        "    path_to_link = []\n",
        "    sequence_links = []\n",
        "    for i in range(n_l):\n",
        "        node_name = f\"l_{i}\"\n",
        "        seq_len = 0\n",
        "        for elem in D_G[node_name]:\n",
        "            path_to_link.append(int(elem.replace(\"p_\", \"\")))\n",
        "            seq_len += 1\n",
        "        sequence_links.extend(np.full(seq_len, i, dtype=np.int32))\n",
        "\n",
        "    # Skip logic: if any path has 0 jitter or 0 delay, sample is skipped in training\n",
        "    jitter_vals = list(nx.get_node_attributes(D_G, \"jitter\").values())\n",
        "    delay_vals = list(nx.get_node_attributes(D_G, \"delay\").values())\n",
        "    if 0 in jitter_vals or 0 in delay_vals:\n",
        "        raise ValueError(\"Sample has zero jitter or delay and is skipped in original Erlang dataset.\")\n",
        "\n",
        "    # Path-level features\n",
        "    traffic = [D_G.nodes[f\"p_{i}\"][\"traffic\"] for i in range(n_p)]\n",
        "    packets = [D_G.nodes[f\"p_{i}\"][\"packets\"] for i in range(n_p)]\n",
        "    time_dist_params = [D_G.nodes[f\"p_{i}\"][\"time_dist_params\"] for i in range(n_p)]\n",
        "    label_vals = [D_G.nodes[f\"p_{i}\"][label] for i in range(n_p)]\n",
        "\n",
        "    # Link-level features\n",
        "    capacity = [D_G.nodes[f\"l_{i}\"][\"capacity\"] for i in range(n_l)]\n",
        "\n",
        "    x = {\n",
        "        \"traffic\": np.array(traffic, dtype=np.float32),\n",
        "        \"packets\": np.array(packets, dtype=np.float32),\n",
        "        \"time_dist_params\": np.array(time_dist_params, dtype=np.float32),\n",
        "        \"capacity\": np.array(capacity, dtype=np.float32),\n",
        "        \"link_to_path\": np.array(link_to_path, dtype=np.int32),\n",
        "        \"path_to_link\": np.array(path_to_link, dtype=np.int32),\n",
        "        \"path_ids\": np.array(path_ids, dtype=np.int32),\n",
        "        \"sequence_links\": np.array(sequence_links, dtype=np.int32),\n",
        "        \"sequence_path\": np.array(sequence_path, dtype=np.int32),\n",
        "        \"n_links\": np.array(n_l, dtype=np.int32),\n",
        "        \"n_paths\": np.array(n_p, dtype=np.int32),\n",
        "    }\n",
        "\n",
        "    y = np.array(label_vals, dtype=np.float32)\n",
        "\n",
        "    # Per-path src/dst + route string using routing matrix\n",
        "    R = sample.get_routing_matrix()\n",
        "    path_src_dst = []\n",
        "    path_routes = []\n",
        "    for i in range(n_p):\n",
        "        src = D_G.nodes[f\"p_{i}\"][\"source\"]\n",
        "        dst = D_G.nodes[f\"p_{i}\"][\"destination\"]\n",
        "        path_src_dst.append((src, dst))\n",
        "        path = R[src, dst]\n",
        "        if path is None or len(path) == 0:\n",
        "            route_str = \"(no path)\"\n",
        "        else:\n",
        "            route_str = \"->\".join(str(node) for node in path)\n",
        "        path_routes.append(route_str)\n",
        "\n",
        "    return x, y, path_src_dst, path_routes, D_G\n",
        "\n",
        "\n",
        "def erlang_get_sample_features(dataset_dir, label, sample_index, DatanetAPI, network_to_hypergraph):\n",
        "    \"\"\"\n",
        "    Iterate over DatanetAPI and return the sample_index-th valid (after skip logic) sample.\n",
        "    \"\"\"\n",
        "    tool = DatanetAPI(dataset_dir, shuffle=False)\n",
        "    it = iter(tool)\n",
        "    valid_idx = -1\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            sample = next(it)\n",
        "        except StopIteration:\n",
        "            raise RuntimeError(\n",
        "                f\"[Erlang] Reached end of dataset before index {sample_index}. \"\n",
        "                f\"Found {valid_idx + 1} valid samples.\"\n",
        "            )\n",
        "\n",
        "        try:\n",
        "            x, y, path_src_dst, path_routes, D_G = erlang_build_graph_and_features(\n",
        "                sample, label, network_to_hypergraph\n",
        "            )\n",
        "        except ValueError:\n",
        "            continue  # skip\n",
        "\n",
        "        valid_idx += 1\n",
        "        if valid_idx == sample_index:\n",
        "            return sample, x, y, path_src_dst, path_routes\n",
        "\n",
        "\n",
        "def erlang_build_features_with_override(sample, label, override_dict, network_to_hypergraph):\n",
        "    \"\"\"\n",
        "    Override routing for some (src,dst) pairs, rebuild features, restore routing.\n",
        "    \"\"\"\n",
        "    R_orig = sample.get_routing_matrix().copy()\n",
        "    R_mod = R_orig.copy()\n",
        "    for (s, d), path in override_dict.items():\n",
        "        R_mod[s, d] = path\n",
        "    sample._set_routing_matrix(R_mod)\n",
        "\n",
        "    try:\n",
        "        x, y, path_src_dst, path_routes, D_G = erlang_build_graph_and_features(\n",
        "            sample, label, network_to_hypergraph\n",
        "        )\n",
        "    finally:\n",
        "        sample._set_routing_matrix(R_orig)\n",
        "\n",
        "    return x, y, path_src_dst, path_routes\n",
        "\n",
        "\n",
        "def erlang_transformation(x, y):\n",
        "    \"\"\"\n",
        "    Same normalization & log transform as Erlang delay script.\n",
        "    \"\"\"\n",
        "    traffic_mean = 660.5723876953125\n",
        "    traffic_std = 420.22003173828125\n",
        "    packets_mean = 0.6605737209320068\n",
        "    packets_std = 0.42021000385284424\n",
        "    capacity_mean = 25442.669921875\n",
        "    capacity_std = 16217.9072265625\n",
        "\n",
        "    x[\"traffic\"] = (x[\"traffic\"] - traffic_mean) / traffic_std\n",
        "    x[\"packets\"] = (x[\"packets\"] - packets_mean) / packets_std\n",
        "    x[\"capacity\"] = (x[\"capacity\"] - capacity_mean) / capacity_std\n",
        "\n",
        "    return x, tf.math.log(y)\n",
        "\n",
        "\n",
        "def erlang_denorm_MAPE(y_true, y_pred):\n",
        "    denorm_y_true = tf.math.exp(y_true)\n",
        "    denorm_y_pred = tf.math.exp(y_pred)\n",
        "    return tf.abs((denorm_y_pred - denorm_y_true) / denorm_y_true) * 100.0\n",
        "\n",
        "\n",
        "def erlang_load_best_model(params, exp_dir, GNN_Model_cls):\n",
        "    \"\"\"\n",
        "    Pick the best checkpoint by filename MRE and load weights.\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.Adam(\n",
        "        learning_rate=float(params[\"HYPERPARAMETERS\"][\"learning_rate\"])\n",
        "    )\n",
        "    model = GNN_Model_cls(params)\n",
        "    loss_object = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "    model.compile(\n",
        "        loss=loss_object,\n",
        "        optimizer=optimizer,\n",
        "        run_eagerly=False,\n",
        "        metrics=erlang_denorm_MAPE,\n",
        "    )\n",
        "\n",
        "    ckpt_dir = os.path.join(exp_dir, \"ckpt_dir\")\n",
        "    best = None\n",
        "    best_mre = float(\"inf\")\n",
        "    for f in os.listdir(ckpt_dir):\n",
        "        full_path = os.path.join(ckpt_dir, f)\n",
        "        if os.path.isfile(full_path):\n",
        "            reg = re.findall(r\"\\d+\\.\\d+\", f)\n",
        "            if reg:\n",
        "                mre = float(reg[0])\n",
        "                if mre <= best_mre:\n",
        "                    best = f.replace(\".index\", \"\")\n",
        "                    if \".data\" in best:\n",
        "                        idx = best.rfind(\".\")\n",
        "                        best = best[:idx]\n",
        "                    best_mre = mre\n",
        "\n",
        "    if best is None:\n",
        "        raise RuntimeError(f\"[Erlang] No checkpoint file found in {ckpt_dir}\")\n",
        "\n",
        "    print(f\"BEST CHECKPOINT FOUND (Erlang): {best}\")\n",
        "    model.load_weights(os.path.join(ckpt_dir, best))\n",
        "    return model\n",
        "\n",
        "\n",
        "def run_erlang(args):\n",
        "    import configparser\n",
        "\n",
        "    DatanetAPI, network_to_hypergraph, GNN_Model = erlang_import_modules()\n",
        "\n",
        "    label = args.metric.lower()\n",
        "    if label == \"loss\":\n",
        "        raise SystemExit(\"RouteNet-Erlang in this script supports 'delay' and 'jitter', not 'loss'.\")\n",
        "\n",
        "    dataset_dir = resolve_dataset_dir(\n",
        "        framework=\"erlang\",\n",
        "        traffic_mode=args.traffic_mode,\n",
        "        split=args.dataset_split,\n",
        "        topo=args.topology,\n",
        "    )\n",
        "\n",
        "    metric_cap = label.capitalize()  # 'Delay' or 'Jitter'\n",
        "    exp_dir = os.path.join(ERLANG_DIR, \"TrafficModels\", metric_cap, args.traffic_mode)\n",
        "\n",
        "    print(f\"\\n=== ERLANG MODE B ===\")\n",
        "    print(f\"Dataset directory : {dataset_dir}\")\n",
        "    print(f\"Experiment dir    : {exp_dir}\")\n",
        "    print(f\"Metric / label    : {label}\")\n",
        "    print(f\"Sample index      : {args.sample_index}\")\n",
        "    print(f\"Routes file       : {args.routes_file}\\n\")\n",
        "\n",
        "    params = configparser.ConfigParser()\n",
        "    params._interpolation = configparser.ExtendedInterpolation()\n",
        "    params.read(os.path.join(exp_dir, \"config.ini\"))\n",
        "\n",
        "    sample, x_base, y_base, path_src_dst_base, path_routes_base = erlang_get_sample_features(\n",
        "        dataset_dir, label, args.sample_index, DatanetAPI, network_to_hypergraph\n",
        "    )\n",
        "\n",
        "    n_paths = int(x_base[\"n_paths\"])\n",
        "    print(f\"Loaded baseline sample: n_paths = {n_paths}, n_links = {int(x_base['n_links'])}\")\n",
        "\n",
        "    model = erlang_load_best_model(params, exp_dir, GNN_Model)\n",
        "\n",
        "    candidates = parse_candidate_routes(args.routes_file)\n",
        "    if not candidates:\n",
        "        raise RuntimeError(f\"No candidate routes parsed from {args.routes_file}\")\n",
        "\n",
        "    # Check that all candidates share the same (src, dst)\n",
        "    first_src = candidates[0][\"src\"]\n",
        "    first_dst = candidates[0][\"dst\"]\n",
        "    for c in candidates:\n",
        "        if c[\"src\"] != first_src or c[\"dst\"] != first_dst:\n",
        "            print(\"WARNING: Not all candidates share the same (src,dst); interpretation is trickier.\")\n",
        "            break\n",
        "\n",
        "    np.set_printoptions(precision=6, suppress=True, linewidth=140)\n",
        "\n",
        "    for cand in candidates:\n",
        "        cand_id = cand[\"id\"]\n",
        "        c_src = cand[\"src\"]\n",
        "        c_dst = cand[\"dst\"]\n",
        "        path_nodes = cand[\"nodes\"]\n",
        "\n",
        "        # Validate candidate path against this topology\n",
        "        validate_candidate_path_for_sample(\n",
        "            sample, cand_id, path_nodes, framework_tag=\"Erlang\", args=args\n",
        "        )\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 100)\n",
        "        print(f\"[Erlang] Candidate {cand_id}: src={c_src}, dst={c_dst}, path = {'->'.join(map(str, path_nodes))}\")\n",
        "        print(\"=\" * 100)\n",
        "\n",
        "        override = {(c_src, c_dst): path_nodes}\n",
        "        x_cand, _, path_src_dst_cand, path_routes_cand = erlang_build_features_with_override(\n",
        "            sample, label, override, network_to_hypergraph\n",
        "        )\n",
        "\n",
        "        if path_src_dst_cand != path_src_dst_base:\n",
        "            raise RuntimeError(\"Path ordering mismatch between baseline and candidate features (Erlang).\")\n",
        "\n",
        "        x_tf = {k: tf.convert_to_tensor(v) for k, v in x_cand.items()}\n",
        "        y_tf = tf.convert_to_tensor(y_base)\n",
        "        x_tf_norm, _ = erlang_transformation(x_tf, y_tf)\n",
        "\n",
        "        preds_log = model(x_tf_norm, training=False)\n",
        "        preds = np.exp(preds_log.numpy()).reshape(-1)\n",
        "\n",
        "        if preds.shape[0] != n_paths:\n",
        "            raise RuntimeError(\"Predictions length mismatch with n_paths (Erlang).\")\n",
        "\n",
        "        header = (\n",
        "            \"  idx src dst            route_nodes    traffic    packets   \"\n",
        "            f\"time_params[0:4] true_{label}_orig   pred_{label}\"\n",
        "        )\n",
        "        print(header)\n",
        "        print(\"-\" * len(header))\n",
        "\n",
        "        for i in range(n_paths):\n",
        "            src_i, dst_i = path_src_dst_cand[i]\n",
        "            route_str = path_routes_cand[i]\n",
        "\n",
        "            traffic_i = float(x_base[\"traffic\"][i])\n",
        "            packets_i = float(x_base[\"packets\"][i])\n",
        "            tparams = x_base[\"time_dist_params\"][i][:4]\n",
        "            true_val = float(y_base[i])\n",
        "            pred_val = float(preds[i])\n",
        "\n",
        "            mark = \"*\" if (src_i == c_src and dst_i == c_dst) else \" \"\n",
        "\n",
        "            print(\n",
        "                f\"{i:5d} {src_i:3d} {dst_i:3d} {mark}{route_str:>25s} \"\n",
        "                f\"{traffic_i:10.3f} {packets_i:10.3f} {str(tparams):>20s} \"\n",
        "                f\"{true_val:13.6f} {pred_val:13.6f}\"\n",
        "            )\n",
        "\n",
        "# =============================================================================\n",
        "# FERMI BRANCH\n",
        "# =============================================================================\n",
        "\n",
        "def fermi_import_modules(metric):\n",
        "    \"\"\"\n",
        "    Add RouteNet-Fermi/traffic_models to sys.path and import metric-specific\n",
        "    model + data_generator.\n",
        "    \"\"\"\n",
        "    tm_dir = os.path.join(FERMI_DIR, \"traffic_models\")\n",
        "    if tm_dir not in sys.path:\n",
        "        sys.path.insert(0, tm_dir)\n",
        "    if FERMI_DIR not in sys.path:\n",
        "        sys.path.insert(0, FERMI_DIR)\n",
        "\n",
        "    from datanetAPI import DatanetAPI\n",
        "\n",
        "    metric = metric.lower()\n",
        "    if metric == \"delay\":\n",
        "        from delay_model import RouteNet_Fermi as FermiModel\n",
        "        from delay.data_generator import network_to_hypergraph, hypergraph_to_input_data\n",
        "        subdir = \"delay\"\n",
        "    elif metric == \"jitter\":\n",
        "        from jitter_model import RouteNet_Fermi as FermiModel\n",
        "        from jitter.data_generator import network_to_hypergraph, hypergraph_to_input_data\n",
        "        subdir = \"jitter\"\n",
        "    elif metric == \"loss\":\n",
        "        from loss_model import RouteNet_Fermi as FermiModel\n",
        "        from losses.data_generator import network_to_hypergraph, hypergraph_to_input_data\n",
        "        subdir = \"losses\"\n",
        "    else:\n",
        "        raise SystemExit(\"Unsupported Fermi metric (should not happen).\")\n",
        "\n",
        "    return DatanetAPI, FermiModel, network_to_hypergraph, hypergraph_to_input_data, subdir\n",
        "\n",
        "\n",
        "def fermi_build_graph_and_features(sample, network_to_hypergraph, hypergraph_to_input_data):\n",
        "    \"\"\"\n",
        "    Build RouteNet-Fermi hypergraph + inputs for a single sample.\n",
        "    \"\"\"\n",
        "    G = nx.DiGraph(sample.get_topology_object())\n",
        "    T = sample.get_traffic_matrix()\n",
        "    P = sample.get_performance_matrix()\n",
        "    R = sample.get_routing_matrix()\n",
        "\n",
        "    HG = network_to_hypergraph(G=G, R=R, T=T, P=P)\n",
        "\n",
        "    x_dict, y_list = hypergraph_to_input_data(HG)\n",
        "\n",
        "    traffic_attr = nx.get_node_attributes(HG, \"traffic\")\n",
        "    p_nodes = list(traffic_attr.keys())\n",
        "\n",
        "    path_src_dst = []\n",
        "    path_routes = []\n",
        "    for p_name in p_nodes:\n",
        "        src = HG.nodes[p_name][\"source\"]\n",
        "        dst = HG.nodes[p_name][\"destination\"]\n",
        "        path_src_dst.append((src, dst))\n",
        "\n",
        "        path_nodes = R[src, dst]\n",
        "        if path_nodes is None or len(path_nodes) == 0:\n",
        "            route_str = \"(no path)\"\n",
        "        else:\n",
        "            route_str = \"->\".join(str(n) for n in path_nodes)\n",
        "        path_routes.append(route_str)\n",
        "\n",
        "    y = np.array(y_list, dtype=np.float32)\n",
        "\n",
        "    return x_dict, y, path_src_dst, path_routes\n",
        "\n",
        "\n",
        "def fermi_get_sample_features(dataset_dir, sample_index, metric,\n",
        "                              DatanetAPI, network_to_hypergraph, hypergraph_to_input_data):\n",
        "    \"\"\"\n",
        "    Iterate over DatanetAPI and return sample_index-th *valid* sample.\n",
        "    Skip logic:\n",
        "      - delay:   all labels > 0\n",
        "      - jitter:  all labels >= 0\n",
        "      - loss:    all labels >= 0\n",
        "    \"\"\"\n",
        "    tool = DatanetAPI(dataset_dir, shuffle=False)\n",
        "    it = iter(tool)\n",
        "    valid_idx = -1\n",
        "    metric = metric.lower()\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            sample = next(it)\n",
        "        except StopIteration:\n",
        "            raise RuntimeError(\n",
        "                f\"[Fermi] Reached end of dataset before index {sample_index}. \"\n",
        "                f\"Found {valid_idx + 1} valid samples.\"\n",
        "            )\n",
        "\n",
        "        x_dict, y, path_src_dst, path_routes = fermi_build_graph_and_features(\n",
        "            sample, network_to_hypergraph, hypergraph_to_input_data\n",
        "        )\n",
        "\n",
        "        if metric == \"delay\":\n",
        "            if not np.all(y > 0):\n",
        "                continue\n",
        "        else:\n",
        "            if not np.all(y >= 0):\n",
        "                continue\n",
        "\n",
        "        valid_idx += 1\n",
        "        if valid_idx == sample_index:\n",
        "            return sample, x_dict, y, path_src_dst, path_routes\n",
        "\n",
        "\n",
        "def fermi_build_features_with_override(sample, override_dict,\n",
        "                                       network_to_hypergraph, hypergraph_to_input_data):\n",
        "    \"\"\"\n",
        "    Override routing for some (src,dst) pairs and rebuild Fermi features.\n",
        "    \"\"\"\n",
        "    R_orig = sample.get_routing_matrix().copy()\n",
        "    R_mod = R_orig.copy()\n",
        "    for (s, d), path in override_dict.items():\n",
        "        R_mod[s, d] = path\n",
        "    sample._set_routing_matrix(R_mod)\n",
        "\n",
        "    try:\n",
        "        x_dict, y, path_src_dst, path_routes = fermi_build_graph_and_features(\n",
        "            sample, network_to_hypergraph, hypergraph_to_input_data\n",
        "        )\n",
        "    finally:\n",
        "        sample._set_routing_matrix(R_orig)\n",
        "\n",
        "    return x_dict, path_src_dst, path_routes\n",
        "\n",
        "\n",
        "def fermi_to_tensor_dict(x_dict):\n",
        "    \"\"\"\n",
        "    Convert a Fermi input dict (numpy / lists / ragged) into a dict of tensors\n",
        "    with the exact dtypes expected by the RouteNet-Fermi models.\n",
        "    \"\"\"\n",
        "    float_keys = {\n",
        "        \"traffic\",\n",
        "        \"packets\",\n",
        "        \"eq_lambda\",\n",
        "        \"avg_pkts_lambda\",\n",
        "        \"exp_max_factor\",\n",
        "        \"pkts_lambda_on\",\n",
        "        \"avg_t_off\",\n",
        "        \"avg_t_on\",\n",
        "        \"ar_a\",\n",
        "        \"sigma\",\n",
        "        \"capacity\",\n",
        "        \"queue_size\",\n",
        "        \"weight\",\n",
        "    }\n",
        "    int_keys = {\"length\", \"model\", \"policy\", \"priority\"}\n",
        "\n",
        "    out = {}\n",
        "    for k, v in x_dict.items():\n",
        "        # Ragged tensors already have correct dtypes\n",
        "        if isinstance(v, tf.RaggedTensor):\n",
        "            out[k] = v\n",
        "            continue\n",
        "\n",
        "        arr = np.array(v)\n",
        "        if k in int_keys:\n",
        "            out[k] = tf.convert_to_tensor(arr, dtype=tf.int32)\n",
        "        elif k in float_keys:\n",
        "            out[k] = tf.convert_to_tensor(arr, dtype=tf.float32)\n",
        "        else:\n",
        "            # Fallback: infer from numpy dtype\n",
        "            if np.issubdtype(arr.dtype, np.integer):\n",
        "                out[k] = tf.convert_to_tensor(arr, dtype=tf.int32)\n",
        "            else:\n",
        "                out[k] = tf.convert_to_tensor(arr, dtype=tf.float32)\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "def fermi_load_best_model(ckpt_dir, FermiModel_cls, example_x):\n",
        "    \"\"\"\n",
        "    Build and load RouteNet-Fermi model from ckpt_dir_* by choosing min-MRE ckpt.\n",
        "    \"\"\"\n",
        "    model = FermiModel_cls()\n",
        "\n",
        "    # Build variables by a forward pass using correct dtypes\n",
        "    x_tf = fermi_to_tensor_dict(example_x)\n",
        "    _ = model(x_tf, training=False)\n",
        "\n",
        "    best = None\n",
        "    best_mre = float(\"inf\")\n",
        "    for f in os.listdir(ckpt_dir):\n",
        "        full_path = os.path.join(ckpt_dir, f)\n",
        "        if os.path.isfile(full_path):\n",
        "            reg = re.findall(r\"\\d+\\.\\d+\", f)\n",
        "            if reg:\n",
        "                mre = float(reg[0])\n",
        "                if mre <= best_mre:\n",
        "                    best = f.replace(\".index\", \"\")\n",
        "                    if \".data\" in best:\n",
        "                        idx = best.rfind(\".\")\n",
        "                        best = best[:idx]\n",
        "                    best_mre = mre\n",
        "\n",
        "    if best is None:\n",
        "        raise RuntimeError(f\"[Fermi] No checkpoint file found in {ckpt_dir}\")\n",
        "\n",
        "    print(f\"BEST CHECKPOINT FOUND (Fermi): {best}\")\n",
        "    model.load_weights(os.path.join(ckpt_dir, best))\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "def run_fermi(args):\n",
        "    metric = args.metric.lower()\n",
        "    DatanetAPI, FermiModel, network_to_hypergraph, hypergraph_to_input_data, subdir = (\n",
        "        fermi_import_modules(metric)\n",
        "    )\n",
        "\n",
        "    dataset_dir = resolve_dataset_dir(\n",
        "        framework=\"fermi\",\n",
        "        traffic_mode=args.traffic_mode,\n",
        "        split=args.dataset_split,\n",
        "        topo=args.topology,\n",
        "    )\n",
        "\n",
        "    # Match Fermi's original code:\n",
        "    #  - jitter: ckpt_dir_mape_<traffic_mode>\n",
        "    #  - delay/loss: ckpt_dir_<traffic_mode>\n",
        "    if metric == \"jitter\":\n",
        "        ckpt_dir_name = f\"ckpt_dir_mape_{args.traffic_mode}\"\n",
        "    else:\n",
        "        ckpt_dir_name = f\"ckpt_dir_{args.traffic_mode}\"\n",
        "\n",
        "    ckpt_dir = os.path.join(\n",
        "        FERMI_DIR,\n",
        "        \"traffic_models\",\n",
        "        subdir,\n",
        "        ckpt_dir_name,\n",
        "    )\n",
        "\n",
        "    print(f\"\\n=== FERMI MODE B (metric={metric}) ===\")\n",
        "    print(f\"Dataset directory : {dataset_dir}\")\n",
        "    print(f\"Checkpoint dir    : {ckpt_dir}\")\n",
        "    print(f\"Sample index      : {args.sample_index}\")\n",
        "    print(f\"Routes file       : {args.routes_file}\\n\")\n",
        "\n",
        "    sample, x_base, y_base, path_src_dst_base, path_routes_base = fermi_get_sample_features(\n",
        "        dataset_dir,\n",
        "        args.sample_index,\n",
        "        metric,\n",
        "        DatanetAPI,\n",
        "        network_to_hypergraph,\n",
        "        hypergraph_to_input_data,\n",
        "    )\n",
        "\n",
        "    n_paths = len(y_base)\n",
        "    print(f\"Loaded baseline sample (Fermi): n_paths = {n_paths}\")\n",
        "\n",
        "    model = fermi_load_best_model(ckpt_dir, FermiModel, x_base)\n",
        "\n",
        "    candidates = parse_candidate_routes(args.routes_file)\n",
        "    if not candidates:\n",
        "        raise RuntimeError(f\"No candidate routes parsed from {args.routes_file}\")\n",
        "\n",
        "    first_src = candidates[0][\"src\"]\n",
        "    first_dst = candidates[0][\"dst\"]\n",
        "    for c in candidates:\n",
        "        if c[\"src\"] != first_src or c[\"dst\"] != first_dst:\n",
        "            print(\"WARNING: Not all candidates share the same (src,dst); interpretation is trickier.\")\n",
        "            break\n",
        "\n",
        "    np.set_printoptions(precision=6, suppress=True, linewidth=140)\n",
        "\n",
        "    for cand in candidates:\n",
        "        cand_id = cand[\"id\"]\n",
        "        c_src = cand[\"src\"]\n",
        "        c_dst = cand[\"dst\"]\n",
        "        path_nodes = cand[\"nodes\"]\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 100)\n",
        "        print(f\"[Fermi] Candidate {cand_id}: src={c_src}, dst={c_dst}, path = {'->'.join(map(str, path_nodes))}\")\n",
        "        print(\"=\" * 100)\n",
        "\n",
        "        override = {(c_src, c_dst): path_nodes}\n",
        "        x_cand, path_src_dst_cand, path_routes_cand = fermi_build_features_with_override(\n",
        "            sample,\n",
        "            override,\n",
        "            network_to_hypergraph,\n",
        "            hypergraph_to_input_data,\n",
        "        )\n",
        "\n",
        "        if path_src_dst_cand != path_src_dst_base:\n",
        "            raise RuntimeError(\"Path ordering mismatch between baseline and candidate features (Fermi).\")\n",
        "\n",
        "        # Convert candidate features to tensors with correct dtypes (int vs float)\n",
        "        x_tf = fermi_to_tensor_dict(x_cand)\n",
        "\n",
        "        preds_raw = model(x_tf, training=False).numpy().reshape(-1)\n",
        "        if preds_raw.shape[0] != n_paths:\n",
        "            raise RuntimeError(\"Predictions length mismatch with n_paths (Fermi).\")\n",
        "\n",
        "        # For jitter, original Fermi predict.py does np.exp(predictions)\n",
        "        if metric == \"jitter\":\n",
        "            preds = np.exp(preds_raw)\n",
        "        else:\n",
        "            preds = preds_raw\n",
        "\n",
        "        label = metric\n",
        "\n",
        "        header = (\n",
        "            \"  idx src dst            route_nodes    traffic    packets   \"\n",
        "            f\"true_{label}_orig   pred_{label}\"\n",
        "        )\n",
        "        print(header)\n",
        "        print(\"-\" * len(header))\n",
        "\n",
        "        traffic_arr = np.array(x_base[\"traffic\"]).reshape(-1)\n",
        "        packets_arr = np.array(x_base[\"packets\"]).reshape(-1)\n",
        "\n",
        "        for i in range(n_paths):\n",
        "            src_i, dst_i = path_src_dst_cand[i]\n",
        "            route_str = path_routes_cand[i]\n",
        "\n",
        "            traffic_i = float(traffic_arr[i])\n",
        "            packets_i = float(packets_arr[i])\n",
        "            true_val = float(y_base[i])\n",
        "            pred_val = float(preds[i])\n",
        "\n",
        "            mark = \"*\" if (src_i == c_src and dst_i == c_dst) else \" \"\n",
        "\n",
        "            print(\n",
        "                f\"{i:5d} {src_i:3d} {dst_i:3d} {mark}{route_str:>25s} \"\n",
        "                f\"{traffic_i:10.3f} {packets_i:10.3f} \"\n",
        "                f\"{true_val:13.6f} {pred_val:13.6f}\"\n",
        "            )\n",
        "\n",
        "# =============================================================================\n",
        "# MAIN\n",
        "# =============================================================================\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description=\"Unified Mode B predictor for RouteNet-Erlang and RouteNet-Fermi with candidate routes.\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--framework\",\n",
        "        type=str,\n",
        "        required=True,\n",
        "        choices=[\"erlang\", \"fermi\"],\n",
        "        help=\"Which GNN: 'erlang' (RouteNet-Erlang) or 'fermi' (RouteNet-Fermi).\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--metric\",\n",
        "        type=str,\n",
        "        default=\"delay\",\n",
        "        choices=[\"delay\", \"jitter\", \"loss\"],\n",
        "        help=\"Target metric.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--traffic_mode\",\n",
        "        type=str,\n",
        "        default=\"all_multiplexed\",\n",
        "        choices=[\"all_multiplexed\", \"autocorrelated\", \"constant_bitrate\", \"modulated\", \"onoff\"],\n",
        "        help=\"Traffic model under data/traffic_models.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--dataset_split\",\n",
        "        type=str,\n",
        "        default=\"test\",\n",
        "        choices=[\"train\", \"test\"],\n",
        "        help=\"Use samples from the training or test set.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--topology\",\n",
        "        type=str,\n",
        "        default=\"gbn\",\n",
        "        choices=[\"gbn\", \"geant2\", \"nsfnet\"],\n",
        "        help=\"gbn for test; geant2/nsfnet for train.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--sample_index\",\n",
        "        type=int,\n",
        "        default=0,\n",
        "        help=\"Index among *valid* samples for this framework/split/topology.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--routes_file\",\n",
        "        type=str,\n",
        "        required=True,\n",
        "        help=\"Text file with candidate routes (e.g., candidate_routes_0_5.txt).\",\n",
        "    )\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    if args.framework == \"erlang\":\n",
        "        run_erlang(args)\n",
        "    else:\n",
        "        run_fermi(args)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OaTTUqvcVodV",
        "outputId": "2c88a02a-7977-4a98-bfbe-edaf5a9aa663"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing predict_unified_with_candidate_routes_scheduling.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile predict_unified_with_candidate_routes_scheduling.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79yjV-Kw-Im9",
        "outputId": "3c8a2eb4-2b6e-403e-dcd0-39cbb7d78d95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing predict_with_candidate_routes.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile predict_with_candidate_routes.py\n",
        "import os\n",
        "import sys\n",
        "import re\n",
        "import argparse\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import configparser\n",
        "import networkx as nx\n",
        "\n",
        "# Disable GPU (same behavior you had before)\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
        "\n",
        "# Use TrafficModels/* versions of datanetAPI, read_dataset, model\n",
        "BASE_DIR = os.path.dirname(os.path.abspath(__file__))\n",
        "TRAFFICMODELS_DIR = os.path.join(BASE_DIR, \"Scheduling\")\n",
        "sys.path.insert(0, TRAFFICMODELS_DIR)\n",
        "\n",
        "from datanetAPI import DatanetAPI\n",
        "try:\n",
        "  from read_dataset import network_to_hypergraph\n",
        "except Exception as e:\n",
        "  from read_dataset import sample_to_dependency_graph\n",
        "\n",
        "from model import GNN_Model\n",
        "\n",
        "\n",
        "def build_graph_and_features(sample, label):\n",
        "    \"\"\"\n",
        "    Same as in inspect_sample_inputs.py: reproduce the GNN input representation\n",
        "    for a single sample.\n",
        "\n",
        "    Returns:\n",
        "        x: dict of numpy arrays\n",
        "        y: numpy array of target values (delay or jitter)\n",
        "        path_src_dst: list[(src, dst)] per path\n",
        "        path_routes: list[str] per path\n",
        "        D_G: relabeled hypergraph (p_i, l_i)\n",
        "    \"\"\"\n",
        "    HG = network_to_hypergraph(sample=sample)\n",
        "\n",
        "    n_p = 0\n",
        "    n_l = 0\n",
        "    mapping = {}\n",
        "    for entity in list(HG.nodes()):\n",
        "        if entity.startswith(\"p\"):\n",
        "            mapping[entity] = f\"p_{n_p}\"\n",
        "            n_p += 1\n",
        "        elif entity.startswith(\"l\"):\n",
        "            mapping[entity] = f\"l_{n_l}\"\n",
        "            n_l += 1\n",
        "\n",
        "    D_G = nx.relabel_nodes(HG, mapping)\n",
        "\n",
        "    # Incidence arrays\n",
        "    link_to_path = []\n",
        "    path_ids = []\n",
        "    sequence_path = []\n",
        "    for i in range(n_p):\n",
        "        node_name = f\"p_{i}\"\n",
        "        seq_len = 0\n",
        "        for elem in D_G[node_name]:\n",
        "            link_to_path.append(int(elem.replace(\"l_\", \"\")))\n",
        "            seq_len += 1\n",
        "        path_ids.extend(np.full(seq_len, i, dtype=np.int32))\n",
        "        sequence_path.extend(range(seq_len))\n",
        "\n",
        "    path_to_link = []\n",
        "    sequence_links = []\n",
        "    for i in range(n_l):\n",
        "        node_name = f\"l_{i}\"\n",
        "        seq_len = 0\n",
        "        for elem in D_G[node_name]:\n",
        "            path_to_link.append(int(elem.replace(\"p_\", \"\")))\n",
        "            seq_len += 1\n",
        "        sequence_links.extend(np.full(seq_len, i, dtype=np.int32))\n",
        "\n",
        "    # Skip logic: sample is ignored in original dataset if any path has 0 jitter or 0 delay\n",
        "    jitter_vals = list(nx.get_node_attributes(D_G, \"jitter\").values())\n",
        "    delay_vals = list(nx.get_node_attributes(D_G, \"delay\").values())\n",
        "    if 0 in jitter_vals or 0 in delay_vals:\n",
        "        raise ValueError(\"Sample has zero jitter or delay and is skipped in original dataset.\")\n",
        "\n",
        "    # Path-level features\n",
        "    traffic = [D_G.nodes[f\"p_{i}\"][\"traffic\"] for i in range(n_p)]\n",
        "    packets = [D_G.nodes[f\"p_{i}\"][\"packets\"] for i in range(n_p)]\n",
        "    time_dist_params = [D_G.nodes[f\"p_{i}\"][\"time_dist_params\"] for i in range(n_p)]\n",
        "    label_vals = [D_G.nodes[f\"p_{i}\"][label] for i in range(n_p)]\n",
        "\n",
        "    # Link-level features\n",
        "    capacity = [D_G.nodes[f\"l_{i}\"][\"capacity\"] for i in range(n_l)]\n",
        "\n",
        "    x = {\n",
        "        \"traffic\": np.array(traffic, dtype=np.float32),\n",
        "        \"packets\": np.array(packets, dtype=np.float32),\n",
        "        \"time_dist_params\": np.array(time_dist_params, dtype=np.float32),\n",
        "        \"capacity\": np.array(capacity, dtype=np.float32),\n",
        "        \"link_to_path\": np.array(link_to_path, dtype=np.int32),\n",
        "        \"path_to_link\": np.array(path_to_link, dtype=np.int32),\n",
        "        \"path_ids\": np.array(path_ids, dtype=np.int32),\n",
        "        \"sequence_links\": np.array(sequence_links, dtype=np.int32),\n",
        "        \"sequence_path\": np.array(sequence_path, dtype=np.int32),\n",
        "        \"n_links\": np.array(n_l, dtype=np.int32),\n",
        "        \"n_paths\": np.array(n_p, dtype=np.int32),\n",
        "    }\n",
        "\n",
        "    y = np.array(label_vals, dtype=np.float32)\n",
        "\n",
        "    # Per-path src/dst + route string using routing matrix\n",
        "    R = sample.get_routing_matrix()\n",
        "    path_src_dst = []\n",
        "    path_routes = []\n",
        "    for i in range(n_p):\n",
        "        src = D_G.nodes[f\"p_{i}\"][\"source\"]\n",
        "        dst = D_G.nodes[f\"p_{i}\"][\"destination\"]\n",
        "        path_src_dst.append((src, dst))\n",
        "        path = R[src, dst]\n",
        "        if path is None or len(path) == 0:\n",
        "            route_str = \"(no path)\"\n",
        "        else:\n",
        "            route_str = \"->\".join(str(node) for node in path)\n",
        "        path_routes.append(route_str)\n",
        "\n",
        "    return x, y, path_src_dst, path_routes, D_G\n",
        "\n",
        "\n",
        "def get_sample_features(dataset_dir, label, sample_index):\n",
        "    \"\"\"\n",
        "    Iterate over DatanetAPI and apply the same skip logic as the training generator.\n",
        "    Returns the sample and its GNN features for the specified generator index.\n",
        "    \"\"\"\n",
        "    tool = DatanetAPI(dataset_dir, shuffle=False)\n",
        "    it = iter(tool)\n",
        "    valid_idx = -1\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            sample = next(it)\n",
        "        except StopIteration:\n",
        "            raise RuntimeError(\n",
        "                f\"Reached end of dataset before index {sample_index}. \"\n",
        "                f\"Found {valid_idx + 1} valid samples.\"\n",
        "            )\n",
        "\n",
        "        try:\n",
        "            x, y, path_src_dst, path_routes, D_G = build_graph_and_features(sample, label)\n",
        "        except ValueError:\n",
        "            continue  # skipped due to zero jitter/delay\n",
        "\n",
        "        valid_idx += 1\n",
        "        if valid_idx == sample_index:\n",
        "            return sample, x, y, path_src_dst, path_routes\n",
        "\n",
        "\n",
        "def build_features_with_override(sample, label, override_dict):\n",
        "    \"\"\"\n",
        "    Temporarily override the routing matrix for given (src, dst) pairs and\n",
        "    rebuild the features. Restores the original routing afterwards.\n",
        "\n",
        "    override_dict: {(src, dst): [node0, node1, ..., nodeK]}\n",
        "    \"\"\"\n",
        "    R_orig = sample.get_routing_matrix().copy()\n",
        "    R_mod = R_orig.copy()\n",
        "    for (s, d), path in override_dict.items():\n",
        "        R_mod[s, d] = path\n",
        "    sample._set_routing_matrix(R_mod)\n",
        "\n",
        "    try:\n",
        "        x, y, path_src_dst, path_routes, D_G = build_graph_and_features(sample, label)\n",
        "    finally:\n",
        "        # Restore original routing\n",
        "        sample._set_routing_matrix(R_orig)\n",
        "\n",
        "    return x, y, path_src_dst, path_routes\n",
        "\n",
        "\n",
        "def transformation(x, y):\n",
        "    \"\"\"\n",
        "    Same normalization & log transform as your Delay check_predictions.py.\n",
        "    Reused here for both delay and jitter (the input scaling is the same).\n",
        "    \"\"\"\n",
        "    traffic_mean = 660.5723876953125\n",
        "    traffic_std = 420.22003173828125\n",
        "    packets_mean = 0.6605737209320068\n",
        "    packets_std = 0.42021000385284424\n",
        "    capacity_mean = 25442.669921875\n",
        "    capacity_std = 16217.9072265625\n",
        "\n",
        "    x[\"traffic\"] = (x[\"traffic\"] - traffic_mean) / traffic_std\n",
        "    x[\"packets\"] = (x[\"packets\"] - packets_mean) / packets_std\n",
        "    x[\"capacity\"] = (x[\"capacity\"] - capacity_mean) / capacity_std\n",
        "\n",
        "    return x, tf.math.log(y)\n",
        "\n",
        "\n",
        "def denorm_MAPE(y_true, y_pred):\n",
        "    denorm_y_true = tf.math.exp(y_true)\n",
        "    denorm_y_pred = tf.math.exp(y_pred)\n",
        "    return tf.abs((denorm_y_pred - denorm_y_true) / denorm_y_true) * 100.0\n",
        "\n",
        "\n",
        "def load_best_model(params, exp_dir):\n",
        "    optimizer = tf.keras.optimizers.Adam(\n",
        "        learning_rate=float(params[\"HYPERPARAMETERS\"][\"learning_rate\"])\n",
        "    )\n",
        "    model = GNN_Model(params)\n",
        "    loss_object = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "    model.compile(\n",
        "        loss=loss_object,\n",
        "        optimizer=optimizer,\n",
        "        run_eagerly=False,\n",
        "        metrics=denorm_MAPE,\n",
        "    )\n",
        "\n",
        "    ckpt_dir = os.path.join(exp_dir, \"ckpt_dir\")\n",
        "    best = None\n",
        "    best_mre = float(\"inf\")\n",
        "    for f in os.listdir(ckpt_dir):\n",
        "        full_path = os.path.join(ckpt_dir, f)\n",
        "        if os.path.isfile(full_path):\n",
        "            reg = re.findall(r\"\\d+\\.\\d+\", f)\n",
        "            if reg:\n",
        "                mre = float(reg[0])\n",
        "                if mre <= best_mre:\n",
        "                    best = f.replace(\".index\", \"\")\n",
        "                    if \".data\" in best:\n",
        "                        idx = best.rfind(\".\")\n",
        "                        best = best[:idx]\n",
        "                    best_mre = mre\n",
        "\n",
        "    if best is None:\n",
        "        raise RuntimeError(f\"No checkpoint file found in {ckpt_dir}\")\n",
        "\n",
        "    print(f\"BEST CHECKPOINT FOUND: {best}\")\n",
        "    model.load_weights(os.path.join(ckpt_dir, best))\n",
        "    return model\n",
        "\n",
        "\n",
        "def parse_candidate_routes(routes_file):\n",
        "    \"\"\"\n",
        "    Expect lines like:\n",
        "        1 (hops=4): 0->2->4->8->5\n",
        "        2 (hops=6): 0->2->4->8->7->6->5\n",
        "    Returns list of dicts: {'id': '1', 'src': 0, 'dst': 5, 'nodes': [0,2,4,8,5]}\n",
        "    \"\"\"\n",
        "    routes = []\n",
        "    with open(routes_file, \"r\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line or line.startswith(\"#\"):\n",
        "                continue\n",
        "            if \":\" not in line:\n",
        "                continue\n",
        "            left, right = line.split(\":\", 1)\n",
        "            left = left.strip()\n",
        "            right = right.strip()\n",
        "\n",
        "            # candidate id is first token on left\n",
        "            cand_id = left.split()[0]\n",
        "\n",
        "            node_strs = [s.strip() for s in right.split(\"->\")]\n",
        "            nodes = [int(s) for s in node_strs if s != \"\"]\n",
        "            if len(nodes) < 2:\n",
        "                continue\n",
        "            src = nodes[0]\n",
        "            dst = nodes[-1]\n",
        "            routes.append({\"id\": cand_id, \"src\": src, \"dst\": dst, \"nodes\": nodes})\n",
        "    return routes\n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description=\"Predict per-path QoS for candidate routes (Mode B counterfactuals).\"\n",
        "    )\n",
        "    parser.add_argument(\"--sample_index\", type=int, default=0)\n",
        "    parser.add_argument(\"--dataset\",default=\"traffic_models\",type=str)\n",
        "    parser.add_argument(\n",
        "        \"--metric\",\n",
        "        type=str,\n",
        "        default=\"delay\",\n",
        "        choices=[\"delay\", \"jitter\"],\n",
        "        help=\"Target metric to predict (delay or jitter).\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--traffic_mode\",\n",
        "        type=str,\n",
        "        default=\"all_multiplexed\",\n",
        "        help=\"Traffic mode folder under data/traffic_models.\",\n",
        "    )\n",
        "    parser.add_argument(\"--tt\",default=\"test\",type=str)\n",
        "    parser.add_argument(\"--graph\",default=\"gbn-multiplexed\",type=str)\n",
        "    parser.add_argument(\n",
        "        \"--routes_file\",\n",
        "        type=str,\n",
        "        required=True,\n",
        "        help=\"Text file defining candidate routes (e.g., candidate_routes_0_5.txt).\",\n",
        "    )\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    label = args.metric.lower()\n",
        "    traffic_mode = args.traffic_mode\n",
        "\n",
        "    if args.dataset!=\"traffic_models\":\n",
        "\n",
        "      dataset_dir = os.path.join(\n",
        "          BASE_DIR, \"data\", args.dataset, traffic_mode, args.tt, args.graph\n",
        "      )\n",
        "      exp_dir = os.path.join(BASE_DIR, \"TrafficModels\", metric_cap, traffic_mode)\n",
        "\n",
        "    else:\n",
        "      dataset_dir = os.path.join(\n",
        "          BASE_DIR, \"data\", args.dataset, args.tt, args.graph\n",
        "      )\n",
        "      exp_dir = os.path.join(BASE_DIR, \"Scheduling\", metric_cap, traffic_mode)\n",
        "\n",
        "    metric_cap = label.capitalize()  # 'Delay' or 'Jitter'\n",
        "\n",
        "    print(f\"Dataset directory : {dataset_dir}\")\n",
        "    print(f\"Experiment dir    : {exp_dir}\")\n",
        "    print(f\"Metric / label    : {label}\")\n",
        "    print(f\"Sample index      : {args.sample_index}\")\n",
        "    print(f\"Routes file       : {args.routes_file}\")\n",
        "\n",
        "    # Load hyperparameters & model\n",
        "    params = configparser.ConfigParser()\n",
        "    params._interpolation = configparser.ExtendedInterpolation()\n",
        "    params.read(os.path.join(exp_dir, \"config.ini\"))\n",
        "\n",
        "    model = load_best_model(params, exp_dir)\n",
        "\n",
        "    # Get baseline sample & features (original routing)\n",
        "    sample, x_base, y_base, path_src_dst_base, path_routes_base = get_sample_features(\n",
        "        dataset_dir, label, args.sample_index\n",
        "    )\n",
        "\n",
        "    n_paths = int(x_base[\"n_paths\"])\n",
        "    print(f\"\\nLoaded baseline sample: n_paths = {n_paths}, n_links = {int(x_base['n_links'])}\")\n",
        "\n",
        "    # Candidate routes\n",
        "    candidates = parse_candidate_routes(args.routes_file)\n",
        "    if not candidates:\n",
        "        raise RuntimeError(f\"No candidate routes parsed from {args.routes_file}\")\n",
        "\n",
        "    # Check that all candidates share the same (src, dst)\n",
        "    first_src = candidates[0][\"src\"]\n",
        "    first_dst = candidates[0][\"dst\"]\n",
        "    for c in candidates:\n",
        "        if c[\"src\"] != first_src or c[\"dst\"] != first_dst:\n",
        "            print(\"WARNING: Not all candidates have the same (src, dst). \"\n",
        "                  \"We will still process them, but interpretation is trickier.\")\n",
        "            break\n",
        "\n",
        "    np.set_printoptions(precision=6, suppress=True, linewidth=140)\n",
        "\n",
        "    for cand in candidates:\n",
        "        cand_id = cand[\"id\"]\n",
        "        c_src = cand[\"src\"]\n",
        "        c_dst = cand[\"dst\"]\n",
        "        path_nodes = cand[\"nodes\"]\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 100)\n",
        "        print(f\"Candidate {cand_id}: src={c_src}, dst={c_dst}, path = {'->'.join(map(str, path_nodes))}\")\n",
        "        print(\"=\" * 100)\n",
        "\n",
        "        # Build features under this overridden routing\n",
        "        override = {(c_src, c_dst): path_nodes}\n",
        "        x_cand, y_cand, path_src_dst_cand, path_routes_cand = build_features_with_override(\n",
        "            sample, label, override\n",
        "        )\n",
        "\n",
        "        # Convert to tensors and apply same transformation\n",
        "        x_tf = {k: tf.convert_to_tensor(v) for k, v in x_cand.items()}\n",
        "        y_tf = tf.convert_to_tensor(y_base)  # we only need it for tf.math.log in transformation\n",
        "        x_tf_norm, _ = transformation(x_tf, y_tf)\n",
        "\n",
        "        preds_log = model(x_tf_norm, training=False)\n",
        "        preds = np.exp(preds_log.numpy()).reshape(-1)\n",
        "\n",
        "        if preds.shape[0] != n_paths:\n",
        "            raise RuntimeError(\"Predictions length mismatch with n_paths\")\n",
        "\n",
        "        # Print table: idx, src, dst, route, traffic, packets, time_params[0:4], true_orig, pred\n",
        "        header = (\n",
        "            \"  idx src dst            route_nodes    traffic    packets   \"\n",
        "            f\"time_params[0:4] true_{label}_orig   pred_{label}\"\n",
        "        )\n",
        "        print(header)\n",
        "        print(\"-\" * len(header))\n",
        "\n",
        "        for i in range(n_paths):\n",
        "            src_i, dst_i = path_src_dst_cand[i]\n",
        "            route_str = path_routes_cand[i]\n",
        "\n",
        "            traffic_i = float(x_base[\"traffic\"][i])\n",
        "            packets_i = float(x_base[\"packets\"][i])\n",
        "            tparams = x_base[\"time_dist_params\"][i][:4]\n",
        "            true_val = float(y_base[i])\n",
        "            pred_val = float(preds[i])\n",
        "\n",
        "            mark = \"*\" if (src_i == c_src and dst_i == c_dst) else \" \"\n",
        "\n",
        "            print(\n",
        "                f\"{i:5d} {src_i:3d} {dst_i:3d} {mark}{route_str:>25s} \"\n",
        "                f\"{traffic_i:10.3f} {packets_i:10.3f} {str(tparams):>20s} \"\n",
        "                f\"{true_val:13.6f} {pred_val:13.6f}\"\n",
        "            )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8d9AAJMD-Ika",
        "outputId": "5094c752-119f-4ad0-e280-659dc46475e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing show_weighted_adj.py\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "%%writefile show_weighted_adj.py\n",
        "import os\n",
        "import argparse\n",
        "import numpy as np\n",
        "\n",
        "from datanetAPI import DatanetAPI  # root-level datanetAPI\n",
        "\n",
        "\n",
        "def get_sample(dataset_dir, sample_index):\n",
        "    reader = DatanetAPI(dataset_dir, shuffle=False)\n",
        "    it = iter(reader)\n",
        "    sample = None\n",
        "    for _ in range(sample_index + 1):\n",
        "        sample = next(it)\n",
        "    return sample\n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description=\"Show weighted adjacency (bandwidths) for one sample.\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--sample_index\",\n",
        "        type=int,\n",
        "        default=0,\n",
        "        help=\"0-based sample index in the dataset.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--traffic_mode\",\n",
        "        type=str,\n",
        "        default=\"all_multiplexed\",\n",
        "        help=\"Traffic mode folder name under data/traffic_models \"\n",
        "             \"(e.g., all_multiplexed, autocorrelated, constant_bitrate, modulated, onoff).\",\n",
        "    )\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    base_dir = os.path.dirname(os.path.abspath(__file__))\n",
        "    dataset_dir = os.path.join(\n",
        "        base_dir, \"data\", \"traffic_models\", args.traffic_mode, \"test\"\n",
        "    )\n",
        "\n",
        "    print(f\"Loading sample_index = {args.sample_index} from {dataset_dir} ...\")\n",
        "\n",
        "    sample = get_sample(dataset_dir, args.sample_index)\n",
        "\n",
        "    G = sample.get_topology_object()  # networkx DiGraph or MultiDiGraph\n",
        "    nodes = sorted(G.nodes())\n",
        "    n = len(nodes)\n",
        "    node_to_idx = {node: idx for idx, node in enumerate(nodes)}\n",
        "\n",
        "    A = np.zeros((n, n), dtype=float)\n",
        "\n",
        "    # Edges in DatanetAPI graphs have structure G[u][v][0]['bandwidth']\n",
        "    for u in nodes:\n",
        "        for v in G[u]:\n",
        "            bw = G[u][v][0][\"bandwidth\"]\n",
        "            i = node_to_idx[u]\n",
        "            j = node_to_idx[v]\n",
        "            A[i, j] = bw\n",
        "\n",
        "    print(\"\\nNodes (index -> node_id):\")\n",
        "    for idx, node in enumerate(nodes):\n",
        "        print(f\"{idx:4d} -> {node}\")\n",
        "\n",
        "    print(f\"\\nWeighted adjacency matrix shape: {A.shape}\")\n",
        "    print(\"Entry [i, j] = bandwidth of link from node i to node j (0 if no link).\\n\")\n",
        "    np.set_printoptions(suppress=True, linewidth=160)\n",
        "    print(\"Weighted adjacency matrix:\\n\")\n",
        "    print(A)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7J9AraX2-IiA",
        "outputId": "48832e16-4ed5-477a-8951-bac5f39d5250"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing list_all_routes_between_nodes.py\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "%%writefile list_all_routes_between_nodes.py\n",
        "import os\n",
        "import argparse\n",
        "\n",
        "import networkx as nx\n",
        "\n",
        "from datanetAPI import DatanetAPI  # root-level datanetAPI\n",
        "\n",
        "\n",
        "def get_sample(dataset_dir, sample_index):\n",
        "    reader = DatanetAPI(dataset_dir, shuffle=False)\n",
        "    it = iter(reader)\n",
        "    sample = None\n",
        "    for _ in range(sample_index + 1):\n",
        "        sample = next(it)\n",
        "    return sample\n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description=\"List simple routes between src and dst in one sample's topology.\"\n",
        "    )\n",
        "    parser.add_argument(\"--sample_index\", type=int, default=0)\n",
        "    parser.add_argument(\"--src\", type=int, required=True)\n",
        "    parser.add_argument(\"--dst\", type=int, required=True)\n",
        "    parser.add_argument(\n",
        "        \"--max_paths\",\n",
        "        type=int,\n",
        "        default=10,\n",
        "        help=\"Maximum number of simple paths to show.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--traffic_mode\",\n",
        "        type=str,\n",
        "        default=\"all_multiplexed\",\n",
        "        help=\"Traffic mode folder under data/traffic_models.\",\n",
        "    )\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    base_dir = os.path.dirname(os.path.abspath(__file__))\n",
        "    dataset_dir = os.path.join(\n",
        "        base_dir, \"data\", \"traffic_models\", args.traffic_mode, \"test\"\n",
        "    )\n",
        "\n",
        "    print(f\"Loading sample_index = {args.sample_index} from {dataset_dir} ...\")\n",
        "    sample = get_sample(dataset_dir, args.sample_index)\n",
        "\n",
        "    G = sample.get_topology_object()\n",
        "    DG = nx.DiGraph(G)  # ensure directed graph\n",
        "\n",
        "    if args.src not in DG.nodes or args.dst not in DG.nodes:\n",
        "        raise ValueError(f\"src {args.src} or dst {args.dst} not in topology nodes {sorted(DG.nodes())}\")\n",
        "\n",
        "    print(f\"\\nAll simple paths from {args.src} to {args.dst} (up to {args.max_paths}):\\n\")\n",
        "\n",
        "    count = 0\n",
        "    try:\n",
        "        for path in nx.all_simple_paths(DG, source=args.src, target=args.dst):\n",
        "            count += 1\n",
        "            print(f\"{count:3d} (hops={len(path)-1}): \" + \"->\".join(map(str, path)))\n",
        "            if count >= args.max_paths:\n",
        "                break\n",
        "    except nx.NetworkXNoPath:\n",
        "        print(\"No path found.\")\n",
        "\n",
        "    if count == 0:\n",
        "        print(\"No simple paths found.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXjpKPZx-Ifa",
        "outputId": "a2097abc-e656-4921-eed0-0c78f589da4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing list_flows_for_sample.py\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "%%writefile list_flows_for_sample.py\n",
        "import os\n",
        "import sys\n",
        "import re\n",
        "import argparse\n",
        "import configparser\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import networkx as nx\n",
        "\n",
        "# Disable GPU\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
        "\n",
        "# --- Path setup so we use TrafficModels code ---\n",
        "BASE_DIR = os.path.dirname(os.path.abspath(__file__))\n",
        "TRAFFICMODELS_DIR = os.path.join(BASE_DIR, \"TrafficModels\")\n",
        "\n",
        "# Make TrafficModels/ the first place Python looks\n",
        "sys.path.insert(0, TRAFFICMODELS_DIR)\n",
        "\n",
        "from datanetAPI import DatanetAPI\n",
        "from read_dataset import input_fn, network_to_hypergraph\n",
        "from model import GNN_Model\n",
        "\n",
        "\n",
        "# === Normalization & loss (same as original check_predictions.py) ===\n",
        "def transformation(x, y):\n",
        "    traffic_mean = 660.5723876953125\n",
        "    traffic_std = 420.22003173828125\n",
        "    packets_mean = 0.6605737209320068\n",
        "    packets_std = 0.42021000385284424\n",
        "    capacity_mean = 25442.669921875\n",
        "    capacity_std = 16217.9072265625\n",
        "\n",
        "    x[\"traffic\"] = (x[\"traffic\"] - traffic_mean) / traffic_std\n",
        "    x[\"packets\"] = (x[\"packets\"] - packets_mean) / packets_std\n",
        "    x[\"capacity\"] = (x[\"capacity\"] - capacity_mean) / capacity_std\n",
        "\n",
        "    return x, tf.math.log(y)\n",
        "\n",
        "\n",
        "def denorm_MAPE(y_true, y_pred):\n",
        "    denorm_y_true = tf.math.exp(y_true)\n",
        "    denorm_y_pred = tf.math.exp(y_pred)\n",
        "    return tf.abs((denorm_y_pred - denorm_y_true) / denorm_y_true) * 100.0\n",
        "\n",
        "\n",
        "# === Helpers to build path metadata from a single Sample ===\n",
        "def build_graph_and_paths(sample, label):\n",
        "    \"\"\"\n",
        "    Build the same hypergraph used by the TF dataset generator, then\n",
        "    extract per-path:\n",
        "      - true label (delay/jitter)\n",
        "      - (src, dst)\n",
        "      - route string (e.g. '0->2->4->8->5')\n",
        "\n",
        "    Returns:\n",
        "      y_true: np.ndarray [n_paths]\n",
        "      path_src_dst: list of (src, dst)\n",
        "      path_routes: list of \"a->b->c\" strings\n",
        "    \"\"\"\n",
        "    HG = network_to_hypergraph(sample=sample)\n",
        "\n",
        "    # Relabel nodes to p_0..p_{n_p-1}, l_0..l_{n_l-1} EXACTLY as in read_dataset.generator\n",
        "    n_p = 0\n",
        "    n_l = 0\n",
        "    mapping = {}\n",
        "    for entity in list(HG.nodes()):\n",
        "        if entity.startswith(\"p\"):\n",
        "            mapping[entity] = f\"p_{n_p}\"\n",
        "            n_p += 1\n",
        "        elif entity.startswith(\"l\"):\n",
        "            mapping[entity] = f\"l_{n_l}\"\n",
        "            n_l += 1\n",
        "\n",
        "    D_G = nx.relabel_nodes(HG, mapping)\n",
        "\n",
        "    # Skip samples with zero jitter/delay (same rule as generator)\n",
        "    jitter_vals = list(nx.get_node_attributes(D_G, \"jitter\").values())\n",
        "    delay_vals = list(nx.get_node_attributes(D_G, \"delay\").values())\n",
        "    if 0 in jitter_vals or 0 in delay_vals:\n",
        "        raise ValueError(\"Sample has zero jitter or delay and is skipped in original dataset.\")\n",
        "\n",
        "    # Path-level labels and src/dst\n",
        "    y_true = []\n",
        "    path_src_dst = []\n",
        "    for i in range(n_p):\n",
        "        node_name = f\"p_{i}\"\n",
        "        y_true.append(D_G.nodes[node_name][label])\n",
        "        src = D_G.nodes[node_name][\"source\"]\n",
        "        dst = D_G.nodes[node_name][\"destination\"]\n",
        "        path_src_dst.append((src, dst))\n",
        "\n",
        "    y_true = np.array(y_true, dtype=np.float32)\n",
        "\n",
        "    # Build route strings from routing matrix\n",
        "    R = sample.get_routing_matrix()\n",
        "    path_routes = []\n",
        "    for (src, dst) in path_src_dst:\n",
        "        path_nodes = R[src, dst]\n",
        "        if path_nodes is None or len(path_nodes) == 0:\n",
        "            route_str = \"(no path)\"\n",
        "        else:\n",
        "            route_str = \"->\".join(str(node) for node in path_nodes)\n",
        "        path_routes.append(route_str)\n",
        "\n",
        "    return y_true, path_src_dst, path_routes, n_p\n",
        "\n",
        "\n",
        "def get_sample_paths(dataset_dir, label, sample_index):\n",
        "    \"\"\"\n",
        "    Iterate over samples in dataset_dir using DatanetAPI, applying\n",
        "    the same skip-logic as the TF generator, and return info\n",
        "    for the 'sample_index'-th VALID sample.\n",
        "    \"\"\"\n",
        "    tool = DatanetAPI(dataset_dir, shuffle=False)\n",
        "    it = iter(tool)\n",
        "    valid_idx = -1\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            sample = next(it)\n",
        "        except StopIteration:\n",
        "            raise RuntimeError(\n",
        "                f\"Reached end of dataset before index {sample_index}. \"\n",
        "                f\"Found {valid_idx + 1} valid samples.\"\n",
        "            )\n",
        "\n",
        "        try:\n",
        "            y_true, path_src_dst, path_routes, n_paths = build_graph_and_paths(sample, label)\n",
        "        except ValueError:\n",
        "            # Sample skipped due to zero jitter/delay, same as in input_fn\n",
        "            continue\n",
        "\n",
        "        valid_idx += 1\n",
        "        if valid_idx == sample_index:\n",
        "            return sample, y_true, path_src_dst, path_routes, n_paths\n",
        "\n",
        "\n",
        "# === Model loading ===\n",
        "def get_experiment_dir(metric, traffic_mode):\n",
        "    \"\"\"\n",
        "    Returns the directory where config.ini and ckpt_dir live.\n",
        "\n",
        "    For example:\n",
        "      metric='delay', traffic_mode='all_multiplexed' ->\n",
        "        TrafficModels/Delay/all_multiplexed\n",
        "      metric='jitter', traffic_mode='all_multiplexed' ->\n",
        "        TrafficModels/Jitter/all_multiplexed\n",
        "    \"\"\"\n",
        "    metric = metric.lower()\n",
        "    if metric == \"delay\":\n",
        "        metric_dir = \"Delay\"\n",
        "    elif metric == \"jitter\":\n",
        "        metric_dir = \"Jitter\"\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported metric '{metric}'. Use 'delay' or 'jitter'.\")\n",
        "\n",
        "    exp_dir = os.path.join(TRAFFICMODELS_DIR, metric_dir, traffic_mode)\n",
        "    if not os.path.isdir(exp_dir):\n",
        "        raise RuntimeError(f\"Experiment directory does not exist: {exp_dir}\")\n",
        "    return exp_dir\n",
        "\n",
        "\n",
        "def load_best_model(params, experiment_dir):\n",
        "    \"\"\"\n",
        "    Build a GNN_Model, compile it, and load the 'best' checkpoint from\n",
        "    experiment_dir/ckpt_dir (min MRE in filename).\n",
        "    \"\"\"\n",
        "    ckpt_dir = os.path.join(experiment_dir, \"ckpt_dir\")\n",
        "    if not os.path.isdir(ckpt_dir):\n",
        "        raise RuntimeError(f\"Checkpoint directory not found: {ckpt_dir}\")\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(\n",
        "        learning_rate=float(params[\"HYPERPARAMETERS\"][\"learning_rate\"])\n",
        "    )\n",
        "    model = GNN_Model(params)\n",
        "    loss_object = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "    model.compile(\n",
        "        loss=loss_object,\n",
        "        optimizer=optimizer,\n",
        "        run_eagerly=False,\n",
        "        metrics=denorm_MAPE,\n",
        "    )\n",
        "\n",
        "    best = None\n",
        "    best_mre = float(\"inf\")\n",
        "    for f in os.listdir(ckpt_dir):\n",
        "        full_path = os.path.join(ckpt_dir, f)\n",
        "        if os.path.isfile(full_path):\n",
        "            reg = re.findall(r\"\\d+\\.\\d+\", f)\n",
        "            if reg:\n",
        "                mre = float(reg[0])\n",
        "                if mre <= best_mre:\n",
        "                    best = f.replace(\".index\", \"\")\n",
        "                    if \".data\" in best:\n",
        "                        idx = best.rfind(\".\")\n",
        "                        best = best[:idx]\n",
        "                    best_mre = mre\n",
        "\n",
        "    if best is None:\n",
        "        raise RuntimeError(f\"No checkpoint file found in {ckpt_dir}\")\n",
        "\n",
        "    print(f\"BEST CHECKPOINT FOUND: {best}\")\n",
        "    model.load_weights(os.path.join(ckpt_dir, best))\n",
        "    return model\n",
        "\n",
        "\n",
        "# === Main ===\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description=\"List flows (paths) for one sample, with true and predicted metric.\"\n",
        "    )\n",
        "    parser.add_argument(\"--sample_index\", type=int, default=0,\n",
        "                        help=\"0-based index of the test sample (after skipping invalid ones).\")\n",
        "    parser.add_argument(\n",
        "        \"--metric\",\n",
        "        type=str,\n",
        "        default=\"delay\",\n",
        "        choices=[\"delay\", \"jitter\"],\n",
        "        help=\"Which target to use as label (and which experiment directory to use).\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--traffic_mode\",\n",
        "        type=str,\n",
        "        default=\"all_multiplexed\",\n",
        "        help=\"Traffic mode folder under data/traffic_models \"\n",
        "             \"(all_multiplexed, autocorrelated, constant_bitrate, modulated, onoff).\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--max_flows\",\n",
        "        type=int,\n",
        "        default=-1,\n",
        "        help=\"Maximum number of flows to print (default -1 = print all).\",\n",
        "    )\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    label = args.metric.lower()\n",
        "\n",
        "    dataset_dir = os.path.join(\n",
        "        BASE_DIR, \"data\", \"traffic_models\", args.traffic_mode, \"test\"\n",
        "    )\n",
        "    experiment_dir = get_experiment_dir(args.metric, args.traffic_mode)\n",
        "    config_path = os.path.join(experiment_dir, \"config.ini\")\n",
        "\n",
        "    print(f\"Dataset directory : {dataset_dir}\")\n",
        "    print(f\"Experiment dir    : {experiment_dir}\")\n",
        "    print(f\"Config file       : {config_path}\")\n",
        "    print(f\"Metric / label    : {label}\")\n",
        "    print(f\"Sample index      : {args.sample_index}\")\n",
        "\n",
        "    # --- Get path metadata & true labels from DatanetAPI / hypergraph ---\n",
        "    sample, y_true, path_src_dst, path_routes, n_paths_meta = get_sample_paths(\n",
        "        dataset_dir, label, args.sample_index\n",
        "    )\n",
        "\n",
        "    # --- Build tf.data pipeline for predictions (same as training) ---\n",
        "    ds_raw = input_fn(dataset_dir, label=label, shuffle=False)\n",
        "    ds_trans = ds_raw.map(lambda x, y: transformation(x, y))\n",
        "    ds_single = ds_trans.skip(args.sample_index).take(1)\n",
        "\n",
        "    # --- Load model and predict ---\n",
        "    params = configparser.ConfigParser()\n",
        "    params._interpolation = configparser.ExtendedInterpolation()\n",
        "    if not os.path.isfile(config_path):\n",
        "        raise RuntimeError(f\"config.ini not found at {config_path}\")\n",
        "    params.read(config_path)\n",
        "\n",
        "    model = load_best_model(params, experiment_dir)\n",
        "\n",
        "    # Get predictions (log-scale) then exp to original scale\n",
        "    preds_log = model.predict(ds_single)\n",
        "    preds = np.exp(preds_log).reshape(-1)\n",
        "\n",
        "    if preds.shape[0] != y_true.shape[0]:\n",
        "        print(\n",
        "            f\"WARNING: mismatch in number of paths \"\n",
        "            f\"(preds={preds.shape[0]}, meta={y_true.shape[0]}). \"\n",
        "            \"Indexing may be inconsistent.\"\n",
        "        )\n",
        "\n",
        "    n_paths = min(preds.shape[0], y_true.shape[0])\n",
        "\n",
        "    print(f\"\\nSample index: {args.sample_index}\")\n",
        "    print(f\"Number of paths (flows): {n_paths}\\n\")\n",
        "\n",
        "    # --- Print table ---\n",
        "    np.set_printoptions(threshold=20, linewidth=200, suppress=True)\n",
        "\n",
        "    header = (\n",
        "        \"  idx src dst            route_nodes    traffic    packets                 time_params[0:4] \"\n",
        "        f\"true_{label:<10} pred_{label}\"\n",
        "    )\n",
        "    print(header)\n",
        "    print(\"-\" * len(header))\n",
        "\n",
        "    # Get a second view of sample's inputs (for traffic, packets, time_params) via inspect-like logic\n",
        "    # We reuse the same hypergraph but also need those features. Easiest: rebuild x here:\n",
        "\n",
        "    # Rebuild D_G to fetch traffic/packets/time_dist_params in path order\n",
        "    HG = network_to_hypergraph(sample=sample)\n",
        "    n_p = 0\n",
        "    n_l = 0\n",
        "    mapping = {}\n",
        "    for entity in list(HG.nodes()):\n",
        "        if entity.startswith(\"p\"):\n",
        "            mapping[entity] = f\"p_{n_p}\"\n",
        "            n_p += 1\n",
        "        elif entity.startswith(\"l\"):\n",
        "            mapping[entity] = f\"l_{n_l}\"\n",
        "            n_l += 1\n",
        "    D_G = nx.relabel_nodes(HG, mapping)\n",
        "\n",
        "    traffic = np.array([D_G.nodes[f\"p_{i}\"][\"traffic\"] for i in range(n_p)], dtype=np.float32)\n",
        "    packets = np.array([D_G.nodes[f\"p_{i}\"][\"packets\"] for i in range(n_p)], dtype=np.float32)\n",
        "    time_dist_params = np.array(\n",
        "        [D_G.nodes[f\"p_{i}\"][\"time_dist_params\"] for i in range(n_p)],\n",
        "        dtype=np.float32,\n",
        "    )\n",
        "\n",
        "    # How many flows to print?\n",
        "    if args.max_flows is None or args.max_flows < 0:\n",
        "        max_show = n_paths\n",
        "    else:\n",
        "        max_show = min(n_paths, args.max_flows)\n",
        "\n",
        "    for i in range(max_show):\n",
        "        src, dst = path_src_dst[i]\n",
        "        route = path_routes[i]\n",
        "        t = float(traffic[i])\n",
        "        p = float(packets[i])\n",
        "        tp = time_dist_params[i, :4]\n",
        "        yt = float(y_true[i])\n",
        "        yp = float(preds[i])\n",
        "        print(\n",
        "            f\"{i:5d} {src:3d} {dst:3d} {route:>25s} \"\n",
        "            f\"{t:10.3f} {p:10.3f} {np.array2string(tp, precision=3):>24s} \"\n",
        "            f\"{yt:12.6f} {yp:11.6f}\"\n",
        "        )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jc7WcjtmGTMC",
        "outputId": "d859516a-3956-4618-dcf1-168470f189a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting read_dataset.py\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "%%writefile read_dataset.py\n",
        "\"\"\"\n",
        "   Copyright 2020 Universitat Politècnica de Catalunya\n",
        "   Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "   you may not use this file except in compliance with the License.\n",
        "   You may obtain a copy of the License at\n",
        "       http://www.apache.org/licenses/LICENSE-2.0\n",
        "   Unless required by applicable law or agreed to in writing, software\n",
        "   distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "   See the License for the specific language governing permissions and\n",
        "   limitations under the License.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import networkx as nx\n",
        "from datanetAPI import DatanetAPI\n",
        "\n",
        "EXTERNAL_DISTRIBUTIONS = ['AR1-0', 'AR1-1']\n",
        "\n",
        "\n",
        "def generator(data_dir, label, shuffle=False):\n",
        "    tool = DatanetAPI(data_dir, shuffle=shuffle)\n",
        "    it = iter(tool)\n",
        "    num_samples = 0\n",
        "    for sample in it:\n",
        "        try:\n",
        "            HG = network_to_hypergraph(sample=sample)\n",
        "            num_samples += 1\n",
        "            n_p = 0\n",
        "            n_l = 0\n",
        "            mapping = {}\n",
        "            for entity in list(HG.nodes()):\n",
        "                if entity.startswith('p'):\n",
        "                    mapping[entity] = ('p_{}'.format(n_p))\n",
        "                    n_p += 1\n",
        "                elif entity.startswith('l'):\n",
        "                    mapping[entity] = ('l_{}'.format(n_l))\n",
        "                    n_l += 1\n",
        "\n",
        "            D_G = nx.relabel_nodes(HG, mapping)\n",
        "\n",
        "            link_to_path = []\n",
        "            path_ids = []\n",
        "            sequence_path = []\n",
        "            for i in range(n_p):\n",
        "                seq_len = 0\n",
        "                for elem in D_G['p_{}'.format(i)]:\n",
        "                    link_to_path.append(int(elem.replace('l_', '')))\n",
        "                    seq_len += 1\n",
        "                path_ids.extend(np.full(seq_len, i))\n",
        "                sequence_path.extend(range(seq_len))\n",
        "\n",
        "            path_to_link = []\n",
        "            sequence_links = []\n",
        "            for i in range(n_l):\n",
        "                seq_len = 0\n",
        "                for elem in D_G['l_{}'.format(i)]:\n",
        "                    path_to_link.append(int(elem.replace('p_', '')))\n",
        "                    seq_len += 1\n",
        "                sequence_links.extend(np.full(seq_len, i))\n",
        "\n",
        "            if 0 in list(nx.get_node_attributes(D_G, 'jitter').values()) or 0 in list(\n",
        "                    nx.get_node_attributes(D_G, 'delay').values()):\n",
        "                continue\n",
        "\n",
        "            yield {\"traffic\": list(nx.get_node_attributes(D_G, 'traffic').values()),\n",
        "                   \"packets\": list(nx.get_node_attributes(D_G, 'packets').values()),\n",
        "                   \"time_dist_params\": list(nx.get_node_attributes(D_G, 'time_dist_params').values()),\n",
        "                   \"capacity\": list(nx.get_node_attributes(D_G, 'capacity').values()),\n",
        "                   \"link_to_path\": link_to_path,\n",
        "                   \"path_to_link\": path_to_link,\n",
        "                   \"path_ids\": path_ids,\n",
        "                   \"sequence_links\": sequence_links,\n",
        "                   \"sequence_path\": sequence_path,\n",
        "                   \"n_links\": n_l,\n",
        "                   \"n_paths\": n_p\n",
        "                   }, list(nx.get_node_attributes(D_G, label).values())\n",
        "\n",
        "        except Exception as e:\n",
        "            pass\n",
        "\n",
        "def network_to_hypergraph(sample):\n",
        "    G = nx.DiGraph(sample.get_topology_object())\n",
        "    R = sample.get_routing_matrix()\n",
        "    T = sample.get_traffic_matrix()\n",
        "    P = sample.get_performance_matrix()\n",
        "\n",
        "    D_G = nx.DiGraph()\n",
        "    for src in range(G.number_of_nodes()):\n",
        "        for dst in range(G.number_of_nodes()):\n",
        "            if src != dst:\n",
        "\n",
        "                time_dist_params = [0] * 12\n",
        "                flow = sample.get_traffic_matrix()[src, dst]['Flows'][0]\n",
        "                if flow['TimeDist'].value != 6:\n",
        "                    time_dist_params[flow['TimeDist'].value] = 1\n",
        "                else:\n",
        "                    time_dist_params[flow['TimeDist'].value + EXTERNAL_DISTRIBUTIONS.index(\n",
        "                        flow['TimeDistParams']['Distribution'])] = 1\n",
        "\n",
        "                idx = 7\n",
        "                for k in flow['TimeDistParams']:\n",
        "                    if isinstance(flow['TimeDistParams'][k], int) or isinstance(flow['TimeDistParams'][k], float):\n",
        "                        time_dist_params[idx] = flow['TimeDistParams'][k]\n",
        "                        idx += 1\n",
        "\n",
        "                D_G.add_node('p_{}_{}'.format(src, dst),\n",
        "                             traffic=T[src, dst]['Flows'][0]['AvgBw'],\n",
        "                             packets=T[src, dst]['Flows'][0]['PktsGen'],\n",
        "                             source=src,\n",
        "                             destination=dst,\n",
        "                             time_dist_params=time_dist_params,\n",
        "                             drops=float(P[src, dst]['AggInfo']['PktsDrop']) / float(\n",
        "                                 T[src, dst]['Flows'][0]['PktsGen']),\n",
        "                             delay=float(P[src, dst]['AggInfo']['AvgDelay']),\n",
        "                             jitter=float(P[src, dst]['AggInfo']['Jitter']))\n",
        "\n",
        "                if G.has_edge(src, dst):\n",
        "                    D_G.add_node('l_{}_{}'.format(src, dst),\n",
        "                                 capacity=G.edges[src, dst]['bandwidth'])\n",
        "\n",
        "                for h_1, h_2 in [R[src, dst][i:i + 2] for i in range(0, len(R[src, dst]) - 1)]:\n",
        "                    D_G.add_edge('p_{}_{}'.format(src, dst), 'l_{}_{}'.format(h_1, h_2))\n",
        "                    D_G.add_edge('l_{}_{}'.format(h_1, h_2), 'p_{}_{}'.format(src, dst))\n",
        "\n",
        "    D_G.remove_nodes_from([node for node, out_degree in D_G.out_degree() if out_degree == 0])\n",
        "\n",
        "    return D_G\n",
        "\n",
        "def input_fn(data_dir, label, shuffle=False, samples=None):\n",
        "    ds = tf.data.Dataset.from_generator(lambda: generator(data_dir=data_dir, label=label, shuffle=shuffle),\n",
        "                                        output_types=(\n",
        "                                            {\"traffic\": tf.float32,\n",
        "                                             \"packets\": tf.float32,\n",
        "                                             \"time_dist_params\": tf.float32,\n",
        "                                             \"capacity\": tf.float32,\n",
        "                                             \"link_to_path\": tf.int32,\n",
        "                                             \"path_to_link\": tf.int32, \"path_ids\": tf.int32,\n",
        "                                             \"sequence_links\": tf.int32, \"sequence_path\": tf.int32,\n",
        "                                             \"n_links\": tf.int32, \"n_paths\": tf.int32},\n",
        "                                            tf.float32),\n",
        "                                        output_shapes=(\n",
        "                                            {\"traffic\": tf.TensorShape([None]),\n",
        "                                             \"packets\": tf.TensorShape([None]),\n",
        "                                             \"time_dist_params\": tf.TensorShape([None, None]),\n",
        "                                             \"capacity\": tf.TensorShape([None]),\n",
        "                                             \"link_to_path\": tf.TensorShape([None]),\n",
        "                                             \"path_to_link\": tf.TensorShape([None]),\n",
        "                                             \"path_ids\": tf.TensorShape([None]),\n",
        "                                             \"sequence_links\": tf.TensorShape([None]),\n",
        "                                             \"sequence_path\": tf.TensorShape([None]),\n",
        "                                             \"n_links\": tf.TensorShape([]),\n",
        "                                             \"n_paths\": tf.TensorShape([])},\n",
        "                                            tf.TensorShape([None])))\n",
        "\n",
        "    if samples:\n",
        "        ds = ds.take(samples)\n",
        "\n",
        "    ds = ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    return ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QE8gMwRVDlbD",
        "outputId": "95ce8149-a828-460e-ae32-864b02ae423b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting k_shortest_routes.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile k_shortest_routes.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o22mQhXAEl8C"
      },
      "source": [
        "## test myron's code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "IYXm-4BDKUC3",
        "outputId": "4db9da4a-515a-4d69-a040-22570a4b17da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "COMMUNITYBYLAWS.md  list_all_routes_between_nodes.py  \u001b[0m\u001b[01;34mScalability\u001b[0m/\n",
            "CONTRIBUTING.md     list_flows_for_sample.py          \u001b[01;34mScheduling\u001b[0m/\n",
            "\u001b[01;34mdata\u001b[0m/               predict_with_candidate_routes.py  show_weighted_adj.py\n",
            "datanetAPI.py       read_dataset.py                   \u001b[01;34mTrafficModels\u001b[0m/\n",
            "\u001b[01;34mfigures\u001b[0m/            README.md\n",
            "LICENSE             requirements.txt\n"
          ]
        }
      ],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AaeAb8L7QT2C"
      },
      "outputs": [],
      "source": [
        "cp /content/RouteNet-Erlang/data/traffic_models/all_multiplexed/test/gbn-multiplexed/datanetAPI.py /content/RouteNet-Erlang"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "PbcdllYh-Ic2",
        "outputId": "a56e5599-0fba-4dce-97f4-ce1000a3617c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing candidate_routes_0_5.txt\n"
          ]
        }
      ],
      "source": [
        "%%writefile candidate_routes_0_5.txt\n",
        "1 (hops=4): 0->2->4->8->5\n",
        "2 (hops=6): 0->2->4->8->7->6->5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRGXo81SQCaq",
        "outputId": "5e4ef6ed-4cf6-45ed-ea55-3b0bf41b3ca9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading sample_index = 15 from /content/RouteNet-Erlang/data/traffic_models/autocorrelated/test ...\n",
            "\n",
            "Nodes (index -> node_id):\n",
            "   0 -> 0\n",
            "   1 -> 1\n",
            "   2 -> 2\n",
            "   3 -> 3\n",
            "   4 -> 4\n",
            "   5 -> 5\n",
            "   6 -> 6\n",
            "   7 -> 7\n",
            "   8 -> 8\n",
            "   9 -> 9\n",
            "  10 -> 10\n",
            "  11 -> 11\n",
            "  12 -> 12\n",
            "  13 -> 13\n",
            "  14 -> 14\n",
            "  15 -> 15\n",
            "  16 -> 16\n",
            "\n",
            "Weighted adjacency matrix shape: (17, 17)\n",
            "Entry [i, j] = bandwidth of link from node i to node j (0 if no link).\n",
            "\n",
            "Weighted adjacency matrix:\n",
            "\n",
            "[[    0.     0. 10000.     0.     0.     0.     0.     0. 25000.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [    0.     0. 10000. 10000. 10000.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [10000. 10000.     0.     0. 25000.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [    0. 10000.     0.     0. 10000.     0.     0.     0.     0. 10000.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [    0. 10000. 25000. 10000.     0.     0.     0.     0. 40000. 10000. 40000.     0.     0.     0.     0.     0.     0.]\n",
            " [    0.     0.     0.     0.     0.     0. 10000.     0. 10000.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [    0.     0.     0.     0.     0. 10000.     0. 25000.     0.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [    0.     0.     0.     0.     0.     0. 25000.     0. 25000.     0. 40000.     0.     0.     0.     0.     0.     0.]\n",
            " [25000.     0.     0.     0. 40000. 10000.     0. 25000.     0.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [    0.     0.     0. 10000. 10000.     0.     0.     0.     0.     0. 10000.     0. 25000.     0.     0.     0.     0.]\n",
            " [    0.     0.     0.     0. 40000.     0.     0. 40000.     0. 10000.     0. 40000. 40000.     0.     0.     0.     0.]\n",
            " [    0.     0.     0.     0.     0.     0.     0.     0.     0.     0. 40000.     0.     0. 25000.     0.     0.     0.]\n",
            " [    0.     0.     0.     0.     0.     0.     0.     0.     0. 25000. 40000.     0.     0.     0. 40000.     0. 40000.]\n",
            " [    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. 25000.     0.     0. 10000.     0.     0.]\n",
            " [    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. 40000. 10000.     0. 10000.     0.]\n",
            " [    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. 10000.     0. 10000.]\n",
            " [    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. 40000.     0.     0. 10000.     0.]]\n"
          ]
        }
      ],
      "source": [
        "!/opt/conda/envs/tf_py37/bin/python show_weighted_adj.py --sample_index 15 --traffic_mode autocorrelated\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Ovb0NY1xWjKf",
        "outputId": "787edbcf-3142-4df4-f74e-cc46dfba4644"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing k_shortest_routes.py\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jaztax17WrFx",
        "outputId": "4bfddf3b-734d-41db-a8f8-f3b5748e8352"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parsing graph...\n",
            "\n",
            "Adjacency matrix (cost=1/bandwidth):\n",
            "  0    0.0    0.00002500 0.00010000    0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
            "  1 0.00002500    0.0       0.0    0.00010000    0.0       0.0    0.00004000    0.0       0.0    0.00002500    0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
            "  2 0.00010000    0.0       0.0    0.00004000 0.00004000    0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
            "  3    0.0    0.00010000 0.00004000    0.0       0.0    0.00002500 0.00002500    0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
            "  4    0.0       0.0    0.00004000    0.0       0.0       0.0       0.0    0.00002500    0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
            "  5    0.0       0.0       0.0    0.00002500    0.0       0.0       0.0       0.0    0.00002500    0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
            "  6    0.0    0.00004000    0.0    0.00002500    0.0       0.0       0.0       0.0    0.00002500 0.00004000    0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
            "  7    0.0       0.0       0.0       0.0    0.00002500    0.0       0.0       0.0    0.00002500    0.0       0.0    0.00010000    0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
            "  8    0.0       0.0       0.0       0.0       0.0    0.00002500 0.00002500 0.00002500    0.0       0.0       0.0    0.00002500 0.00001000    0.0       0.0       0.0       0.0    0.00002500 0.00002500    0.0    0.00004000    0.0       0.0       0.0   \n",
            "  9    0.0    0.00002500    0.0       0.0       0.0       0.0    0.00004000    0.0       0.0       0.0    0.00004000    0.0    0.00002500 0.00010000    0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
            " 10    0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0    0.00004000    0.0       0.0       0.0    0.00010000    0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
            " 11    0.0       0.0       0.0       0.0       0.0       0.0       0.0    0.00010000 0.00002500    0.0       0.0       0.0       0.0       0.0    0.00002500    0.0       0.0       0.0       0.0       0.0    0.00010000    0.0       0.0       0.0   \n",
            " 12    0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0    0.00001000 0.00002500    0.0       0.0       0.0    0.00002500    0.0       0.0       0.0       0.0       0.0    0.00002500    0.0    0.00002500    0.0       0.0   \n",
            " 13    0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0    0.00010000 0.00010000    0.0    0.00002500    0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
            " 14    0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0    0.00002500    0.0       0.0       0.0    0.00004000    0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
            " 15    0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0    0.00004000    0.0    0.00004000    0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
            " 16    0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0    0.00004000    0.0    0.00002500    0.0       0.0       0.0       0.0       0.0       0.0   \n",
            " 17    0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0    0.00002500    0.0       0.0       0.0       0.0       0.0       0.0       0.0    0.00002500    0.0    0.00004000    0.0       0.0       0.0       0.0       0.0   \n",
            " 18    0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0    0.00002500    0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0    0.00004000    0.0       0.0       0.0    0.00002500    0.0       0.0   \n",
            " 19    0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0    0.00002500    0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0    0.00002500\n",
            " 20    0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0    0.00004000    0.0       0.0    0.00010000    0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
            " 21    0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0    0.00002500    0.0       0.0       0.0       0.0       0.0    0.00002500    0.0       0.0       0.0    0.00002500    0.0   \n",
            " 22    0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0    0.00002500    0.0    0.00010000\n",
            " 23    0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0    0.00002500    0.0       0.0    0.00010000    0.0   \n",
            "Adjacency matrix saved to: adj_matrix.txt\n",
            "\n",
            "Running k-shortest path search...\n",
            "1 (hops=5): 0->1->9->12->8->5\n",
            "2 (hops=4): 0->1->6->3->5\n",
            "3 (hops=4): 0->1->6->8->5\n",
            "4 (hops=5): 0->1->9->6->3->5\n",
            "5 (hops=5): 0->1->9->6->8->5\n",
            "\n",
            "Saved candidate routes to: candidate_routes_0_5.txt\n"
          ]
        }
      ],
      "source": [
        "!python ./k_shortest_routes.py --src 0 --target 5 --k 5 --graph_attr_path /content/RouteNet-Erlang/data/scheduling/train/graphs/graph-geant2-wfq-0.txt --output_path candidate_routes_0_5.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "QzBtIUKCuAUz",
        "outputId": "8e0a7e96-adba-43ea-d6f9-7783c4df6462"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/RouteNet-Erlang'"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0zPVszIcqe2"
      },
      "outputs": [],
      "source": [
        "mv /content/RouteNet-Erlang/data /content/RouteNet-Fermi/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IN4N568MXMkI",
        "outputId": "31fa2258-c0e2-4c60-bcdb-82a429949306"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-12-08 00:59:19.669995: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2025-12-08 00:59:19.670029: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
            "Dataset directory : /content/RouteNet-Erlang/data/scheduling/all_multiplexed/train/graphs/graph-geant2-wfq-0.txt\n",
            "Experiment dir    : /content/RouteNet-Erlang/TrafficModels/Delay/all_multiplexed\n",
            "Metric / label    : delay\n",
            "Sample index      : 0\n",
            "Routes file       : candidate_routes_0_5.txt\n",
            "2025-12-08 00:59:20.852614: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2025-12-08 00:59:20.852658: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: baef6562fbb3\n",
            "2025-12-08 00:59:20.852673: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: baef6562fbb3\n",
            "2025-12-08 00:59:20.852779: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 550.54.15\n",
            "2025-12-08 00:59:20.852818: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 550.54.15\n",
            "2025-12-08 00:59:20.852832: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 550.54.15\n",
            "2025-12-08 00:59:20.853108: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "BEST CHECKPOINT FOUND: 192-3.56\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/RouteNet-Erlang/predict_with_candidate_routes.py\", line 138, in get_sample_features\n",
            "    sample = next(it)\n",
            "StopIteration\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/RouteNet-Erlang/predict_with_candidate_routes.py\", line 406, in <module>\n",
            "    main()\n",
            "  File \"/content/RouteNet-Erlang/predict_with_candidate_routes.py\", line 328, in main\n",
            "    sample, x_base, y_base, path_src_dst_base, path_routes_base = get_sample_features(\n",
            "  File \"/content/RouteNet-Erlang/predict_with_candidate_routes.py\", line 140, in get_sample_features\n",
            "    raise RuntimeError(\n",
            "RuntimeError: Reached end of dataset before index 0. Found 0 valid samples.\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).link_embedding\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).path_embedding\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).aggr_mlp\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).link_update.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).link_update.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).link_update.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).path_update.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).path_update.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).path_update.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).link_embedding.layer_with_weights-0\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).link_embedding.layer_with_weights-1\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).path_embedding.layer_with_weights-0\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).path_embedding.layer_with_weights-1\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).aggr_mlp.layer-0\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).aggr_mlp.layer_with_weights-0\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).aggr_mlp.layer-2\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).aggr_mlp.layer_with_weights-1\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).aggr_mlp.layer-4\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).aggr_mlp.layer_with_weights-2\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).link_embedding.layer_with_weights-0.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).link_embedding.layer_with_weights-0.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).link_embedding.layer_with_weights-1.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).link_embedding.layer_with_weights-1.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).path_embedding.layer_with_weights-0.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).path_embedding.layer_with_weights-0.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).path_embedding.layer_with_weights-1.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).path_embedding.layer_with_weights-1.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).aggr_mlp.layer_with_weights-0.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).aggr_mlp.layer_with_weights-0.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).aggr_mlp.layer_with_weights-1.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).aggr_mlp.layer_with_weights-1.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).aggr_mlp.layer_with_weights-2.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).aggr_mlp.layer_with_weights-2.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).link_update.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).link_update.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).link_update.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).path_update.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).path_update.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).path_update.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).link_update.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).link_update.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).link_update.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).path_update.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).path_update.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).path_update.bias\n",
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
          ]
        }
      ],
      "source": [
        "!/opt/conda/envs/tf_py37/bin/python predict_with_candidate_routes.py --sample_index 0 --metric delay --dataset scheduling --routes_file candidate_routes_0_5.txt --tt train --graph graphs/graph-geant2-wfq-0.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OM03unmViqdM",
        "outputId": "37e76d9d-cd80-43a0-8c52-133804c93153"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "cd /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPji6Bqyiisn",
        "outputId": "62430d0b-2079-4cb5-f329-beb3325e0db5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-12-08 01:43:55.746245: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2025-12-08 01:43:55.746299: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
            "\n",
            "=== FERMI MODE B (metric=delay) ===\n",
            "Dataset directory : /content/RouteNet-Fermi/data/traffic_models/autocorrelated/train/geant2-multiplexed\n",
            "Checkpoint dir    : /content/RouteNet-Fermi/traffic_models/delay/ckpt_dir_autocorrelated\n",
            "Sample index      : 0\n",
            "Routes file       : /content/candidate_routes_0_5_test_geant2-autocorrelated.txt\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/predict_unified_with_candidate_routes.py\", line 541, in fermi_get_sample_features\n",
            "    sample = next(it)\n",
            "StopIteration\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/predict_unified_with_candidate_routes.py\", line 839, in <module>\n",
            "    main()\n",
            "  File \"/content/predict_unified_with_candidate_routes.py\", line 835, in main\n",
            "    run_fermi(args)\n",
            "  File \"/content/predict_unified_with_candidate_routes.py\", line 687, in run_fermi\n",
            "    sample, x_base, y_base, path_src_dst_base, path_routes_base = fermi_get_sample_features(\n",
            "  File \"/content/predict_unified_with_candidate_routes.py\", line 543, in fermi_get_sample_features\n",
            "    raise RuntimeError(\n",
            "RuntimeError: [Fermi] Reached end of dataset before index 0. Found 0 valid samples.\n"
          ]
        }
      ],
      "source": [
        "!/opt/conda/envs/tf_py37/bin/python predict_unified_with_candidate_routes.py \\\n",
        "    --framework fermi \\\n",
        "    --dataset_split train \\\n",
        "    --model_repo TrafficModels \\\n",
        "    --metric delay \\\n",
        "    --traffic_mode autocorrelated \\\n",
        "    --routes_file /content/candidate_routes_0_5_test_geant2-autocorrelated.txt \\\n",
        "    --topology geant2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7xHz6yDwJ3G",
        "outputId": "8695b0ea-3866-4854-ac90-4fe0f5e7fa5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Routenet/RouteNet-Fermi/scheduling/jitter\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive/Routenet/RouteNet-Fermi/scheduling/jitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7l6qk1gMPFI",
        "outputId": "5978e182-0d96-4f71-87b4-2286b4304c04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "01-10.00.data-00000-of-00001  26-3.80.index\n",
            "01-10.00.index                27-3.26.data-00000-of-00001\n",
            "02-8.36.data-00000-of-00001   27-3.26.index\n",
            "02-8.36.index                 28-3.47.data-00000-of-00001\n",
            "03-7.14.data-00000-of-00001   28-3.47.index\n",
            "03-7.14.index                 29-3.47.data-00000-of-00001\n",
            "04-6.14.data-00000-of-00001   29-3.47.index\n",
            "04-6.14.index                 30-3.35.data-00000-of-00001\n",
            "05-5.88.data-00000-of-00001   30-3.35.index\n",
            "05-5.88.index                 31-3.30.data-00000-of-00001\n",
            "06-5.29.data-00000-of-00001   31-3.30.index\n",
            "06-5.29.index                 32-3.55.data-00000-of-00001\n",
            "07-5.32.data-00000-of-00001   32-3.55.index\n",
            "07-5.32.index                 33-3.37.data-00000-of-00001\n",
            "08-5.33.data-00000-of-00001   33-3.37.index\n",
            "08-5.33.index                 34-3.05.data-00000-of-00001\n",
            "09-5.66.data-00000-of-00001   34-3.05.index\n",
            "09-5.66.index                 35-3.12.data-00000-of-00001\n",
            "10-5.06.data-00000-of-00001   35-3.12.index\n",
            "10-5.06.index                 36-3.26.data-00000-of-00001\n",
            "11-5.20.data-00000-of-00001   36-3.26.index\n",
            "11-5.20.index                 37-3.38.data-00000-of-00001\n",
            "12-4.08.data-00000-of-00001   37-3.38.index\n",
            "12-4.08.index                 38-2.85.data-00000-of-00001\n",
            "13-4.49.data-00000-of-00001   38-2.85.index\n",
            "13-4.49.index                 39-3.20.data-00000-of-00001\n",
            "14-4.18.data-00000-of-00001   39-3.20.index\n",
            "14-4.18.index                 40-3.25.data-00000-of-00001\n",
            "15-4.15.data-00000-of-00001   40-3.25.index\n",
            "15-4.15.index                 41-3.50.data-00000-of-00001\n",
            "16-4.02.data-00000-of-00001   41-3.50.index\n",
            "16-4.02.index                 42-3.34.data-00000-of-00001\n",
            "17-5.91.data-00000-of-00001   42-3.34.index\n",
            "17-5.91.index                 43-3.24.data-00000-of-00001\n",
            "18-4.42.data-00000-of-00001   43-3.24.index\n",
            "18-4.42.index                 44-2.93.data-00000-of-00001\n",
            "19-4.29.data-00000-of-00001   44-2.93.index\n",
            "19-4.29.index                 45-3.21.data-00000-of-00001\n",
            "20-3.70.data-00000-of-00001   45-3.21.index\n",
            "20-3.70.index                 46-3.94.data-00000-of-00001\n",
            "21-3.81.data-00000-of-00001   46-3.94.index\n",
            "21-3.81.index                 47-3.17.data-00000-of-00001\n",
            "22-3.36.data-00000-of-00001   47-3.17.index\n",
            "22-3.36.index                 48-3.64.data-00000-of-00001\n",
            "23-4.86.data-00000-of-00001   48-3.64.index\n",
            "23-4.86.index                 49-3.13.data-00000-of-00001\n",
            "24-3.82.data-00000-of-00001   49-3.13.index\n",
            "24-3.82.index                 50-2.92.data-00000-of-00001\n",
            "25-3.40.data-00000-of-00001   50-2.92.index\n",
            "25-3.40.index                 checkpoint\n",
            "26-3.80.data-00000-of-00001\n"
          ]
        }
      ],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gTVaK9KnfZu",
        "outputId": "4d167432-7b6c-44dc-b39b-5fa6bcc7c204"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-12-17 21:24:13.869051: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.12/dist-packages/cv2/../../lib64:\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/Routenet/RouteNet-Fermi/scheduling/delay/ckpt_dir/../predict.py\", line 10, in <module>\n",
            "    from delay_model import RouteNet_Fermi\n",
            "ModuleNotFoundError: No module named 'delay_model'\n"
          ]
        }
      ],
      "source": [
        "!/opt/conda/envs/tf_py37/bin/python ../predict.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPEhIkf4AeaU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuG2u3BNzU4c",
        "outputId": "13af120e-dad6-415a-ae54-2102d84430e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Routenet/RouteNet-Fermi/scheduling/jitter\n"
          ]
        }
      ],
      "source": [
        "cd RouteNet-Fermi/scheduling/jitter/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTAItb8BZbfQ"
      },
      "source": [
        "## Full Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPW0UiiXr_NT",
        "outputId": "668b42ae-97e4-42cb-b79b-867e69057150"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Routenet\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive/Routenet/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-vX5XVE4a83"
      },
      "source": [
        "### Traffic Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4IHI_oH-IVR",
        "outputId": "b91fbcbd-6cb9-4d60-b289-115b1840d3f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting predict_with_candidate_routes.sh\n"
          ]
        }
      ],
      "source": [
        "%%writefile predict_with_candidate_routes.sh\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ia-6ezzl1SX5",
        "outputId": "01b4a5f7-3a50-49d5-9a5a-f3fbfc4ff343"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Routenet\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive/Routenet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuHtIF7zq30d"
      },
      "outputs": [],
      "source": [
        "cp /content/drive/MyDrive/Routenet/RouteNet-Fermi/scheduling/datanetAPI.py /content/drive/MyDrive/Routenet/RouteNet-Fermi/datanetAPI.py\n",
        "cp /content/drive/MyDrive/Routenet/RouteNet-Erlang/scheduling/datanetAPI.py /content/drive/MyDrive/Routenet/RouteNet-Erlang/datanetAPI.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7bo50wMq6sQ"
      },
      "outputs": [],
      "source": [
        "# mv  /content/drive/MyDrive/Routenet/RouteNet-Fermi/data /content/drive/MyDrive/Routenet/RouteNet-Fermi/RouteNet-Fermi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40rUafkZ3zoU"
      },
      "outputs": [],
      "source": [
        "# !rm -rf /content/drive/MyDrive/Routenet/Results\n",
        "# !mkdir /content/drive/MyDrive/Routenet/Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WGKgGATPZ-MR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Kkh_OGORmC41",
        "outputId": "8750b8d7-c9ab-4d4b-cf41-0c49d66d1bd8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating routes for tt=train, graph=geant2 ...\n",
            "Parsing graph...\n",
            "\n",
            "Adjacency matrix (cost=1/bandwidth):\n",
            "  0    0.0    0.00002500 0.00010000    0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
            "  1 0.00002500    0.0       0.0    0.00010000    0.0       0.0    0.00004000    0.0       0.0    0.00002500    0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
            "  2 0.00010000    0.0       0.0    0.00004000 0.00004000    0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
            "  3    0.0    0.00010000 0.00004000    0.0       0.0    0.00002500 0.00002500    0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
            "  4    0.0       0.0    0.00004000    0.0       0.0       0.0       0.0    0.00002500    0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
            "  5    0.0       0.0       0.0    0.00002500    0.0       0.0       0.0       0.0    0.00002500    0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
            "  6    0.0    0.00004000    0.0    0.00002500    0.0       0.0       0.0       0.0    0.00002500 0.00004000    0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
            "  7    0.0       0.0       0.0       0.0    0.00002500    0.0       0.0       0.0    0.00002500    0.0       0.0    0.00010000    0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
            "  8    0.0       0.0       0.0       0.0       0.0    0.00002500 0.00002500 0.00002500    0.0       0.0       0.0    0.00002500 0.00001000    0.0       0.0       0.0       0.0    0.00002500 0.00002500    0.0    0.00004000    0.0       0.0       0.0   \n",
            "  9    0.0    0.00002500    0.0       0.0       0.0       0.0    0.00004000    0.0       0.0       0.0    0.00004000    0.0    0.00002500 0.00010000    0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
            " 10    0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0    0.00004000    0.0       0.0       0.0    0.00010000    0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
            " 11    0.0       0.0       0.0       0.0       0.0       0.0       0.0    0.00010000 0.00002500    0.0       0.0       0.0       0.0       0.0    0.00002500    0.0       0.0       0.0       0.0       0.0    0.00010000    0.0       0.0       0.0   \n",
            " 12    0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0    0.00001000 0.00002500    0.0       0.0       0.0    0.00002500    0.0       0.0       0.0       0.0       0.0    0.00002500    0.0    0.00002500    0.0       0.0   \n",
            " 13    0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0    0.00010000 0.00010000    0.0    0.00002500    0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
            " 14    0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0    0.00002500    0.0       0.0       0.0    0.00004000    0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
            " 15    0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0    0.00004000    0.0    0.00004000    0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
            " 16    0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0    0.00004000    0.0    0.00002500    0.0       0.0       0.0       0.0       0.0       0.0   \n",
            " 17    0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0    0.00002500    0.0       0.0       0.0       0.0       0.0       0.0       0.0    0.00002500    0.0    0.00004000    0.0       0.0       0.0       0.0       0.0   \n",
            " 18    0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0    0.00002500    0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0    0.00004000    0.0       0.0       0.0    0.00002500    0.0       0.0   \n",
            " 19    0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0    0.00002500    0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0    0.00002500\n",
            " 20    0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0    0.00004000    0.0       0.0    0.00010000    0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
            " 21    0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0    0.00002500    0.0       0.0       0.0       0.0       0.0    0.00002500    0.0       0.0       0.0    0.00002500    0.0   \n",
            " 22    0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0    0.00002500    0.0    0.00010000\n",
            " 23    0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0    0.00002500    0.0       0.0    0.00010000    0.0   \n",
            "Adjacency matrix saved to: adj_matrix.txt\n",
            "\n",
            "Running k-shortest path search...\n",
            "1 (hops=5): 0->1->9->12->8->5\n",
            "2 (hops=4): 0->1->6->3->5\n",
            "3 (hops=4): 0->1->6->8->5\n",
            "4 (hops=5): 0->1->9->6->3->5\n",
            "5 (hops=5): 0->1->9->6->8->5\n",
            "\n",
            "Saved candidate routes to: /content/candidate_routes_0_5_train_geant2.txt\n",
            "Running sample_index=0, metric=delay, traffic_mode=all_multiplexed ...\n"
          ]
        }
      ],
      "source": [
        "!bash predict_with_candidate_routes.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9Kzfrg4l9UJ",
        "outputId": "025af63b-d3aa-4cf5-b9b4-bd88a88ff1a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mtraffic_models\u001b[0m/\n"
          ]
        }
      ],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3jc2e4YH5BB"
      },
      "outputs": [],
      "source": [
        "# ls /content/drive/MyDrive/Routenet/Results/erlang_TM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVtcPwGrTkdO"
      },
      "outputs": [],
      "source": [
        "# cp -r /content/Routenet/RouteNet-Fermi/data/Scheduling /content/Routenet/RouteNet-Erlang/data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mY9evfoz4fZ-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reFmH97Y4goX"
      },
      "source": [
        "### Scheduling\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "P3erH3WCTfLt",
        "outputId": "6dc35514-ec73-4c64-ef00-2fc62a0dce39"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/Routenet'"
            ]
          },
          "execution_count": 131,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iirRwNAWPXsM",
        "outputId": "81ca016c-eab1-44ff-8e6d-db94be1905e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing predict_with_candidate_routes_scheduling.sh\n"
          ]
        }
      ],
      "source": [
        "%%writefile predict_with_candidate_routes_scheduling.sh\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCUR3YKrdSxc"
      },
      "outputs": [],
      "source": [
        "mv /content/drive/MyDrive/Routenet/RouteNet-Fermi/data/ /content/drive/MyDrive/Routenet/RouteNet-Erlang/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NnqdKfrUId8h"
      },
      "outputs": [],
      "source": [
        "mv /content/drive/MyDrive/Routenet/RouteNet-Erlang/data/ /content/drive/MyDrive/Routenet/RouteNet-Fermi/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggqlcvY4rfoN"
      },
      "outputs": [],
      "source": [
        "mv /content/Routenet/RouteNet-Erlang/data/Scheduling /content/drive/MyDrive/Routenet/RouteNet-Fermi/data/scheduling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-dNLtyVmaS0l"
      },
      "outputs": [],
      "source": [
        "# cp /content/drive/MyDrive/Routenet/RouteNet-Erlang/Scalability/datanetAPI.py /content/drive/MyDrive/Routenet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6QE_FdDY0Df"
      },
      "outputs": [],
      "source": [
        "# mv /content/drive/MyDrive/Routenet/RouteNet-Erlang/data/scheduling/wfq/test  /content/drive/MyDrive/Routenet/RouteNet-Erlang/data/scheduling/wfq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NM_N58EBepUc"
      },
      "outputs": [],
      "source": [
        "# cp -r /content/drive/MyDrive/Routenet/RouteNet-Erlang/data/scheduling/wfq/test /content/drive/MyDrive/Routenet/RouteNet-Erlang/data/scheduling/wfq-drr-sp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gje_LgdsevtX"
      },
      "outputs": [],
      "source": [
        "# rm -rf /content/drive/MyDrive/Routenet/RouteNet-Fermi/data/scheduling/wfq/rediris-wfq/train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_zAn5nupyKa",
        "outputId": "9aa010cb-073a-4905-b3f8-b6678a2c59a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Routenet/RouteNet-Fermi\n"
          ]
        }
      ],
      "source": [
        "cd RouteNet-Fermi/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ucGImwoHJe85"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0yLYEp9PXqW",
        "outputId": "ba038e77-5049-4440-b86b-18493f745b29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating routes for tt=train, graph=geant2 ...\n",
            "Running sample_index=0, metric=delay, traffic_mode=wfq ...\n",
            "Saved output to /content/drive/MyDrive/Routenet/Results/fermi_S/Scheduling_results_0_5_wfq_delay_Scheduling_geant2_fermi.txt\n",
            "Running sample_index=0, metric=delay, traffic_mode=wfq-drr-sp ...\n",
            "Saved output to /content/drive/MyDrive/Routenet/Results/fermi_S/Scheduling_results_0_5_wfq-drr-sp_delay_Scheduling_geant2_fermi.txt\n",
            "Running sample_index=0, metric=jitter, traffic_mode=wfq ...\n",
            "Saved output to /content/drive/MyDrive/Routenet/Results/fermi_S/Scheduling_results_0_5_wfq_jitter_Scheduling_geant2_fermi.txt\n",
            "Running sample_index=0, metric=jitter, traffic_mode=wfq-drr-sp ...\n",
            "Saved output to /content/drive/MyDrive/Routenet/Results/fermi_S/Scheduling_results_0_5_wfq-drr-sp_jitter_Scheduling_geant2_fermi.txt\n",
            "Running sample_index=0, metric=losses, traffic_mode=wfq ...\n",
            "Saved output to /content/drive/MyDrive/Routenet/Results/fermi_S/Scheduling_results_0_5_wfq_losses_Scheduling_geant2_fermi.txt\n",
            "Running sample_index=0, metric=losses, traffic_mode=wfq-drr-sp ...\n",
            "Saved output to /content/drive/MyDrive/Routenet/Results/fermi_S/Scheduling_results_0_5_wfq-drr-sp_losses_Scheduling_geant2_fermi.txt\n",
            "Generating routes for tt=train, graph=nsfnet ...\n",
            "Running sample_index=0, metric=delay, traffic_mode=wfq ...\n",
            "Saved output to /content/drive/MyDrive/Routenet/Results/fermi_S/Scheduling_results_0_5_wfq_delay_Scheduling_nsfnet_fermi.txt\n",
            "Running sample_index=0, metric=delay, traffic_mode=wfq-drr-sp ...\n",
            "Saved output to /content/drive/MyDrive/Routenet/Results/fermi_S/Scheduling_results_0_5_wfq-drr-sp_delay_Scheduling_nsfnet_fermi.txt\n",
            "Running sample_index=0, metric=jitter, traffic_mode=wfq ...\n",
            "Saved output to /content/drive/MyDrive/Routenet/Results/fermi_S/Scheduling_results_0_5_wfq_jitter_Scheduling_nsfnet_fermi.txt\n",
            "Running sample_index=0, metric=jitter, traffic_mode=wfq-drr-sp ...\n",
            "Saved output to /content/drive/MyDrive/Routenet/Results/fermi_S/Scheduling_results_0_5_wfq-drr-sp_jitter_Scheduling_nsfnet_fermi.txt\n",
            "Running sample_index=0, metric=losses, traffic_mode=wfq ...\n",
            "Saved output to /content/drive/MyDrive/Routenet/Results/fermi_S/Scheduling_results_0_5_wfq_losses_Scheduling_nsfnet_fermi.txt\n",
            "Running sample_index=0, metric=losses, traffic_mode=wfq-drr-sp ...\n",
            "Saved output to /content/drive/MyDrive/Routenet/Results/fermi_S/Scheduling_results_0_5_wfq-drr-sp_losses_Scheduling_nsfnet_fermi.txt\n",
            "Generating routes for tt=test, graph=rediris ...\n",
            "Running sample_index=0, metric=delay, traffic_mode=wfq ...\n",
            "Saved output to /content/drive/MyDrive/Routenet/Results/fermi_S/Scheduling_results_0_5_wfq_delay_Scheduling_rediris_fermi.txt\n",
            "Running sample_index=0, metric=delay, traffic_mode=wfq-drr-sp ...\n",
            "Saved output to /content/drive/MyDrive/Routenet/Results/fermi_S/Scheduling_results_0_5_wfq-drr-sp_delay_Scheduling_rediris_fermi.txt\n",
            "Running sample_index=0, metric=jitter, traffic_mode=wfq ...\n",
            "Saved output to /content/drive/MyDrive/Routenet/Results/fermi_S/Scheduling_results_0_5_wfq_jitter_Scheduling_rediris_fermi.txt\n",
            "Running sample_index=0, metric=jitter, traffic_mode=wfq-drr-sp ...\n",
            "Saved output to /content/drive/MyDrive/Routenet/Results/fermi_S/Scheduling_results_0_5_wfq-drr-sp_jitter_Scheduling_rediris_fermi.txt\n",
            "Running sample_index=0, metric=losses, traffic_mode=wfq ...\n",
            "Saved output to /content/drive/MyDrive/Routenet/Results/fermi_S/Scheduling_results_0_5_wfq_losses_Scheduling_rediris_fermi.txt\n",
            "Running sample_index=0, metric=losses, traffic_mode=wfq-drr-sp ...\n",
            "Saved output to /content/drive/MyDrive/Routenet/Results/fermi_S/Scheduling_results_0_5_wfq-drr-sp_losses_Scheduling_rediris_fermi.txt\n"
          ]
        }
      ],
      "source": [
        "!bash predict_with_candidate_routes_scheduling.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQPLIHfXPXnx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sy1Uud3tPXlq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNfdKCXNPXjV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jj0x0mZpPXhQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uiNIj09PPXe6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXStdCdL-I-i"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2rgp6d-8zpf",
        "outputId": "24f3deda-fce5-4181-a0e4-4810a184015a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/RouteNet-Erlang/Scheduling/Delay\n"
          ]
        }
      ],
      "source": [
        "%cd /content/RouteNet-Erlang/Scheduling/Delay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxUCYH0Y9nSW",
        "outputId": "e5513e5a-d1a7-4e61-e199-7bec45424c49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-12-08 01:06:18.021034: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2025-12-08 01:06:18.021067: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
            "2025-12-08 01:06:19.084251: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2025-12-08 01:06:19.084298: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: baef6562fbb3\n",
            "2025-12-08 01:06:19.084313: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: baef6562fbb3\n",
            "2025-12-08 01:06:19.084439: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 550.54.15\n",
            "2025-12-08 01:06:19.084479: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 550.54.15\n",
            "2025-12-08 01:06:19.084493: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 550.54.15\n",
            "2025-12-08 01:06:19.084803: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Starting training from scratch...\n",
            "2025-12-08 01:06:19.892901: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
            "Epoch 1/200\n",
            "/opt/conda/envs/tf_py37/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/GatherV2_28_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/GatherV2_28_grad/Reshape:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradients/GatherV2_28_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/tf_py37/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/concat_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/concat:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradients/GatherV2_29_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/tf_py37/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/GatherV2_26_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/GatherV2_26_grad/Reshape:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradients/GatherV2_26_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/tf_py37/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/GatherV2_24_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/GatherV2_24_grad/Reshape:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradients/GatherV2_24_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/tf_py37/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/GatherV2_25_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/GatherV2_25_grad/Reshape:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradients/GatherV2_25_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/tf_py37/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/GatherV2_23_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/GatherV2_23_grad/Reshape:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradients/GatherV2_23_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/tf_py37/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/GatherV2_22_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/GatherV2_22_grad/Reshape:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradients/GatherV2_22_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/tf_py37/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/GatherV2_20_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/GatherV2_20_grad/Reshape:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradients/GatherV2_20_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/tf_py37/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/GatherV2_21_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/GatherV2_21_grad/Reshape:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradients/GatherV2_21_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/tf_py37/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/GatherV2_19_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/GatherV2_19_grad/Reshape:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradients/GatherV2_19_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/tf_py37/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/GatherV2_18_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/GatherV2_18_grad/Reshape:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradients/GatherV2_18_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/tf_py37/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/GatherV2_16_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/GatherV2_16_grad/Reshape:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradients/GatherV2_16_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/tf_py37/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/GatherV2_17_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/GatherV2_17_grad/Reshape:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradients/GatherV2_17_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/tf_py37/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/GatherV2_15_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/GatherV2_15_grad/Reshape:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradients/GatherV2_15_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/tf_py37/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/GatherV2_14_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/GatherV2_14_grad/Reshape:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradients/GatherV2_14_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/tf_py37/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/GatherV2_12_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/GatherV2_12_grad/Reshape:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradients/GatherV2_12_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/tf_py37/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/GatherV2_13_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/GatherV2_13_grad/Reshape:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradients/GatherV2_13_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/tf_py37/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/GatherV2_11_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/GatherV2_11_grad/Reshape:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradients/GatherV2_11_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/tf_py37/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/GatherV2_10_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/GatherV2_10_grad/Reshape:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradients/GatherV2_10_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/tf_py37/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/GatherV2_8_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/GatherV2_8_grad/Reshape:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradients/GatherV2_8_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/tf_py37/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/GatherV2_9_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/GatherV2_9_grad/Reshape:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradients/GatherV2_9_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/tf_py37/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/GatherV2_7_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/GatherV2_7_grad/Reshape:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradients/GatherV2_7_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/tf_py37/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/GatherV2_6_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/GatherV2_6_grad/Reshape:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradients/GatherV2_6_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/tf_py37/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/GatherV2_4_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/GatherV2_4_grad/Reshape:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradients/GatherV2_4_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/tf_py37/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/GatherV2_5_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/GatherV2_5_grad/Reshape:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradients/GatherV2_5_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/tf_py37/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/GatherV2_3_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/GatherV2_3_grad/Reshape:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradients/GatherV2_3_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/tf_py37/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/GatherV2_2_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/GatherV2_2_grad/Reshape:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradients/GatherV2_2_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/tf_py37/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/GatherV2_1_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/GatherV2_1_grad/Reshape:0\", shape=(None, None), dtype=float32), dense_shape=Tensor(\"gradients/GatherV2_1_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/tf_py37/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/GatherV2_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/GatherV2_grad/Reshape:0\", shape=(None, None), dtype=float32), dense_shape=Tensor(\"gradients/GatherV2_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  warnings.warn(\n",
            " 112/4000 [..............................] - ETA: 15:34 - loss: 1.7045 - denorm_MAPE: 236.6142Traceback (most recent call last):\n",
            "  File \"/content/RouteNet-Erlang/Scheduling/Delay/main.py\", line 79, in <module>\n",
            "    model.fit(ds_train,\n",
            "  File \"/opt/conda/envs/tf_py37/lib/python3.9/site-packages/keras/engine/training.py\", line 1184, in fit\n",
            "    tmp_logs = self.train_function(iterator)\n",
            "  File \"/opt/conda/envs/tf_py37/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\", line 885, in __call__\n",
            "    result = self._call(*args, **kwds)\n",
            "  File \"/opt/conda/envs/tf_py37/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\", line 917, in _call\n",
            "    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\n",
            "  File \"/opt/conda/envs/tf_py37/lib/python3.9/site-packages/tensorflow/python/eager/function.py\", line 3039, in __call__\n",
            "    return graph_function._call_flat(\n",
            "  File \"/opt/conda/envs/tf_py37/lib/python3.9/site-packages/tensorflow/python/eager/function.py\", line 1963, in _call_flat\n",
            "    return self._build_call_outputs(self._inference_function.call(\n",
            "  File \"/opt/conda/envs/tf_py37/lib/python3.9/site-packages/tensorflow/python/eager/function.py\", line 591, in call\n",
            "    outputs = execute.execute(\n",
            "  File \"/opt/conda/envs/tf_py37/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\", line 59, in quick_execute\n",
            "    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!/opt/conda/envs/tf_py37/bin/python main.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUbLOl1z9tyc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "o22mQhXAEl8C",
        "MXStdCdL-I-i"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
