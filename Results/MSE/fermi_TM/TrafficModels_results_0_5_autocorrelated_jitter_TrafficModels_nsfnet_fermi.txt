2025-12-18 19:28:30.566810: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/spack/gilbreth-r9/apps/cuda/12.6.0-gcc-11.5.0-a7cv7sp/lib64:/apps/spack/gilbreth-r9/apps/libxml2/2.10.3-gcc-11.5.0-wroyiwb/lib:/apps/spack/gilbreth-r9/apps/libiconv/1.17-gcc-11.5.0-36njf44/lib:/apps/spack/gilbreth-r9/apps/libfabric/1.19.1-gcc-11.5.0-b5idhcg/lib64:/apps/spack/gilbreth-r9/apps/libfabric/1.19.1-gcc-11.5.0-b5idhcg/lib:/apps/spack/gilbreth-r9/apps/openmpi/4.1.6-gcc-11.5.0-w7wc45k/lib
2025-12-18 19:28:30.567413: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2025-12-18 19:28:33.425248: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2025-12-18 19:28:33.425355: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: gilbreth-fe03.rcac.purdue.edu
2025-12-18 19:28:33.425372: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: gilbreth-fe03.rcac.purdue.edu
2025-12-18 19:28:33.425484: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 565.57.1
2025-12-18 19:28:33.425518: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 565.57.1
2025-12-18 19:28:33.425529: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 565.57.1
2025-12-18 19:28:33.426078: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-12-18 19:28:37.593751: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
2025-12-18 19:28:37.943184: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:907] Skipping loop optimization for Merge node with control input: RaggedConcat_11/assert_equal_1/Assert/AssertGuard/branch_executed/_702
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).path_update.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).path_update.recurrent_kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).path_update.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).link_update.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).link_update.recurrent_kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).link_update.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).queue_update.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).queue_update.recurrent_kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).queue_update.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).path_embedding.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).path_embedding.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).path_embedding.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).path_embedding.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).queue_embedding.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).queue_embedding.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).queue_embedding.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).queue_embedding.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).link_embedding.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).link_embedding.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).link_embedding.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).link_embedding.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).readout_path.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).readout_path.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).readout_path.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).readout_path.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).readout_path.layer_with_weights-2.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).readout_path.layer_with_weights-2.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).path_update.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).path_update.recurrent_kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).path_update.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).link_update.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).link_update.recurrent_kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).link_update.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).queue_update.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).queue_update.recurrent_kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).queue_update.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).path_embedding.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).path_embedding.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).path_embedding.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).path_embedding.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).queue_embedding.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).queue_embedding.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).queue_embedding.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).queue_embedding.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).link_embedding.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).link_embedding.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).link_embedding.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).link_embedding.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).readout_path.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).readout_path.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).readout_path.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).readout_path.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).readout_path.layer_with_weights-2.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).readout_path.layer_with_weights-2.bias
WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.

=== FERMI MODE B (metric=jitter) ===
Dataset directory : /scratch/gilbreth/mohame43/Routenet/RouteNet-Fermi/data/traffic_models/autocorrelated/train/nsfnet-autocorrelated
Checkpoint dir    : /scratch/gilbreth/mohame43/Routenet/RouteNet-Fermi/traffic_models/jitter/ckpt_dir_mape_autocorrelated
Sample index      : 0
Routes file       : /scratch/gilbreth/mohame43/Routenet/candidate_routes_0_5_train_nsfnet.txt

Loaded baseline sample (Fermi): n_paths = 182
BEST CHECKPOINT FOUND (Fermi): 30-24.19

====================================================================================================
[Fermi] Candidate 1: src=0, dst=5, path = 0->2->5
====================================================================================================
  idx src dst            route_nodes    traffic    packets   true_jitter_orig   pred_jitter
-------------------------------------------------------------------------------------------
    0   0   1                       0->1    950.996      0.951      0.023547      0.025333
    1   0   2                       0->2    756.477      0.754      0.011663      0.011103
    2   0   3                       0->3    869.635      0.870      0.062319      0.085056
    3   0   4                    0->3->4   1153.040      1.154      0.064657      0.078752
    4   0   5 *                  0->2->5    566.117      0.569      0.016770      0.016368
    5   0   6                 0->1->7->6    737.684      0.737      0.195905      0.263307
    6   0   7                    0->1->7   1191.700      1.192      0.165272      0.215357
    7   0   8                    0->3->8    226.220      0.227      0.069816      0.089401
    8   0   9                 0->3->8->9   1087.080      1.085      0.090880      0.122315
    9   0  10                0->1->7->10   1120.720      1.128      0.163538      0.196975
   10   0  11                0->3->8->11   1047.130      1.047      0.096767      0.124918
   11   0  12                0->2->5->12   1022.130      1.025      0.121103      0.155315
   12   0  13                0->2->5->13    860.969      0.862      0.151567      0.191971
   13   1   0                       1->0   1216.410      1.213      0.011931      0.013385
   14   1   2                       1->2    605.016      0.604      0.010445      0.011257
   15   1   3                    1->0->3   1023.580      1.019      0.089480      0.116416
   16   1   4                 1->0->3->4    779.641      0.782      0.098276      0.127885
   17   1   5                    1->2->5    837.155      0.838      0.014820      0.016306
   18   1   6                    1->7->6    842.462      0.838      0.143324      0.184409
   19   1   7                       1->7    909.031      0.912      0.118334      0.142863
   20   1   8                 1->0->3->8    629.704      0.631      0.095578      0.118072
   21   1   9                1->7->10->9    193.369      0.198      0.149065      0.200611
   22   1  10                   1->7->10    900.564      0.900      0.121397      0.160841
   23   1  11               1->7->10->11    276.972      0.275      0.143481      0.180791
   24   1  12                1->2->5->12    545.926      0.546      0.119042      0.161446
   25   1  13               1->7->10->13    632.698      0.632      0.146197      0.188002
   26   2   0                       2->0    242.107      0.245      0.007169      0.006883
   27   2   1                       2->1    315.499      0.314      0.006771      0.006721
   28   2   3                    2->0->3    415.798      0.418      0.079679      0.102024
   29   2   4                    2->5->4    414.973      0.410      0.003843      0.004340
   30   2   5                       2->5    256.580      0.255      0.001233      0.001306
   31   2   6                 2->1->7->6    286.609      0.286      0.175537      0.257680
   32   2   7                    2->1->7    548.554      0.552      0.129510      0.167705
   33   2   8                 2->0->3->8    239.071      0.239      0.085613      0.111990
   34   2   9                2->5->12->9    238.676      0.239      0.141402      0.207923
   35   2  10               2->5->13->10    668.616      0.666      0.324949      0.441632
   36   2  11               2->5->12->11    821.637      0.824      0.110536      0.157768
   37   2  12                   2->5->12    169.016      0.172      0.096419      0.128274
   38   2  13                   2->5->13    266.965      0.265      0.122029      0.163471
   39   3   0                       3->0    265.782      0.265      0.102535      0.146801
   40   3   1                    3->0->1    601.328      0.603      0.141878      0.158980
   41   3   2                    3->0->2    924.028      0.923      0.118843      0.144216
   42   3   4                       3->4    431.646      0.433      0.001317      0.001415
   43   3   5                    3->4->5    236.023      0.236      0.003986      0.004677
   44   3   6                    3->4->6   1194.290      1.196      0.018147      0.015987
   45   3   7                 3->0->1->7    896.603      0.893      0.290485      0.433125
   46   3   8                       3->8    956.946      0.952      0.001308      0.001400
   47   3   9                    3->8->9    342.076      0.339      0.018410      0.019603
   48   3  10                3->8->9->10    150.251      0.151      0.058462      0.068636
   49   3  11                   3->8->11    135.246      0.134      0.020228      0.020677
   50   3  12               3->8->11->12    441.364      0.438      0.040358      0.046219
   51   3  13                3->4->5->13   1232.330      1.236      0.128638      0.190265
   52   4   0                    4->3->0    791.535      0.791      0.105935      0.135386
   53   4   1                 4->3->0->1    780.928      0.786      0.147225      0.198076
   54   4   2                    4->5->2   1220.060      1.223      0.003829      0.004561
   55   4   3                       4->3   1109.560      1.101      0.001375      0.001498
   56   4   5                       4->5    473.103      0.470      0.001144      0.001202
   57   4   6                       4->6    586.372      0.584      0.012171      0.012079
   58   4   7                    4->6->7    798.424      0.796      0.032474      0.037227
   59   4   8                    4->3->8    920.226      0.924      0.004218      0.004700
   60   4   9                 4->3->8->9    707.241      0.706      0.023637      0.027231
   61   4  10               4->5->13->10    868.116      0.869      0.328876      0.500361
   62   4  11                4->3->8->11    825.132      0.826      0.026717      0.028327
   63   4  12                   4->5->12    481.624      0.476      0.091776      0.119463
   64   4  13                   4->5->13    460.771      0.461      0.118893      0.154318
   65   5   0                    5->2->0    620.598      0.621      0.011900      0.012050
   66   5   1                    5->2->1    467.680      0.471      0.011607      0.010709
   67   5   2                       5->2    809.369      0.807      0.001171      0.001205
   68   5   3                    5->4->3    877.157      0.879      0.003968      0.004601
   69   5   4                       5->4    621.810      0.624      0.000993      0.000987
   70   5   6                    5->4->6    547.246      0.550      0.018145      0.016659
   71   5   7               5->13->10->7   1153.200      1.154      0.324813      0.442888
   72   5   8                5->12->9->8    936.355      0.939      0.258714      0.390296
   73   5   9                   5->12->9   1189.470      1.185      0.135780      0.180311
   74   5  10                  5->13->10   1280.920      1.276      0.313853      0.445239
   75   5  11                  5->12->11   1125.100      1.126      0.105871      0.144717
   76   5  12                      5->12    834.535      0.836      0.089844      0.113426
   77   5  13                      5->13   1101.830      1.103      0.114515      0.145389
   78   6   0                 6->4->3->0    783.190      0.783      0.132571      0.172848
   79   6   1                    6->7->1    204.998      0.207      0.074628      0.089107
   80   6   2                 6->7->1->2    650.772      0.649      0.097334      0.120941
   81   6   3                    6->4->3   1128.410      1.125      0.017118      0.017878
   82   6   4                       6->4   1109.700      1.113      0.013641      0.012945
   83   6   5                    6->4->5    546.107      0.542      0.017724      0.018594
   84   6   7                       6->7    327.736      0.326      0.012159      0.011101
   85   6   8                 6->4->3->8    190.086      0.189      0.025247      0.028254
   86   6   9                6->7->10->9    151.687      0.151      0.048316      0.056093
   87   6  10                   6->7->10    382.617      0.384      0.016969      0.017740
   88   6  11               6->7->10->11   1193.960      1.195      0.038486      0.039840
   89   6  12                6->4->5->12    164.477      0.166      0.119741      0.162280
   90   6  13               6->7->10->13    845.147      0.843      0.045068      0.052803
   91   7   0                    7->1->0    402.073      0.401      0.071960      0.081612
   92   7   1                       7->1    836.404      0.834      0.052651      0.062752
   93   7   2                    7->1->2    806.385      0.806      0.069964      0.079422
   94   7   3                 7->1->0->3    484.897      0.487      0.159925      0.224897
   95   7   4                    7->6->4    371.303      0.368      0.046776      0.053635
   96   7   5               7->10->13->5   1212.300      1.213      0.047812      0.049544
   97   7   6                       7->6    694.965      0.694      0.023670      0.026459
   98   7   8                7->10->9->8    422.978      0.422      0.129435      0.150992
   99   7   9                   7->10->9    628.554      0.629      0.023035      0.023642
  100   7  10                      7->10    924.967      0.929      0.001627      0.001695
  101   7  11                  7->10->11    336.399      0.335      0.016147      0.016846
  102   7  12              7->10->11->12    204.025      0.206      0.039162      0.044596
  103   7  13                  7->10->13    641.413      0.649      0.022266      0.023835
  104   8   0                    8->3->0   1158.230      1.159      0.102162      0.120965
  105   8   1                 8->3->0->1    375.262      0.377      0.145614      0.182597
  106   8   2                 8->3->0->2    427.994      0.430      0.132719      0.153833
  107   8   3                       8->3   1002.060      1.000      0.001484      0.001549
  108   8   4                    8->3->4    566.517      0.570      0.004322      0.004915
  109   8   5               8->11->12->5   1083.920      1.083      0.082757      0.122754
  110   8   6                 8->3->4->6   1134.910      1.131      0.024686      0.024375
  111   8   7                8->9->10->7   1044.790      1.045      0.061419      0.069022
  112   8   9                       8->9   1228.210      1.225      0.012670      0.014041
  113   8  10                   8->9->10    374.248      0.375      0.049464      0.051528
  114   8  11                      8->11    969.183      0.969      0.014362      0.013401
  115   8  12                  8->11->12    372.232      0.373      0.033265      0.036382
  116   8  13               8->9->10->13    156.281      0.156      0.089033      0.113983
  117   9   0                 9->8->3->0    617.431      0.615      0.210339      0.300044
  118   9   1                9->10->7->1    659.700      0.661      0.102881      0.125587
  119   9   2                9->12->5->2    808.157      0.802      0.056384      0.079020
  120   9   3                    9->8->3    265.941      0.263      0.088521      0.118356
  121   9   4                 9->8->3->4    278.295      0.280      0.096680      0.132670
  122   9   5                   9->12->5    584.732      0.584      0.050068      0.065555
  123   9   6                9->10->7->6    339.453      0.338      0.071804      0.090406
  124   9   7                   9->10->7    690.367      0.689      0.033246      0.039667
  125   9   8                       9->8   1210.950      1.209      0.084009      0.105951
  126   9  10                      9->10    988.611      0.988      0.024550      0.029152
  127   9  11                   9->8->11    223.161      0.224      0.108338      0.129268
  128   9  12                      9->12    794.612      0.800      0.006120      0.006420
  129   9  13                  9->10->13    464.918      0.463      0.053022      0.055633
  130  10   0                10->7->1->0    241.395      0.243      0.086241      0.119287
  131  10   1                   10->7->1   1192.140      1.195      0.060675      0.067678
  132  10   2                10->7->1->2   1032.760      1.031      0.083331      0.101180
  133  10   3                10->9->8->3    801.689      0.802      0.126265      0.156619
  134  10   4                10->7->6->4    687.574      0.687      0.053329      0.063874
  135  10   5                  10->13->5   1143.740      1.138      0.041931      0.047114
  136  10   6                   10->7->6   1242.260      1.235      0.030779      0.030963
  137  10   7                      10->7   1219.180      1.219      0.002691      0.002644
  138  10   8                   10->9->8   1052.090      1.048      0.115444      0.146543
  139  10   9                      10->9   1128.340      1.133      0.017101      0.017089
  140  10  11                     10->11   1107.930      1.105      0.010033      0.010265
  141  10  12                 10->11->12    140.038      0.143      0.032163      0.036392
  142  10  13                     10->13    297.650      0.298      0.016389      0.016385
  143  11   0                11->8->3->0    676.348      0.679      0.125975      0.173339
  144  11   1               11->10->7->1    860.497      0.865      0.082153      0.107191
  145  11   2               11->12->5->2    287.491      0.285      0.062409      0.079294
  146  11   3                   11->8->3    345.232      0.347      0.011725      0.010492
  147  11   4                11->8->3->4    384.355      0.386      0.017940      0.019748
  148  11   5                  11->12->5   1119.160      1.120      0.051081      0.062430
  149  11   6               11->10->7->6   1236.920      1.233      0.048717      0.055291
  150  11   7                  11->10->7   1012.440      1.018      0.015249      0.016324
  151  11   8                      11->8    221.457      0.221      0.006339      0.006879
  152  11   9                  11->12->9    394.798      0.395      0.058434      0.063301
  153  11  10                     11->10    189.859      0.190      0.009141      0.009410
  154  11  12                     11->12    139.739      0.140      0.010508      0.011056
  155  11  13                 11->10->13    296.510      0.298      0.036333      0.039786
  156  12   0                12->5->2->0    232.149      0.231      0.059981      0.078898
  157  12   1                12->5->2->1    413.449      0.411      0.057997      0.076809
  158  12   2                   12->5->2    768.263      0.769      0.037574      0.047337
  159  12   3                12->9->8->3    965.157      0.963      0.149768      0.183144
  160  12   4                   12->5->4    278.762      0.279      0.039601      0.052842
  161  12   5                      12->5    476.085      0.475      0.034693      0.046304
  162  12   6                12->5->4->6    649.998      0.653      0.063622      0.070955
  163  12   7               12->9->10->7    924.688      0.929      0.083021      0.097209
  164  12   8                   12->9->8    566.047      0.566      0.142271      0.150903
  165  12   9                      12->9   1029.540      1.035      0.038275      0.036371
  166  12  10                  12->9->10    923.881      0.930      0.070884      0.086350
  167  12  11                     12->11   1019.540      1.022      0.007274      0.007212
  168  12  13                  12->5->13    560.061      0.558      0.160439      0.188410
  169  13   0                13->5->2->0    441.067      0.441      0.041007      0.047157
  170  13   1               13->10->7->1    354.674      0.353      0.237107      0.357230
  171  13   2                   13->5->2    178.976      0.179      0.021322      0.022477
  172  13   3                13->5->4->3    215.649      0.216      0.027666      0.027961
  173  13   4                   13->5->4    483.453      0.481      0.019896      0.020425
  174  13   5                      13->5   1240.700      1.232      0.014524      0.014549
  175  13   6               13->10->7->6    165.227      0.168      0.202838      0.311339
  176  13   7                  13->10->7    714.297      0.713      0.163810      0.232497
  177  13   8               13->10->9->8    909.047      0.912      0.301860      0.487609
  178  13   9                  13->10->9    277.418      0.276      0.190465      0.258607
  179  13  10                     13->10   1225.230      1.228      0.160929      0.234262
  180  13  11                 13->10->11   1167.790      1.168      0.181880      0.325562
  181  13  12                  13->5->12    656.660      0.654      0.111019      0.134575
