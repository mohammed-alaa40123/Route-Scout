2025-12-18 19:29:35.039088: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/spack/gilbreth-r9/apps/cuda/12.6.0-gcc-11.5.0-a7cv7sp/lib64:/apps/spack/gilbreth-r9/apps/libxml2/2.10.3-gcc-11.5.0-wroyiwb/lib:/apps/spack/gilbreth-r9/apps/libiconv/1.17-gcc-11.5.0-36njf44/lib:/apps/spack/gilbreth-r9/apps/libfabric/1.19.1-gcc-11.5.0-b5idhcg/lib64:/apps/spack/gilbreth-r9/apps/libfabric/1.19.1-gcc-11.5.0-b5idhcg/lib:/apps/spack/gilbreth-r9/apps/openmpi/4.1.6-gcc-11.5.0-w7wc45k/lib
2025-12-18 19:29:35.039523: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2025-12-18 19:29:38.494729: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2025-12-18 19:29:38.494807: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: gilbreth-fe03.rcac.purdue.edu
2025-12-18 19:29:38.494823: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: gilbreth-fe03.rcac.purdue.edu
2025-12-18 19:29:38.494914: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 565.57.1
2025-12-18 19:29:38.494944: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 565.57.1
2025-12-18 19:29:38.494953: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 565.57.1
2025-12-18 19:29:38.495613: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-12-18 19:29:43.020792: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
2025-12-18 19:29:43.361051: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:907] Skipping loop optimization for Merge node with control input: Assert_1/AssertGuard/branch_executed/_1027
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).path_update.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).path_update.recurrent_kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).path_update.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).link_update.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).link_update.recurrent_kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).link_update.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).queue_update.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).queue_update.recurrent_kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).queue_update.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).path_embedding.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).path_embedding.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).path_embedding.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).path_embedding.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).queue_embedding.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).queue_embedding.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).queue_embedding.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).queue_embedding.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).link_embedding.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).link_embedding.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).link_embedding.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).link_embedding.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).readout_path.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).readout_path.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).readout_path.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).readout_path.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).readout_path.layer_with_weights-2.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).readout_path.layer_with_weights-2.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).path_update.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).path_update.recurrent_kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).path_update.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).link_update.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).link_update.recurrent_kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).link_update.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).queue_update.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).queue_update.recurrent_kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).queue_update.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).path_embedding.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).path_embedding.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).path_embedding.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).path_embedding.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).queue_embedding.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).queue_embedding.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).queue_embedding.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).queue_embedding.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).link_embedding.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).link_embedding.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).link_embedding.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).link_embedding.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).readout_path.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).readout_path.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).readout_path.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).readout_path.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).readout_path.layer_with_weights-2.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).readout_path.layer_with_weights-2.bias
WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.

=== FERMI MODE B (metric=delay) ===
Dataset directory : /scratch/gilbreth/mohame43/Routenet/RouteNet-Fermi/data/traffic_models/modulated/test/gbn-modulated
Checkpoint dir    : /scratch/gilbreth/mohame43/Routenet/RouteNet-Fermi/traffic_models/delay/ckpt_dir_modulated
Sample index      : 0
Routes file       : /scratch/gilbreth/mohame43/Routenet/candidate_routes_0_5_test_gbn.txt

Loaded baseline sample (Fermi): n_paths = 272
BEST CHECKPOINT FOUND (Fermi): 48-5.26

====================================================================================================
[Fermi] Candidate 1: src=0, dst=5, path = 0->8->5
====================================================================================================
  idx src dst            route_nodes    traffic    packets   true_delay_orig   pred_delay
-----------------------------------------------------------------------------------------
    0   0   1                    0->2->1    950.317      0.953      1.343900      1.410458
    1   0   2                       0->2   1118.710      1.119      0.951242      0.936290
    2   0   3                 0->2->1->3    610.118      0.611      1.783560      2.055608
    3   0   4                    0->8->4    567.984      0.564      0.449582      0.464806
    4   0   5 *                  0->8->5   1145.850      1.146      2.187390      2.167821
    5   0   6                 0->8->7->6    438.206      0.441      0.552875      0.627574
    6   0   7                    0->8->7   1047.980      1.050      0.359494      0.389075
    7   0   8                       0->8    696.593      0.698      0.337142      0.331598
    8   0   9                 0->8->4->9   1188.900      1.191      1.044010      1.086919
    9   0  10                0->8->4->10    534.550      0.535      0.708619      0.747684
   10   0  11            0->8->4->10->11    145.898      0.146      0.890338      0.876226
   11   0  12            0->8->4->10->12   1221.160      1.225      0.813340      0.874636
   12   0  13        0->8->4->10->11->13   1126.200      1.129      0.894268      0.910746
   13   0  14        0->8->4->10->12->14    670.521      0.673      0.904421      0.975936
   14   0  15    0->8->4->10->12->14->15    645.570      0.642      2.877530      2.904882
   15   0  16        0->8->4->10->12->16    908.015      0.911      0.854128      0.934380
   16   1   0                    1->2->0    860.701      0.864      1.582230      1.567798
   17   1   2                       1->2    742.714      0.746      0.820805      0.781635
   18   1   3                       1->3    728.429      0.727      0.814999      0.834793
   19   1   4                       1->4    660.905      0.661      1.463750      1.451901
   20   1   5                 1->4->8->5   1022.870      1.021      3.376150      3.213190
   21   1   6             1->4->10->7->6    393.487      0.393      2.043770      1.932176
   22   1   7                1->4->10->7    140.037      0.139      2.057230      1.889042
   23   1   8                    1->4->8    293.544      0.292      1.559440      1.490667
   24   1   9                    1->3->9    845.023      0.843      2.299080      2.179650
   25   1  10                   1->4->10   1147.010      1.148      1.618950      1.622114
   26   1  11               1->4->10->11    524.469      0.528      1.834690      1.798679
   27   1  12               1->4->10->12   1172.390      1.170      1.723830      1.828548
   28   1  13           1->4->10->11->13    188.123      0.188      1.921260      1.928144
   29   1  14           1->4->10->12->14    882.044      0.882      1.805370      1.929365
   30   1  15       1->4->10->12->14->15    362.496      0.363      3.835990      3.847460
   31   1  16           1->4->10->12->16    990.600      0.992      1.827620      1.884532
   32   2   0                       2->0    552.180      0.552      1.128910      1.067835
   33   2   1                       2->1    205.762      0.208      1.021700      0.874261
   34   2   3                    2->1->3   1261.990      1.264      1.112170      1.218837
   35   2   4                       2->4    894.711      0.891      0.181496      0.178421
   36   2   5                 2->0->8->5    311.665      0.313      3.157100      3.356666
   37   2   6              2->0->8->7->6    796.668      0.797      1.399760      1.468266
   38   2   7                 2->0->8->7    664.070      0.666      1.292960      1.337595
   39   2   8                    2->0->8    845.887      0.845      1.232070      1.243021
   40   2   9                    2->4->9    237.873      0.239      1.013660      1.062841
   41   2  10                   2->4->10    500.869      0.503      0.497609      0.509909
   42   2  11               2->4->10->11    336.106      0.338      0.635355      0.599329
   43   2  12               2->4->10->12    891.259      0.888      0.583549      0.673451
   44   2  13           2->4->10->11->13    247.171      0.247      0.707225      0.755306
   45   2  14           2->4->10->12->14    885.708      0.890      0.649384      0.734866
   46   2  15       2->4->10->12->14->15   1251.670      1.251      2.476790      2.636470
   47   2  16           2->4->10->12->16    692.698      0.699      0.637654      0.728359
   48   3   0                 3->1->2->0    828.425      0.830      1.918850      1.913755
   49   3   1                       3->1    150.515      0.151      0.779704      0.764972
   50   3   2                    3->1->2    450.810      0.454      1.168390      1.240068
   51   3   4                       3->4    567.009      0.567      0.657866      0.688218
   52   3   5                 3->4->8->5    255.229      0.255      2.497790      2.615319
   53   3   6             3->9->10->7->6   1161.060      1.156      2.972290      3.035761
   54   3   7                3->9->10->7    212.729      0.213      2.891630      3.049535
   55   3   8                    3->4->8    775.146      0.774      0.682807      0.661501
   56   3   9                       3->9    429.252      0.430      1.497300      1.495996
   57   3  10                   3->9->10   1155.740      1.161      2.532770      2.664309
   58   3  11               3->9->10->11    792.514      0.791      2.752120      2.833900
   59   3  12                   3->9->12    437.195      0.438      1.500410      1.618117
   60   3  13           3->9->10->11->13    887.828      0.887      2.776220      2.910205
   61   3  14               3->9->12->14    863.102      0.864      1.579910      1.646469
   62   3  15           3->9->12->14->15    444.552      0.447      3.521560      3.604943
   63   3  16               3->9->12->16    586.714      0.587      1.537230      1.649065
   64   4   0                    4->8->0    146.469      0.147      0.360391      0.343605
   65   4   1                       4->1    545.385      0.544      1.250980      1.268392
   66   4   2                       4->2    902.845      0.904      0.161785      0.179545
   67   4   3                       4->3    934.537      0.936      0.893575      0.915589
   68   4   5                    4->8->5   1246.070      1.245      1.915050      1.952405
   69   4   6                4->10->7->6    987.027      0.990      0.634357      0.642806
   70   4   7                   4->10->7    820.666      0.818      0.504941      0.510694
   71   4   8                       4->8    862.528      0.861      0.067914      0.073932
   72   4   9                       4->9   1212.960      1.209      0.756409      0.731662
   73   4  10                      4->10    654.485      0.654      0.305545      0.312184
   74   4  11                  4->10->11    931.661      0.930      0.353255      0.383305
   75   4  12                  4->10->12   1026.650      1.029      0.426672      0.490423
   76   4  13              4->10->11->13    572.555      0.572      0.520289      0.561226
   77   4  14              4->10->12->14    409.165      0.406      0.479640      0.570593
   78   4  15          4->10->12->14->15   1129.480      1.134      2.300460      2.420954
   79   4  16              4->10->12->16    818.681      0.823      0.484394      0.526654
   80   5   0                    5->8->0    704.299      0.707      1.746560      1.718979
   81   5   1                 5->8->4->1    388.018      0.387      2.827210      2.856526
   82   5   2                 5->8->0->2    609.497      0.609      2.404520      2.466447
   83   5   3                 5->8->4->3    669.773      0.670      2.451550      2.413039
   84   5   4                    5->8->4    475.667      0.477      1.700990      1.731977
   85   5   6                       5->6    457.115      0.456      0.710631      0.667779
   86   5   7                    5->6->7    848.175      0.843      0.878300      0.741466
   87   5   8                       5->8   1145.350      1.145      1.588930      1.595016
   88   5   9                 5->8->4->9    473.911      0.472      2.319780      2.323078
   89   5  10                5->8->4->10    540.779      0.543      1.962700      1.977040
   90   5  11            5->8->4->10->11    788.199      0.787      1.985620      2.060324
   91   5  12            5->8->4->10->12   1243.110      1.246      2.068080      2.106156
   92   5  13        5->8->4->10->11->13    630.739      0.627      2.092700      2.174102
   93   5  14        5->8->4->10->12->14    276.221      0.276      2.162890      2.261364
   94   5  15    5->8->4->10->12->14->15    632.852      0.632      4.051240      4.098865
   95   5  16        5->8->4->10->12->16    421.213      0.421      2.170770      2.205972
   96   6   0                 6->7->8->0    939.551      0.940      1.090300      1.111251
   97   6   1              6->7->8->4->1    227.058      0.226      2.316730      2.372610
   98   6   2              6->7->8->0->2    952.916      0.951      1.940170      1.894580
   99   6   3              6->7->8->4->3   1096.930      1.097      1.904770      1.900103
  100   6   4                 6->7->8->4   1128.280      1.128      1.124970      1.071713
  101   6   5                       6->5    699.235      0.696      0.546248      0.556950
  102   6   7                       6->7    270.972      0.270      0.323372      0.336468
  103   6   8                    6->7->8   1135.930      1.134      0.975884      0.927452
  104   6   9                6->7->10->9    781.868      0.782      1.029630      1.069777
  105   6  10                   6->7->10    168.575      0.170      0.382682      0.414103
  106   6  11               6->7->10->11    917.001      0.922      0.409174      0.442698
  107   6  12               6->7->10->12    612.674      0.613      0.539691      0.583182
  108   6  13           6->7->10->11->13   1123.100      1.126      0.566545      0.566099
  109   6  14           6->7->10->12->14    138.757      0.139      0.630059      0.695768
  110   6  15       6->7->10->12->14->15    148.751      0.148      2.555070      2.686801
  111   6  16           6->7->10->12->16   1225.200      1.224      0.538879      0.567148
  112   7   0                    7->8->0    326.628      0.326      0.785272      0.827277
  113   7   1                 7->8->4->1    702.506      0.703      1.896830      1.966417
  114   7   2                 7->8->0->2    367.311      0.368      1.687720      1.667160
  115   7   3                 7->8->4->3    326.469      0.325      1.538120      1.706277
  116   7   4                    7->8->4    367.260      0.367      0.855594      0.861748
  117   7   5                    7->8->5   1230.700      1.228      2.577650      2.486541
  118   7   6                       7->6    931.821      0.936      0.208407      0.218993
  119   7   8                       7->8   1023.750      1.022      0.709589      0.667953
  120   7   9                   7->10->9    844.141      0.844      0.861688      0.842735
  121   7  10                      7->10    929.098      0.928      0.089164      0.095537
  122   7  11                  7->10->11   1204.150      1.203      0.159197      0.176867
  123   7  12                  7->10->12    919.858      0.919      0.274085      0.306406
  124   7  13              7->10->11->13    810.317      0.806      0.318714      0.367522
  125   7  14              7->10->12->14   1227.610      1.229      0.320839      0.342492
  126   7  15          7->10->12->14->15    574.865      0.577      2.366530      2.364148
  127   7  16              7->10->12->16    338.168      0.340      0.373496      0.374853
  128   8   0                       8->0   1203.050      1.206      0.202117      0.208880
  129   8   1                    8->4->1    654.582      0.656      1.370910      1.422307
  130   8   2                    8->0->2    369.867      0.368      1.330450      1.218573
  131   8   3                    8->4->3    211.631      0.211      1.077100      1.190898
  132   8   4                       8->4    625.853      0.627      0.169809      0.167598
  133   8   5                       8->5   1141.250      1.141      1.866890      1.901417
  134   8   6                    8->7->6    569.340      0.570      0.358333      0.336750
  135   8   7                       8->7    748.968      0.748      0.147370      0.142458
  136   8   9                    8->4->9    232.834      0.232      1.080330      1.039075
  137   8  10                   8->4->10    679.626      0.681      0.438357      0.454637
  138   8  11               8->4->10->11    728.494      0.731      0.498305      0.536280
  139   8  12               8->4->10->12    949.192      0.948      0.562739      0.634856
  140   8  13           8->4->10->11->13    356.586      0.362      0.641478      0.713809
  141   8  14           8->4->10->12->14   1152.260      1.152      0.619423      0.671484
  142   8  15       8->4->10->12->14->15    144.552      0.145      2.474210      2.722085
  143   8  16           8->4->10->12->16   1138.660      1.141      0.593728      0.643876
  144   9   0                 9->4->8->0    579.314      0.578      1.130450      1.268777
  145   9   1                    9->4->1    495.839      0.495      2.319430      2.175719
  146   9   2                    9->4->2    861.721      0.859      1.125900      1.121288
  147   9   3                       9->3    496.149      0.494      1.116820      1.105670
  148   9   4                       9->4   1043.450      1.046      1.032990      1.010868
  149   9   5                 9->4->8->5    911.896      0.913      2.953310      2.866062
  150   9   6                9->10->7->6    921.247      0.921      1.665240      1.702506
  151   9   7                   9->10->7   1030.570      1.029      1.494950      1.566632
  152   9   8                    9->4->8    956.031      0.963      1.069230      1.045360
  153   9  10                      9->10    490.272      0.490      1.495390      1.477315
  154   9  11                  9->10->11    741.895      0.741      1.467210      1.484933
  155   9  12                      9->12    945.898      0.952      0.164024      0.159139
  156   9  13              9->10->11->13    615.952      0.618      1.568870      1.610384
  157   9  14                  9->12->14    867.576      0.867      0.241231      0.239027
  158   9  15              9->12->14->15    440.762      0.440      2.236790      2.267554
  159   9  16                  9->12->16   1072.440      1.069      0.194068      0.203781
  160  10   0                10->7->8->0    296.665      0.297      1.098990      1.003859
  161  10   1                   10->4->1    437.905      0.437      1.361950      1.379470
  162  10   2                   10->4->2    955.584      0.953      0.234807      0.238357
  163  10   3                   10->4->3    673.247      0.675      1.059150      1.059712
  164  10   4                      10->4    334.055      0.335      0.127080      0.119113
  165  10   5                10->7->8->5    323.301      0.321      2.855940      2.734712
  166  10   6                   10->7->6    541.158      0.541      0.392086      0.407657
  167  10   7                      10->7    165.106      0.165      0.266347      0.252574
  168  10   8                   10->7->8    977.023      0.981      0.912614      0.840320
  169  10   9                      10->9    386.267      0.382      0.766991      0.862705
  170  10  11                     10->11    773.123      0.771      0.112168      0.116817
  171  10  12                     10->12    182.134      0.181      0.226324      0.248257
  172  10  13                 10->11->13    465.600      0.468      0.321270      0.315791
  173  10  14                 10->12->14    903.096      0.905      0.262304      0.276226
  174  10  15             10->12->14->15    139.924      0.142      2.303470      2.245731
  175  10  16                 10->12->16    111.827      0.112      0.280320      0.294088
  176  11   0            11->10->7->8->0   1030.310      1.030      1.149070      1.083839
  177  11   1               11->10->4->1    943.766      0.937      1.410770      1.384318
  178  11   2               11->10->4->2    208.255      0.206      0.425505      0.389406
  179  11   3               11->10->4->3    799.802      0.801      1.045670      1.111950
  180  11   4                  11->10->4   1143.420      1.140      0.181082      0.179952
  181  11   5            11->10->7->8->5    379.056      0.380      2.906080      2.910760
  182  11   6               11->10->7->6    397.771      0.396      0.530675      0.544460
  183  11   7                  11->10->7    702.244      0.700      0.342025      0.347233
  184  11   8               11->10->7->8   1052.070      1.051      1.015420      0.956575
  185  11   9                  11->10->9    938.564      0.944      0.825261      0.829369
  186  11  10                     11->10    459.708      0.460      0.131475      0.130765
  187  11  12                 11->10->12    367.497      0.366      0.304056      0.358194
  188  11  13                     11->13    128.530      0.130      0.262066      0.270770
  189  11  14                 11->13->14   1111.330      1.108      1.682550      1.557132
  190  11  15             11->13->14->15   1262.440      1.259      3.429110      3.296027
  191  11  16             11->10->12->16   1002.250      1.003      0.329346      0.351082
  192  12   0            12->10->7->8->0    868.500      0.873      1.236430      1.162835
  193  12   1               12->10->4->1    605.537      0.603      1.587390      1.542539
  194  12   2               12->10->4->2   1067.520      1.069      0.392769      0.401136
  195  12   3                   12->9->3    595.678      0.595      1.159030      1.233797
  196  12   4                  12->10->4    413.914      0.413      0.298195      0.312362
  197  12   5            12->10->7->8->5    595.664      0.594      2.902220      2.930681
  198  12   6               12->10->7->6   1244.390      1.245      0.511382      0.550032
  199  12   7                  12->10->7   1037.000      1.037      0.406587      0.418210
  200  12   8               12->10->7->8   1017.850      1.017      1.076620      1.037778
  201  12   9                      12->9    596.075      0.599      0.194687      0.201067
  202  12  10                     12->10    583.298      0.588      0.212376      0.229209
  203  12  11                 12->10->11   1186.350      1.188      0.268443      0.296906
  204  12  13                 12->14->13    175.833      0.175      0.855749      0.924364
  205  12  14                     12->14    137.777      0.136      0.129220      0.132412
  206  12  15                 12->14->15   1029.580      1.027      1.999590      1.960819
  207  12  16                     12->16   1134.090      1.137      0.074346      0.074717
  208  13   0        13->11->10->7->8->0    230.598      0.229      1.351950      1.339637
  209  13   1           13->11->10->4->1   1065.930      1.065      1.480180      1.506173
  210  13   2           13->11->10->4->2    605.809      0.606      0.480967      0.494623
  211  13   3           13->14->12->9->3   1077.120      1.078      2.426650      2.325007
  212  13   4              13->11->10->4    952.127      0.957      0.336749      0.345719
  213  13   5        13->11->10->7->8->5    428.307      0.427      3.190900      3.105010
  214  13   6           13->11->10->7->6    186.644      0.185      0.697982      0.738564
  215  13   7              13->11->10->7    687.504      0.693      0.496266      0.519131
  216  13   8           13->11->10->7->8    646.771      0.646      1.214510      1.168586
  217  13   9              13->14->12->9    642.845      0.644      1.645560      1.710256
  218  13  10                 13->11->10    448.158      0.451      0.254691      0.315924
  219  13  11                     13->11   1078.550      1.079      0.181603      0.185773
  220  13  12                 13->14->12   1032.260      1.035      1.534470      1.511520
  221  13  14                     13->14    887.064      0.891      1.525140      1.489350
  222  13  15                 13->14->15   1036.490      1.035      3.327610      3.255559
  223  13  16             13->14->15->16    914.502      0.914      5.299230      5.285673
  224  14   0        14->12->10->7->8->0    553.605      0.552      1.346940      1.288051
  225  14   1           14->12->10->4->1    143.725      0.144      1.620630      1.693882
  226  14   2           14->12->10->4->2    554.881      0.555      0.507842      0.517854
  227  14   3               14->12->9->3   1027.370      1.022      1.260070      1.205088
  228  14   4              14->12->10->4    184.853      0.187      0.400260      0.414416
  229  14   5        14->12->10->7->8->5    670.482      0.672      3.062040      3.068603
  230  14   6           14->12->10->7->6    730.276      0.730      0.648103      0.675330
  231  14   7              14->12->10->7    684.878      0.685      0.499251      0.522044
  232  14   8           14->12->10->7->8   1237.740      1.237      1.146460      1.099213
  233  14   9                  14->12->9    664.123      0.664      0.261262      0.256849
  234  14  10                 14->12->10    616.590      0.616      0.311656      0.310680
  235  14  11                 14->13->11    308.304      0.310      0.851150      0.973704
  236  14  12                     14->12    903.155      0.904      0.078974      0.091520
  237  14  13                     14->13    759.215      0.758      0.749917      0.747065
  238  14  15                     14->15    567.689      0.570      1.961600      1.904467
  239  14  16                 14->15->16    308.602      0.310      3.887820      3.931292
  240  15   0    15->16->12->10->7->8->0   1167.140      1.170      3.171440      3.181376
  241  15   1       15->16->12->10->4->1    232.966      0.232      3.403680      3.638217
  242  15   2       15->16->12->10->4->2   1147.220      1.148      2.450780      2.402675
  243  15   3           15->16->12->9->3   1063.950      1.060      3.040150      3.106933
  244  15   4          15->16->12->10->4    585.660      0.585      2.476350      2.409244
  245  15   5    15->16->12->10->7->8->5    612.161      0.610      4.813290      5.064507
  246  15   6       15->16->12->10->7->6    922.733      0.924      2.575020      2.587657
  247  15   7          15->16->12->10->7    947.471      0.949      2.497850      2.532148
  248  15   8       15->16->12->10->7->8    864.045      0.864      3.079220      3.118678
  249  15   9              15->16->12->9    984.144      0.985      2.250450      2.259987
  250  15  10             15->16->12->10   1045.890      1.043      2.268550      2.343685
  251  15  11             15->14->13->11    400.315      0.402      1.333870      1.447592
  252  15  12                 15->16->12    799.937      0.802      2.097830      2.190701
  253  15  13                 15->14->13    781.134      0.779      1.123420      1.187191
  254  15  14                     15->14   1226.120      1.226      0.637199      0.646443
  255  15  16                     15->16   1132.940      1.130      2.060210      2.122818
  256  16   0        16->12->10->7->8->0   1176.980      1.174      1.274560      1.263525
  257  16   1           16->12->10->4->1    224.844      0.226      1.761710      1.704456
  258  16   2           16->12->10->4->2    841.559      0.839      0.490919      0.525938
  259  16   3               16->12->9->3   1214.000      1.210      1.187530      1.211617
  260  16   4              16->12->10->4    614.830      0.614      0.399252      0.417667
  261  16   5        16->12->10->7->8->5    963.350      0.959      3.099480      3.050241
  262  16   6           16->12->10->7->6    172.735      0.174      0.793020      0.754394
  263  16   7              16->12->10->7    269.490      0.270      0.544484      0.582828
  264  16   8           16->12->10->7->8    535.624      0.539      1.177060      1.208527
  265  16   9                  16->12->9    720.403      0.723      0.340180      0.294837
  266  16  10                 16->12->10    583.240      0.583      0.324412      0.344150
  267  16  11             16->12->10->11    902.361      0.904      0.386257      0.420654
  268  16  12                     16->12   1231.400      1.229      0.111935      0.123029
  269  16  13             16->12->14->13    211.276      0.211      1.020190      1.022692
  270  16  14                 16->12->14    496.268      0.498      0.223938      0.235817
  271  16  15                     16->15    426.713      0.428      0.656254      0.598470
