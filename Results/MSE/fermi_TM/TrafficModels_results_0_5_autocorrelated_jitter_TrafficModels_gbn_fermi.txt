2025-12-18 19:30:04.261368: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/spack/gilbreth-r9/apps/cuda/12.6.0-gcc-11.5.0-a7cv7sp/lib64:/apps/spack/gilbreth-r9/apps/libxml2/2.10.3-gcc-11.5.0-wroyiwb/lib:/apps/spack/gilbreth-r9/apps/libiconv/1.17-gcc-11.5.0-36njf44/lib:/apps/spack/gilbreth-r9/apps/libfabric/1.19.1-gcc-11.5.0-b5idhcg/lib64:/apps/spack/gilbreth-r9/apps/libfabric/1.19.1-gcc-11.5.0-b5idhcg/lib:/apps/spack/gilbreth-r9/apps/openmpi/4.1.6-gcc-11.5.0-w7wc45k/lib
2025-12-18 19:30:04.261840: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2025-12-18 19:30:07.036234: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2025-12-18 19:30:07.036302: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: gilbreth-fe03.rcac.purdue.edu
2025-12-18 19:30:07.036316: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: gilbreth-fe03.rcac.purdue.edu
2025-12-18 19:30:07.036389: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 565.57.1
2025-12-18 19:30:07.036417: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 565.57.1
2025-12-18 19:30:07.036426: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 565.57.1
2025-12-18 19:30:07.036799: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-12-18 19:30:11.915278: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
2025-12-18 19:30:14.210249: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:907] Skipping loop optimization for Merge node with control input: Assert_1/AssertGuard/branch_executed/_1027
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).path_update.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).path_update.recurrent_kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).path_update.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).link_update.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).link_update.recurrent_kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).link_update.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).queue_update.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).queue_update.recurrent_kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).queue_update.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).path_embedding.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).path_embedding.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).path_embedding.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).path_embedding.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).queue_embedding.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).queue_embedding.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).queue_embedding.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).queue_embedding.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).link_embedding.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).link_embedding.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).link_embedding.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).link_embedding.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).readout_path.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).readout_path.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).readout_path.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).readout_path.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).readout_path.layer_with_weights-2.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).readout_path.layer_with_weights-2.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).path_update.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).path_update.recurrent_kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).path_update.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).link_update.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).link_update.recurrent_kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).link_update.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).queue_update.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).queue_update.recurrent_kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).queue_update.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).path_embedding.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).path_embedding.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).path_embedding.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).path_embedding.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).queue_embedding.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).queue_embedding.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).queue_embedding.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).queue_embedding.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).link_embedding.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).link_embedding.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).link_embedding.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).link_embedding.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).readout_path.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).readout_path.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).readout_path.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).readout_path.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).readout_path.layer_with_weights-2.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).readout_path.layer_with_weights-2.bias
WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.

=== FERMI MODE B (metric=jitter) ===
Dataset directory : /scratch/gilbreth/mohame43/Routenet/RouteNet-Fermi/data/traffic_models/autocorrelated/test/gbn-autocorrelated
Checkpoint dir    : /scratch/gilbreth/mohame43/Routenet/RouteNet-Fermi/traffic_models/jitter/ckpt_dir_mape_autocorrelated
Sample index      : 0
Routes file       : /scratch/gilbreth/mohame43/Routenet/candidate_routes_0_5_test_gbn.txt

Loaded baseline sample (Fermi): n_paths = 272
BEST CHECKPOINT FOUND (Fermi): 30-24.19

====================================================================================================
[Fermi] Candidate 1: src=0, dst=5, path = 0->8->5
====================================================================================================
  idx src dst            route_nodes    traffic    packets   true_jitter_orig   pred_jitter
-------------------------------------------------------------------------------------------
    0   0   1                    0->2->1    945.860      0.949      0.028948      0.033621
    1   0   2                       0->2    752.327      0.753      0.014666      0.015473
    2   0   3                 0->2->1->3    878.467      0.870      0.053242      0.076331
    3   0   4                    0->8->4   1148.200      1.152      0.005518      0.007377
    4   0   5 *                  0->8->5    566.371      0.570      0.223400      0.226076
    5   0   6                 0->8->7->6    733.358      0.735      0.009569      0.009730
    6   0   7                    0->8->7   1196.560      1.194      0.005163      0.005126
    7   0   8                       0->8    225.218      0.227      0.002714      0.002661
    8   0   9                 0->8->4->9   1088.140      1.085      0.023000      0.023515
    9   0  10                0->8->4->10   1123.050      1.128      0.012245      0.013869
   10   0  11            0->8->4->10->11   1047.290      1.048      0.014282      0.017021
   11   0  12            0->8->4->10->12   1027.320      1.024      0.015410      0.017199
   12   0  13        0->8->4->10->11->13    859.786      0.863      0.019005      0.023235
   13   0  14        0->8->4->10->12->14   1217.410      1.214      0.017371      0.020150
   14   0  15    0->8->4->10->12->14->15    602.048      0.606      0.227553      0.296590
   15   0  16        0->8->4->10->12->16   1014.150      1.017      0.017497      0.020750
   16   1   0                    1->2->0    781.965      0.782      0.024193      0.026370
   17   1   2                       1->2    842.611      0.839      0.006410      0.006529
   18   1   3                       1->3    833.877      0.838      0.006405      0.006721
   19   1   4                       1->4    912.152      0.912      0.017211      0.020122
   20   1   5                 1->4->8->5    635.152      0.632      0.243253      0.336237
   21   1   6             1->4->10->7->6    197.707      0.198      0.038612      0.040536
   22   1   7                1->4->10->7    897.493      0.899      0.032454      0.033420
   23   1   8                    1->4->8    275.720      0.275      0.020808      0.020430
   24   1   9                    1->3->9    550.026      0.547      0.092639      0.120173
   25   1  10                   1->4->10    631.519      0.633      0.024723      0.019966
   26   1  11               1->4->10->11    245.646      0.244      0.028424      0.027726
   27   1  12               1->4->10->12    311.528      0.312      0.028421      0.027883
   28   1  13           1->4->10->11->13    423.179      0.419      0.034501      0.029415
   29   1  14           1->4->10->12->14    408.707      0.409      0.031308      0.029577
   30   1  15       1->4->10->12->14->15    251.131      0.254      0.242454      0.196660
   31   1  16           1->4->10->12->16    285.301      0.286      0.032166      0.032869
   32   2   0                       2->0    552.917      0.554      0.007735      0.007640
   33   2   1                       2->1    239.563      0.237      0.006584      0.007120
   34   2   3                    2->1->3    238.082      0.238      0.022826      0.024178
   35   2   4                       2->4    668.458      0.667      0.001198      0.001308
   36   2   5                 2->0->8->5    815.663      0.821      0.244700      0.240971
   37   2   6              2->0->8->7->6    176.842      0.174      0.028546      0.030703
   38   2   7                 2->0->8->7    262.554      0.265      0.020078      0.021355
   39   2   8                    2->0->8    261.008      0.264      0.014480      0.013995
   40   2   9                    2->4->9    601.563      0.604      0.018686      0.019189
   41   2  10                   2->4->10    920.187      0.924      0.007186      0.009364
   42   2  11               2->4->10->11    428.410      0.433      0.008941      0.010310
   43   2  12               2->4->10->12    235.524      0.236      0.009741      0.012348
   44   2  13           2->4->10->11->13   1202.830      1.197      0.013351      0.016150
   45   2  14           2->4->10->12->14    892.912      0.895      0.011919      0.013869
   46   2  15       2->4->10->12->14->15    952.672      0.952      0.215412      0.225686
   47   2  16           2->4->10->12->16    339.244      0.339      0.011566      0.015728
   48   3   0                 3->1->2->0    153.296      0.151      0.047939      0.049135
   49   3   1                       3->1    129.008      0.134      0.005380      0.005969
   50   3   2                    3->1->2    436.737      0.438      0.021558      0.021151
   51   3   4                       3->4   1241.540      1.237      0.007818      0.007963
   52   3   5                 3->4->8->5    793.554      0.791      0.239210      0.214025
   53   3   6             3->9->10->7->6    786.283      0.789      0.113390      0.186442
   54   3   7                3->9->10->7   1221.850      1.226      0.108273      0.138422
   55   3   8                    3->4->8   1107.810      1.100      0.010275      0.009896
   56   3   9                       3->9    469.109      0.470      0.077185      0.125369
   57   3  10                   3->9->10    580.096      0.585      0.098605      0.112492
   58   3  11               3->9->10->11    797.716      0.798      0.100278      0.162150
   59   3  12                   3->9->12    928.541      0.925      0.081036      0.095292
   60   3  13           3->9->10->11->13    704.746      0.704      0.106362      0.145013
   61   3  14               3->9->12->14    871.764      0.869      0.085254      0.123936
   62   3  15           3->9->12->14->15    820.646      0.824      0.301044      0.342048
   63   3  16               3->9->12->16    472.877      0.475      0.080268      0.129752
   64   4   0                    4->8->0    460.109      0.461      0.003176      0.003225
   65   4   1                       4->1    619.576      0.622      0.206424      0.286703
   66   4   2                       4->2    472.253      0.471      0.001090      0.001125
   67   4   3                       4->3    800.855      0.807      0.017247      0.016507
   68   4   5                    4->8->5    881.288      0.879      0.219991      0.273541
   69   4   6                4->10->7->6    623.013      0.624      0.011569      0.016033
   70   4   7                   4->10->7    555.375      0.551      0.009211      0.012310
   71   4   8                       4->8   1155.170      1.155      0.000405      0.000404
   72   4   9                       4->9    943.270      0.939      0.013455      0.011280
   73   4  10                      4->10   1184.210      1.184      0.004732      0.003058
   74   4  11                  4->10->11   1267.780      1.275      0.005883      0.004202
   75   4  12                  4->10->12   1125.460      1.127      0.006710      0.007556
   76   4  13              4->10->11->13    831.991      0.835      0.009278      0.011501
   77   4  14              4->10->12->14   1104.490      1.104      0.008206      0.008707
   78   4  15          4->10->12->14->15    786.855      0.785      0.211617      0.177031
   79   4  16              4->10->12->16    207.943      0.207      0.008115      0.009131
   80   5   0                    5->8->0    648.323      0.648      0.337558      0.406065
   81   5   1                 5->8->4->1   1117.370      1.123      0.557771      0.715290
   82   5   2                 5->8->0->2   1112.090      1.113      0.370181      0.443913
   83   5   3                 5->8->4->3    542.812      0.544      0.368764      0.412418
   84   5   4                    5->8->4    320.187      0.326      0.350367      0.526868
   85   5   6                       5->6    192.441      0.191      0.005091      0.004765
   86   5   7                    5->6->7    147.122      0.148      0.011192      0.006235
   87   5   8                       5->8    388.026      0.385      0.328173      0.487619
   88   5   9                 5->8->4->9   1183.900      1.193      0.357449      0.407875
   89   5  10                5->8->4->10    164.650      0.166      0.341242      0.525073
   90   5  11            5->8->4->10->11    843.842      0.843      0.349894      0.460525
   91   5  12            5->8->4->10->12    401.441      0.402      0.349740      0.423570
   92   5  13        5->8->4->10->11->13    828.951      0.834      0.352483      0.404532
   93   5  14        5->8->4->10->12->14    805.423      0.807      0.357401      0.409196
   94   5  15    5->8->4->10->12->14->15    480.017      0.486      0.559829      0.416411
   95   5  16        5->8->4->10->12->16    367.388      0.368      0.358908      0.388649
   96   6   0                 6->7->8->0   1219.120      1.212      0.154953      0.038475
   97   6   1              6->7->8->4->1    689.078      0.691      0.386479      0.398011
   98   6   2              6->7->8->0->2    418.589      0.420      0.178160      0.075346
   99   6   3              6->7->8->4->3    626.397      0.628      0.185176      0.065653
  100   6   4                 6->7->8->4    926.144      0.928      0.157769      0.041313
  101   6   5                       6->5    339.061      0.336      0.005014      0.005160
  102   6   7                       6->7    205.803      0.204      0.002289      0.002300
  103   6   8                    6->7->8    640.602      0.648      0.152410      0.023735
  104   6   9                6->7->10->9   1159.710      1.160      0.015367      0.013288
  105   6  10                   6->7->10    379.152      0.377      0.003588      0.003840
  106   6  11               6->7->10->11    431.991      0.430      0.005571      0.006093
  107   6  12               6->7->10->12    992.711      1.001      0.006191      0.007111
  108   6  13           6->7->10->11->13    568.576      0.570      0.009898      0.010157
  109   6  14           6->7->10->12->14   1084.230      1.084      0.008136      0.010061
  110   6  15       6->7->10->12->14->15   1137.640      1.134      0.212244      0.273110
  111   6  16           6->7->10->12->16   1046.150      1.044      0.008312      0.009827
  112   7   0                    7->8->0   1214.160      1.222      0.150697      0.017162
  113   7   1                 7->8->4->1    375.117      0.376      0.365409      0.436839
  114   7   2                 7->8->0->2    968.851      0.966      0.172729      0.048808
  115   7   3                 7->8->4->3    376.978      0.372      0.174341      0.061859
  116   7   4                    7->8->4    157.323      0.158      0.152472      0.037025
  117   7   5                    7->8->5    617.331      0.617      0.427076      0.207832
  118   7   6                       7->6    653.314      0.656      0.001463      0.001582
  119   7   8                       7->8    804.427      0.804      0.145834      0.014273
  120   7   9                   7->10->9    266.978      0.263      0.010128      0.010975
  121   7  10                      7->10    279.030      0.280      0.000501      0.000487
  122   7  11                  7->10->11    582.917      0.585      0.001651      0.001785
  123   7  12                  7->10->12    339.890      0.339      0.002363      0.003380
  124   7  13              7->10->11->13    684.323      0.686      0.005065      0.005800
  125   7  14              7->10->12->14   1204.880      1.210      0.003789      0.004692
  126   7  15          7->10->12->14->15    994.493      0.987      0.206962      0.212270
  127   7  16              7->10->12->16    222.694      0.224      0.003660      0.004456
  128   8   0                       8->0    803.873      0.800      0.001741      0.002169
  129   8   1                    8->4->1    466.724      0.465      0.210549      0.361574
  130   8   2                    8->0->2    245.742      0.245      0.021336      0.021679
  131   8   3                    8->4->3   1193.800      1.196      0.020616      0.019100
  132   8   4                       8->4   1035.390      1.030      0.001975      0.001440
  133   8   5                       8->5    800.305      0.803      0.216615      0.190476
  134   8   6                    8->7->6    687.144      0.686      0.003946      0.004513
  135   8   7                       8->7   1132.590      1.134      0.000980      0.000967
  136   8   9                    8->4->9   1232.210      1.233      0.017372      0.015318
  137   8  10                   8->4->10   1220.410      1.220      0.007829      0.008060
  138   8  11               8->4->10->11   1050.430      1.047      0.009347      0.010384
  139   8  12               8->4->10->12   1137.300      1.132      0.010091      0.010004
  140   8  13           8->4->10->11->13   1092.750      1.104      0.012802      0.014381
  141   8  14           8->4->10->12->14    142.179      0.143      0.011612      0.014794
  142   8  15       8->4->10->12->14->15    298.982      0.299      0.212317      0.256509
  143   8  16           8->4->10->12->16    680.467      0.679      0.011601      0.013765
  144   9   0                 9->4->8->0    857.637      0.864      0.019199      0.019505
  145   9   1                    9->4->1    290.034      0.285      0.225185      0.307966
  146   9   2                    9->4->2    344.602      0.344      0.015460      0.015384
  147   9   3                       9->3    382.558      0.386      0.009746      0.010122
  148   9   4                       9->4   1116.340      1.118      0.009189      0.009761
  149   9   5                 9->4->8->5   1233.170      1.233      0.242379      0.221004
  150   9   6                9->10->7->6   1008.710      1.016      0.024657      0.033290
  151   9   7                   9->10->7    221.838      0.223      0.019384      0.025460
  152   9   8                    9->4->8    396.733      0.395      0.013701      0.013897
  153   9  10                      9->10    189.183      0.189      0.013515      0.019100
  154   9  11                  9->10->11    139.611      0.141      0.017511      0.023778
  155   9  12                      9->12    299.572      0.296      0.001139      0.001255
  156   9  13              9->10->11->13    229.729      0.230      0.021897      0.028987
  157   9  14                  9->12->14    411.895      0.410      0.002835      0.002962
  158   9  15              9->12->14->15    763.475      0.769      0.206209      0.226008
  159   9  16                  9->12->16    968.170      0.966      0.002431      0.002655
  160  10   0                10->7->8->0    284.117      0.282      0.158425      0.042743
  161  10   1                   10->4->1    475.772      0.476      0.212926      0.342822
  162  10   2                   10->4->2    661.410      0.654      0.002598      0.002763
  163  10   3                   10->4->3    924.646      0.929      0.020417      0.020452
  164  10   4                      10->4    566.379      0.567      0.000677      0.000612
  165  10   5                10->7->8->5   1043.010      1.036      0.445098      0.229308
  166  10   6                   10->7->6    926.244      0.929      0.005613      0.007396
  167  10   7                      10->7   1028.830      1.022      0.003485      0.001808
  168  10   8                   10->7->8    552.056      0.559      0.156978      0.030963
  169  10   9                      10->9    448.537      0.441      0.006949      0.006776
  170  10  11                     10->11    357.087      0.354      0.000599      0.000583
  171  10  12                     10->12    181.192      0.177      0.001144      0.001610
  172  10  13                 10->11->13    215.275      0.216      0.003199      0.003545
  173  10  14                 10->12->14    481.039      0.482      0.002427      0.003104
  174  10  15             10->12->14->15   1237.350      1.233      0.203789      0.181869
  175  10  16                 10->12->16    170.598      0.170      0.002165      0.003211
  176  11   0            11->10->7->8->0    707.253      0.714      0.161454      0.042763
  177  11   1               11->10->4->1    915.432      0.912      0.215536      0.373092
  178  11   2               11->10->4->2    279.790      0.277      0.004618      0.005662
  179  11   3               11->10->4->3   1225.140      1.229      0.023931      0.023752
  180  11   4                  11->10->4   1165.410      1.167      0.001862      0.001806
  181  11   5            11->10->7->8->5    647.563      0.654      0.452593      0.282699
  182  11   6               11->10->7->6    653.519      0.656      0.007723      0.009965
  183  11   7                  11->10->7    587.074      0.586      0.004689      0.003688
  184  11   8               11->10->7->8    923.484      0.921      0.157688      0.041143
  185  11   9                  11->10->9    503.580      0.504      0.009706      0.009688
  186  11  10                     11->10    858.410      0.859      0.000667      0.000715
  187  11  12                 11->10->12    309.641      0.313      0.002556      0.003441
  188  11  13                     11->13   1225.990      1.230      0.001648      0.001768
  189  11  14                 11->13->14    815.593      0.806      0.018775      0.023301
  190  11  15             11->13->14->15    654.585      0.653      0.235075      0.185224
  191  11  16             11->10->12->16    787.668      0.790      0.003942      0.004209
  192  12   0            12->10->7->8->0   1170.570      1.174      0.164489      0.048598
  193  12   1               12->10->4->1    308.279      0.310      0.220700      0.370586
  194  12   2               12->10->4->2    592.384      0.587      0.005623      0.006411
  195  12   3                   12->9->3    617.978      0.616      0.014505      0.016021
  196  12   4                  12->10->4   1109.830      1.104      0.002799      0.002891
  197  12   5            12->10->7->8->5   1245.650      1.233      0.446191      0.294413
  198  12   6               12->10->7->6    539.951      0.538      0.008954      0.011190
  199  12   7                  12->10->7    342.698      0.339      0.006331      0.004857
  200  12   8               12->10->7->8   1197.540      1.195      0.160246      0.040425
  201  12   9                      12->9   1212.180      1.214      0.001212      0.001232
  202  12  10                     12->10    648.179      0.649      0.001562      0.001616
  203  12  11                 12->10->11    397.759      0.395      0.002772      0.002562
  204  12  13                 12->14->13    167.934      0.168      0.016600      0.015794
  205  12  14                     12->14    620.183      0.622      0.000749      0.000759
  206  12  15                 12->14->15    418.912      0.418      0.204137      0.220710
  207  12  16                     12->16    421.340      0.420      0.000405      0.000396
  208  13   0        13->11->10->7->8->0    139.257      0.140      0.171952      0.073610
  209  13   1           13->11->10->4->1    839.637      0.839      0.220754      0.288557
  210  13   2           13->11->10->4->2   1263.990      1.268      0.008958      0.010076
  211  13   3           13->14->12->9->3    319.834      0.317      0.046638      0.048321
  212  13   4              13->11->10->4    689.078      0.691      0.005001      0.005561
  213  13   5        13->11->10->7->8->5    201.493      0.204      0.459852      0.334887
  214  13   6           13->11->10->7->6    462.353      0.460      0.012350      0.012649
  215  13   7              13->11->10->7    899.834      0.891      0.007940      0.010125
  216  13   8           13->11->10->7->8    886.052      0.894      0.163434      0.045156
  217  13   9              13->14->12->9    617.100      0.618      0.022812      0.026874
  218  13  10                 13->11->10    461.192      0.458      0.003108      0.003241
  219  13  11                     13->11    605.039      0.606      0.001505      0.001718
  220  13  12                 13->14->12   1070.730      1.082      0.015684      0.017532
  221  13  14                     13->14    177.825      0.179      0.014622      0.016290
  222  13  15                 13->14->15    532.973      0.532      0.220882      0.190472
  223  13  16             13->14->15->16   1269.380      1.262      0.779123      0.742541
  224  14   0        14->12->10->7->8->0   1170.350      1.168      0.165641      0.064661
  225  14   1           14->12->10->4->1   1268.010      1.269      0.220140      0.299277
  226  14   2           14->12->10->4->2    249.192      0.250      0.007687      0.008959
  227  14   3               14->12->9->3    394.672      0.398      0.018326      0.019410
  228  14   4              14->12->10->4    195.082      0.195      0.004433      0.004810
  229  14   5        14->12->10->7->8->5   1146.150      1.143      0.450805      0.303051
  230  14   6           14->12->10->7->6    569.663      0.566      0.011113      0.011588
  231  14   7              14->12->10->7    984.122      0.986      0.007639      0.007061
  232  14   8           14->12->10->7->8    325.746      0.324      0.160423      0.047590
  233  14   9                  14->12->9    480.855      0.478      0.002753      0.002775
  234  14  10                 14->12->10    400.532      0.402      0.002844      0.002943
  235  14  11                 14->13->11   1212.190      1.212      0.016817      0.015437
  236  14  12                     14->12   1119.100      1.126      0.000472      0.000460
  237  14  13                     14->13    996.384      0.992      0.011705      0.010457
  238  14  15                     14->15   1238.380      1.240      0.196633      0.166610
  239  14  16                 14->15->16    649.052      0.644      0.742367      0.523166
  240  15   0    15->16->12->10->7->8->0    461.650      0.464      0.715112      0.417208
  241  15   1       15->16->12->10->4->1    616.903      0.609      0.741555      0.567661
  242  15   2       15->16->12->10->4->2    827.253      0.823      0.533863      0.436297
  243  15   3           15->16->12->9->3   1159.370      1.157      0.554413      0.394607
  244  15   4          15->16->12->10->4    759.419      0.757      0.531883      0.439107
  245  15   5    15->16->12->10->7->8->5    806.449      0.807      1.000250      0.582805
  246  15   6       15->16->12->10->7->6    598.585      0.596      0.542476      0.398835
  247  15   7          15->16->12->10->7    435.331      0.440      0.544074      0.410386
  248  15   8       15->16->12->10->7->8    588.506      0.592      0.706668      0.398876
  249  15   9              15->16->12->9    717.528      0.714      0.529697      0.426452
  250  15  10             15->16->12->10    557.449      0.557      0.526701      0.509002
  251  15  11             15->14->13->11    994.423      0.992      0.035887      0.037860
  252  15  12                 15->16->12   1188.900      1.192      0.521021      0.469926
  253  15  13                 15->14->13    183.301      0.183      0.029296      0.032009
  254  15  14                     15->14    649.283      0.640      0.006403      0.006511
  255  15  16                     15->16    530.724      0.529      0.529101      0.384178
  256  16   0        16->12->10->7->8->0    915.244      0.911      0.168097      0.065466
  257  16   1           16->12->10->4->1    949.759      0.947      0.223618      0.335213
  258  16   2           16->12->10->4->2    519.895      0.520      0.007897      0.008535
  259  16   3               16->12->9->3   1150.860      1.142      0.017511      0.018697
  260  16   4              16->12->10->4    452.462      0.453      0.004659      0.005146
  261  16   5        16->12->10->7->8->5    939.388      0.944      0.448423      0.312408
  262  16   6           16->12->10->7->6    553.593      0.557      0.011313      0.011512
  263  16   7              16->12->10->7   1176.790      1.188      0.008033      0.008315
  264  16   8           16->12->10->7->8    591.427      0.591      0.161810      0.045022
  265  16   9                  16->12->9    522.052      0.521      0.003007      0.002980
  266  16  10                 16->12->10    449.847      0.449      0.003036      0.002756
  267  16  11             16->12->10->11    264.288      0.260      0.004704      0.005114
  268  16  12                     16->12    928.720      0.929      0.000776      0.001038
  269  16  13             16->12->14->13   1213.710      1.217      0.019715      0.019329
  270  16  14                 16->12->14    873.951      0.872      0.002121      0.002319
  271  16  15                     16->15    806.318      0.805      0.004902      0.005024
