2025-12-18 19:30:39.301047: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/spack/gilbreth-r9/apps/cuda/12.6.0-gcc-11.5.0-a7cv7sp/lib64:/apps/spack/gilbreth-r9/apps/libxml2/2.10.3-gcc-11.5.0-wroyiwb/lib:/apps/spack/gilbreth-r9/apps/libiconv/1.17-gcc-11.5.0-36njf44/lib:/apps/spack/gilbreth-r9/apps/libfabric/1.19.1-gcc-11.5.0-b5idhcg/lib64:/apps/spack/gilbreth-r9/apps/libfabric/1.19.1-gcc-11.5.0-b5idhcg/lib:/apps/spack/gilbreth-r9/apps/openmpi/4.1.6-gcc-11.5.0-w7wc45k/lib
2025-12-18 19:30:39.303196: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2025-12-18 19:30:41.829937: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2025-12-18 19:30:41.830014: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: gilbreth-fe03.rcac.purdue.edu
2025-12-18 19:30:41.830028: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: gilbreth-fe03.rcac.purdue.edu
2025-12-18 19:30:41.830110: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 565.57.1
2025-12-18 19:30:41.830146: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 565.57.1
2025-12-18 19:30:41.830159: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 565.57.1
2025-12-18 19:30:41.830578: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-12-18 19:30:46.303648: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
2025-12-18 19:30:46.768579: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:907] Skipping loop optimization for Merge node with control input: Assert/AssertGuard/branch_executed/_1017
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).path_update.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).path_update.recurrent_kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).path_update.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).link_update.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).link_update.recurrent_kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).link_update.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).queue_update.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).queue_update.recurrent_kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).queue_update.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).path_embedding.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).path_embedding.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).path_embedding.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).path_embedding.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).queue_embedding.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).queue_embedding.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).queue_embedding.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).queue_embedding.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).link_embedding.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).link_embedding.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).link_embedding.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).link_embedding.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).readout_path.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).readout_path.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).readout_path.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).readout_path.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).readout_path.layer_with_weights-2.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).readout_path.layer_with_weights-2.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).path_update.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).path_update.recurrent_kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).path_update.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).link_update.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).link_update.recurrent_kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).link_update.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).queue_update.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).queue_update.recurrent_kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).queue_update.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).path_embedding.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).path_embedding.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).path_embedding.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).path_embedding.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).queue_embedding.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).queue_embedding.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).queue_embedding.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).queue_embedding.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).link_embedding.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).link_embedding.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).link_embedding.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).link_embedding.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).readout_path.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).readout_path.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).readout_path.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).readout_path.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).readout_path.layer_with_weights-2.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).readout_path.layer_with_weights-2.bias
WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.

=== FERMI MODE B (metric=jitter) ===
Dataset directory : /scratch/gilbreth/mohame43/Routenet/RouteNet-Fermi/data/traffic_models/onoff/test/gbn-onoff
Checkpoint dir    : /scratch/gilbreth/mohame43/Routenet/RouteNet-Fermi/traffic_models/jitter/ckpt_dir_mape_onoff
Sample index      : 0
Routes file       : /scratch/gilbreth/mohame43/Routenet/candidate_routes_0_5_test_gbn.txt

Loaded baseline sample (Fermi): n_paths = 272
BEST CHECKPOINT FOUND (Fermi): 30-30.34

====================================================================================================
[Fermi] Candidate 1: src=0, dst=5, path = 0->8->5
====================================================================================================
  idx src dst            route_nodes    traffic    packets   true_jitter_orig   pred_jitter
-------------------------------------------------------------------------------------------
    0   0   1                    0->2->1   1020.080      1.023      0.027195      0.027457
    1   0   2                       0->2   1169.700      1.167      0.011116      0.013202
    2   0   3                 0->2->1->3    677.681      0.680      0.051799      0.045125
    3   0   4                    0->8->4    634.639      0.634      0.004875      0.005727
    4   0   5 *                  0->8->5   1207.130      1.211      0.518646      0.386354
    5   0   6                 0->8->7->6    503.704      0.505      0.009440      0.008856
    6   0   7                    0->8->7   1118.830      1.116      0.004839      0.004961
    7   0   8                       0->8    739.637      0.740      0.002533      0.002188
    8   0   9                 0->8->4->9   1251.210      1.250      0.017736      0.019107
    9   0  10                0->8->4->10    575.625      0.581      0.027079      0.021614
   10   0  11            0->8->4->10->11    218.669      0.219      0.029308      0.021358
   11   0  12            0->8->4->10->12   1319.460      1.320      0.031985      0.022222
   12   0  13        0->8->4->10->11->13   1190.810      1.193      0.035452      0.028476
   13   0  14        0->8->4->10->12->14    736.904      0.740      0.033134      0.028823
   14   0  15    0->8->4->10->12->14->15    728.946      0.729      0.487906      0.516789
   15   0  16        0->8->4->10->12->16    963.588      0.967      0.034831      0.028290
   16   1   0                    1->2->0    911.111      0.913      0.027623      0.025277
   17   1   2                       1->2    824.223      0.825      0.006955      0.006536
   18   1   3                       1->3    792.981      0.797      0.007633      0.008283
   19   1   4                       1->4    741.083      0.738      0.353103      0.561991
   20   1   5                 1->4->8->5   1082.760      1.083      0.926253      0.836538
   21   1   6             1->4->10->7->6    436.788      0.440      0.388186      0.510301
   22   1   7                1->4->10->7    216.906      0.217      0.366277      0.488945
   23   1   8                    1->4->8    370.876      0.371      0.338662      0.578706
   24   1   9                    1->3->9    890.239      0.895      0.367947      0.330462
   25   1  10                   1->4->10   1251.410      1.251      0.399323      0.562972
   26   1  11               1->4->10->11    606.476      0.607      0.385703      0.397831
   27   1  12               1->4->10->12   1228.810      1.232      0.406636      0.416132
   28   1  13           1->4->10->11->13    264.758      0.262      0.374057      0.425308
   29   1  14           1->4->10->12->14    938.004      0.939      0.401288      0.448845
   30   1  15       1->4->10->12->14->15    440.481      0.439      0.867133      0.980777
   31   1  16           1->4->10->12->16   1063.110      1.069      0.409167      0.396558
   32   2   0                       2->0    612.504      0.613      0.011615      0.012661
   33   2   1                       2->1    238.000      0.236      0.007800      0.008895
   34   2   3                    2->1->3   1304.850      1.308      0.022684      0.020948
   35   2   4                       2->4    941.425      0.944      0.001195      0.001140
   36   2   5                 2->0->8->5    378.662      0.379      0.593803      0.476714
   37   2   6              2->0->8->7->6    872.067      0.874      0.031030      0.038650
   38   2   7                 2->0->8->7    721.813      0.728      0.023393      0.022137
   39   2   8                    2->0->8    894.160      0.894      0.017312      0.015500
   40   2   9                    2->4->9    325.506      0.326      0.013582      0.013663
   41   2  10                   2->4->10    563.162      0.561      0.023504      0.010595
   42   2  11               2->4->10->11    426.269      0.425      0.025016      0.018240
   43   2  12               2->4->10->12    953.041      0.956      0.027486      0.017848
   44   2  13           2->4->10->11->13    306.753      0.307      0.029566      0.027103
   45   2  14           2->4->10->12->14    974.209      0.973      0.029231      0.020384
   46   2  15       2->4->10->12->14->15   1335.650      1.339      0.455155      0.287792
   47   2  16           2->4->10->12->16    780.541      0.778      0.028022      0.022014
   48   3   0                 3->1->2->0    892.988      0.892      0.050588      0.045925
   49   3   1                       3->1    217.253      0.217      0.006240      0.005011
   50   3   2                    3->1->2    513.783      0.514      0.022321      0.020011
   51   3   4                       3->4    601.213      0.601      0.005883      0.005479
   52   3   5                 3->4->8->5    330.430      0.330      0.604476      0.426372
   53   3   6             3->9->10->7->6   1216.520      1.214      0.883060      0.871608
   54   3   7                3->9->10->7    266.123      0.266      0.804226      0.828982
   55   3   8                    3->4->8    852.634      0.861      0.008557      0.009460
   56   3   9                       3->9    504.097      0.501      0.331946      0.336727
   57   3  10                   3->9->10   1246.430      1.248      0.862535      0.816813
   58   3  11               3->9->10->11    846.553      0.850      0.865729      0.797773
   59   3  12                   3->9->12    514.430      0.516      0.326123      0.349468
   60   3  13           3->9->10->11->13    961.002      0.961      0.861983      0.792551
   61   3  14               3->9->12->14    936.669      0.934      0.361193      0.360648
   62   3  15           3->9->12->14->15    527.009      0.524      0.813347      0.764273
   63   3  16               3->9->12->16    644.486      0.648      0.346980      0.323551
   64   4   0                    4->8->0    219.318      0.217      0.003120      0.004211
   65   4   1                       4->1    615.579      0.614      0.036637      0.051712
   66   4   2                       4->2    977.637      0.974      0.001206      0.001385
   67   4   3                       4->3   1002.550      1.009      0.010560      0.011604
   68   4   5                    4->8->5   1296.590      1.298      0.518076      0.417519
   69   4   6                4->10->7->6   1058.970      1.060      0.028724      0.018817
   70   4   7                   4->10->7    868.899      0.866      0.024523      0.013101
   71   4   8                       4->8    944.541      0.944      0.000410      0.000507
   72   4   9                       4->9   1296.930      1.294      0.007299      0.008335
   73   4  10                      4->10    715.862      0.718      0.020744      0.007127
   74   4  11                  4->10->11    994.467      0.995      0.022966      0.013668
   75   4  12                  4->10->12   1075.400      1.075      0.024925      0.014536
   76   4  13              4->10->11->13    653.211      0.652      0.024940      0.016927
   77   4  14              4->10->12->14    483.597      0.482      0.025036      0.014195
   78   4  15          4->10->12->14->15   1216.890      1.221      0.440043      0.395481
   79   4  16              4->10->12->16    877.837      0.874      0.024946      0.015580
   80   5   0                    5->8->0    771.288      0.768      1.108890      0.869998
   81   5   1                 5->8->4->1    463.830      0.463      1.139740      0.820011
   82   5   2                 5->8->0->2    684.641      0.689      1.127150      0.983238
   83   5   3                 5->8->4->3    748.567      0.751      1.114600      0.738361
   84   5   4                    5->8->4    550.860      0.549      1.089380      0.896640
   85   5   6                       5->6    512.081      0.517      0.005593      0.002886
   86   5   7                    5->6->7    909.730      0.911      0.011483      0.009850
   87   5   8                       5->8   1197.070      1.193      1.087080      1.047920
   88   5   9                 5->8->4->9    539.186      0.540      1.110860      0.742881
   89   5  10                5->8->4->10    608.809      0.607      1.139320      0.834424
   90   5  11            5->8->4->10->11    841.839      0.845      1.136230      0.823039
   91   5  12            5->8->4->10->12   1312.530      1.314      1.129360      0.803974
   92   5  13        5->8->4->10->11->13    684.318      0.684      1.139920      0.742258
   93   5  14        5->8->4->10->12->14    356.800      0.357      1.135940      0.804404
   94   5  15    5->8->4->10->12->14->15    702.846      0.699      1.578140      1.321140
   95   5  16        5->8->4->10->12->16    495.965      0.498      1.136920      0.752682
   96   6   0                 6->7->8->0   1008.380      1.001      0.143062      0.043768
   97   6   1              6->7->8->4->1    307.941      0.307      0.197213      0.159067
   98   6   2              6->7->8->0->2   1004.020      0.999      0.166891      0.069537
   99   6   3              6->7->8->4->3   1166.950      1.164      0.160314      0.065197
  100   6   4                 6->7->8->4   1186.650      1.186      0.141705      0.048867
  101   6   5                       6->5    786.172      0.784      0.004908      0.004910
  102   6   7                       6->7    336.430      0.341      0.002336      0.001918
  103   6   8                    6->7->8   1201.370      1.202      0.140383      0.032788
  104   6   9                6->7->10->9    845.511      0.845      0.016787      0.018076
  105   6  10                   6->7->10    229.455      0.231      0.003769      0.004776
  106   6  11               6->7->10->11    992.855      0.995      0.005760      0.007121
  107   6  12               6->7->10->12    670.988      0.670      0.006887      0.007995
  108   6  13           6->7->10->11->13   1199.390      1.198      0.010047      0.009070
  109   6  14           6->7->10->12->14    212.467      0.213      0.009201      0.010683
  110   6  15       6->7->10->12->14->15    208.437      0.207      0.468653      0.407586
  111   6  16           6->7->10->12->16   1290.270      1.291      0.008670      0.008606
  112   7   0                    7->8->0    391.467      0.391      0.142426      0.036317
  113   7   1                 7->8->4->1    760.676      0.761      0.183102      0.134965
  114   7   2                 7->8->0->2    426.669      0.423      0.162685      0.043778
  115   7   3                 7->8->4->3    392.272      0.394      0.163539      0.090485
  116   7   4                    7->8->4    453.944      0.454      0.143048      0.038168
  117   7   5                    7->8->5   1288.400      1.286      0.710236      0.610842
  118   7   6                       7->6    993.130      0.994      0.001566      0.001342
  119   7   8                       7->8   1086.200      1.093      0.136635      0.034614
  120   7   9                   7->10->9    906.067      0.909      0.010016      0.010931
  121   7  10                      7->10   1005.970      1.003      0.000530      0.000732
  122   7  11                  7->10->11   1255.710      1.253      0.001812      0.002936
  123   7  12                  7->10->12    983.200      0.981      0.002779      0.003851
  124   7  13              7->10->11->13    883.527      0.881      0.005056      0.005070
  125   7  14              7->10->12->14   1322.920      1.312      0.004183      0.005342
  126   7  15          7->10->12->14->15    650.718      0.646      0.452084      0.416404
  127   7  16              7->10->12->16    452.693      0.452      0.004252      0.004622
  128   8   0                       8->0   1282.990      1.283      0.001518      0.002238
  129   8   1                    8->4->1    723.574      0.722      0.040936      0.062801
  130   8   2                    8->0->2    457.388      0.459      0.017527      0.019315
  131   8   3                    8->4->3    298.139      0.298      0.015524      0.014674
  132   8   4                       8->4    697.106      0.699      0.001233      0.001304
  133   8   5                       8->5   1212.810      1.214      0.522794      0.594443
  134   8   6                    8->7->6    620.888      0.630      0.004196      0.004256
  135   8   7                       8->7    825.193      0.823      0.000992      0.001110
  136   8   9                    8->4->9    275.720      0.274      0.012503      0.010760
  137   8  10                   8->4->10    744.313      0.742      0.023338      0.013215
  138   8  11               8->4->10->11    801.327      0.802      0.025797      0.015927
  139   8  12               8->4->10->12   1018.420      1.019      0.027513      0.019292
  140   8  13           8->4->10->11->13    443.434      0.446      0.027996      0.023583
  141   8  14           8->4->10->12->14   1243.230      1.241      0.029106      0.021366
  142   8  15       8->4->10->12->14->15    222.102      0.220      0.510842      0.358018
  143   8  16           8->4->10->12->16   1193.370      1.192      0.028907      0.021432
  144   9   0                 9->4->8->0    635.346      0.639      0.020910      0.014607
  145   9   1                    9->4->1    574.865      0.576      0.057781      0.073840
  146   9   2                    9->4->2    915.516      0.916      0.016286      0.015472
  147   9   3                       9->3    553.800      0.552      0.012534      0.013895
  148   9   4                       9->4   1127.030      1.128      0.010913      0.010883
  149   9   5                 9->4->8->5    983.416      0.982      0.554017      0.361248
  150   9   6                9->10->7->6   1008.010      1.005      0.377925      0.419360
  151   9   7                   9->10->7   1104.210      1.101      0.381612      0.437878
  152   9   8                    9->4->8   1056.120      1.048      0.013905      0.014658
  153   9  10                      9->10    545.844      0.551      0.355787      0.374472
  154   9  11                  9->10->11    801.395      0.798      0.380312      0.427763
  155   9  12                      9->12   1010.740      1.011      0.001107      0.001202
  156   9  13              9->10->11->13    680.395      0.681      0.363817      0.332284
  157   9  14                  9->12->14    940.018      0.939      0.002706      0.002749
  158   9  15              9->12->14->15    509.574      0.511      0.443302      0.457030
  159   9  16                  9->12->16   1112.780      1.115      0.002517      0.002631
  160  10   0                10->7->8->0    326.562      0.327      0.152891      0.049549
  161  10   1                   10->4->1    511.911      0.515      0.039551      0.064685
  162  10   2                   10->4->2   1019.600      1.018      0.002720      0.003199
  163  10   3                   10->4->3    731.825      0.729      0.014011      0.013913
  164  10   4                      10->4    403.716      0.403      0.000625      0.000958
  165  10   5                10->7->8->5    388.676      0.388      0.779858      0.505099
  166  10   6                   10->7->6    604.146      0.605      0.005309      0.004683
  167  10   7                      10->7    224.558      0.227      0.002935      0.001464
  168  10   8                   10->7->8   1061.990      1.069      0.144678      0.032294
  169  10   9                      10->9    440.876      0.443      0.007713      0.008013
  170  10  11                     10->11    834.965      0.831      0.000720      0.001025
  171  10  12                     10->12    247.469      0.249      0.001697      0.002657
  172  10  13                 10->11->13    522.095      0.528      0.003245      0.004061
  173  10  14                 10->12->14    973.967      0.976      0.002595      0.003989
  174  10  15             10->12->14->15    213.586      0.213      0.465069      0.468025
  175  10  16                 10->12->16    204.502      0.204      0.002657      0.004316
  176  11   0            11->10->7->8->0   1090.260      1.093      0.150529      0.068142
  177  11   1               11->10->4->1   1007.850      1.009      0.044243      0.058323
  178  11   2               11->10->4->2    271.458      0.271      0.004755      0.004932
  179  11   3               11->10->4->3    880.379      0.882      0.017167      0.019674
  180  11   4                  11->10->4   1207.650      1.205      0.001799      0.003141
  181  11   5            11->10->7->8->5    445.423      0.446      0.792703      0.589122
  182  11   6               11->10->7->6    465.720      0.463      0.007548      0.007684
  183  11   7                  11->10->7    755.216      0.761      0.004242      0.003661
  184  11   8               11->10->7->8   1111.860      1.107      0.146441      0.036572
  185  11   9                  11->10->9   1007.840      1.008      0.010077      0.011123
  186  11  10                     11->10    537.269      0.534      0.000666      0.000854
  187  11  12                 11->10->12    423.490      0.420      0.002912      0.004720
  188  11  13                     11->13    210.339      0.208      0.001686      0.001546
  189  11  14                 11->13->14   1191.440      1.184      0.401462      0.376131
  190  11  15             11->13->14->15   1321.530      1.328      0.917269      0.673823
  191  11  16             11->10->12->16   1061.090      1.064      0.004258      0.005139
  192  12   0            12->10->7->8->0    920.506      0.921      0.157623      0.067142
  193  12   1               12->10->4->1    657.765      0.663      0.044160      0.057470
  194  12   2               12->10->4->2   1138.730      1.141      0.005661      0.008288
  195  12   3                   12->9->3    665.876      0.660      0.017465      0.018725
  196  12   4                  12->10->4    452.434      0.458      0.002906      0.004113
  197  12   5            12->10->7->8->5    663.281      0.667      0.765526      0.579200
  198  12   6               12->10->7->6   1301.260      1.302      0.008669      0.010598
  199  12   7                  12->10->7   1114.100      1.112      0.005610      0.004915
  200  12   8               12->10->7->8   1071.030      1.070      0.151572      0.038620
  201  12   9                      12->9    646.011      0.651      0.001422      0.001348
  202  12  10                     12->10    659.713      0.662      0.001712      0.001964
  203  12  11                 12->10->11   1253.850      1.254      0.002935      0.003986
  204  12  13                 12->14->13    257.981      0.257      0.011036      0.011354
  205  12  14                     12->14    207.000      0.208      0.000663      0.000788
  206  12  15                 12->14->15   1087.420      1.089      0.409467      0.396860
  207  12  16                     12->16   1198.890      1.204      0.000447      0.000585
  208  13   0        13->11->10->7->8->0    298.665      0.298      0.164182      0.066518
  209  13   1           13->11->10->4->1   1116.750      1.116      0.050522      0.077261
  210  13   2           13->11->10->4->2    683.225      0.680      0.009077      0.013447
  211  13   3           13->14->12->9->3   1154.810      1.149      0.422103      0.434366
  212  13   4              13->11->10->4   1055.930      1.051      0.004831      0.006263
  213  13   5        13->11->10->7->8->5    493.497      0.492      0.791816      0.444253
  214  13   6           13->11->10->7->6    258.383      0.260      0.011906      0.016577
  215  13   7              13->11->10->7    749.874      0.750      0.007393      0.008115
  216  13   8           13->11->10->7->8    714.886      0.718      0.155652      0.057748
  217  13   9              13->14->12->9    705.523      0.700      0.392377      0.380436
  218  13  10                 13->11->10    517.486      0.519      0.003007      0.003152
  219  13  11                     13->11   1151.630      1.145      0.001323      0.001177
  220  13  12                 13->14->12   1103.120      1.108      0.391789      0.281489
  221  13  14                     13->14    962.872      0.966      0.385055      0.338389
  222  13  15                 13->14->15   1083.050      1.083      0.940634      0.877880
  223  13  16             13->14->15->16    970.472      0.979      1.205900      0.949534
  224  14   0        14->12->10->7->8->0    622.393      0.625      0.158295      0.061224
  225  14   1           14->12->10->4->1    205.839      0.205      0.045781      0.066074
  226  14   2           14->12->10->4->2    628.388      0.629      0.008170      0.009846
  227  14   3               14->12->9->3   1088.460      1.084      0.020322      0.023267
  228  14   4              14->12->10->4    247.772      0.246      0.004641      0.005843
  229  14   5        14->12->10->7->8->5    762.409      0.758      0.759426      0.499473
  230  14   6           14->12->10->7->6    792.239      0.793      0.011005      0.014536
  231  14   7              14->12->10->7    760.106      0.762      0.007121      0.006784
  232  14   8           14->12->10->7->8   1292.250      1.297      0.150497      0.045799
  233  14   9                  14->12->9    737.179      0.740      0.002890      0.003791
  234  14  10                 14->12->10    680.520      0.679      0.002919      0.004079
  235  14  11                 14->13->11    378.120      0.378      0.013054      0.013351
  236  14  12                     14->12    977.330      0.974      0.000507      0.000831
  237  14  13                     14->13    807.086      0.804      0.007257      0.007762
  238  14  15                     14->15    641.906      0.645      0.414396      0.435262
  239  14  16                 14->15->16    375.002      0.379      0.736271      0.986877
  240  15   0    15->16->12->10->7->8->0   1232.790      1.234      0.401076      0.157918
  241  15   1       15->16->12->10->4->1    304.897      0.300      0.323371      0.222801
  242  15   2       15->16->12->10->4->2   1211.150      1.204      0.245747      0.163832
  243  15   3           15->16->12->9->3   1120.420      1.118      0.270263      0.172222
  244  15   4          15->16->12->10->4    649.100      0.651      0.253592      0.203295
  245  15   5    15->16->12->10->7->8->5    692.318      0.688      1.069450      0.730841
  246  15   6       15->16->12->10->7->6   1000.020      1.004      0.252356      0.161500
  247  15   7          15->16->12->10->7   1034.810      1.026      0.246813      0.187225
  248  15   8       15->16->12->10->7->8    925.930      0.922      0.400333      0.245327
  249  15   9              15->16->12->9   1078.690      1.077      0.248674      0.244735
  250  15  10             15->16->12->10   1113.120      1.116      0.249410      0.247232
  251  15  11             15->14->13->11    462.453      0.463      0.032332      0.027005
  252  15  12                 15->16->12    885.388      0.881      0.245851      0.263786
  253  15  13                 15->14->13    836.644      0.840      0.023067      0.022214
  254  15  14                     15->14   1298.950      1.302      0.006007      0.008679
  255  15  16                     15->16   1195.550      1.197      0.226392      0.133384
  256  16   0        16->12->10->7->8->0   1250.800      1.252      0.152669      0.058214
  257  16   1           16->12->10->4->1    295.553      0.294      0.045914      0.064959
  258  16   2           16->12->10->4->2    917.960      0.911      0.007965      0.010478
  259  16   3               16->12->9->3   1304.090      1.300      0.020396      0.023349
  260  16   4              16->12->10->4    678.709      0.677      0.004611      0.006908
  261  16   5        16->12->10->7->8->5   1022.390      1.026      0.770937      0.485510
  262  16   6           16->12->10->7->6    234.599      0.234      0.011172      0.015542
  263  16   7              16->12->10->7    329.737      0.332      0.007270      0.006943
  264  16   8           16->12->10->7->8    599.902      0.600      0.158992      0.047738
  265  16   9                  16->12->9    807.625      0.807      0.003109      0.003802
  266  16  10                 16->12->10    647.506      0.646      0.003082      0.004515
  267  16  11             16->12->10->11    979.318      0.981      0.004757      0.007021
  268  16  12                     16->12   1278.530      1.281      0.000726      0.001184
  269  16  13             16->12->14->13    301.134      0.302      0.014936      0.016650
  270  16  14                 16->12->14    571.655      0.571      0.001983      0.002580
  271  16  15                     16->15    473.460      0.470      0.004921      0.004649
