2025-12-18 19:30:19.440605: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/spack/gilbreth-r9/apps/cuda/12.6.0-gcc-11.5.0-a7cv7sp/lib64:/apps/spack/gilbreth-r9/apps/libxml2/2.10.3-gcc-11.5.0-wroyiwb/lib:/apps/spack/gilbreth-r9/apps/libiconv/1.17-gcc-11.5.0-36njf44/lib:/apps/spack/gilbreth-r9/apps/libfabric/1.19.1-gcc-11.5.0-b5idhcg/lib64:/apps/spack/gilbreth-r9/apps/libfabric/1.19.1-gcc-11.5.0-b5idhcg/lib:/apps/spack/gilbreth-r9/apps/openmpi/4.1.6-gcc-11.5.0-w7wc45k/lib
2025-12-18 19:30:19.441198: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2025-12-18 19:30:21.542900: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2025-12-18 19:30:21.542979: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: gilbreth-fe03.rcac.purdue.edu
2025-12-18 19:30:21.542994: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: gilbreth-fe03.rcac.purdue.edu
2025-12-18 19:30:21.543081: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 565.57.1
2025-12-18 19:30:21.543112: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 565.57.1
2025-12-18 19:30:21.543121: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 565.57.1
2025-12-18 19:30:21.543625: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-12-18 19:30:26.258989: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
2025-12-18 19:30:26.594179: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:907] Skipping loop optimization for Merge node with control input: Assert/AssertGuard/branch_executed/_1017
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).path_update.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).path_update.recurrent_kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).path_update.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).link_update.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).link_update.recurrent_kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).link_update.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).queue_update.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).queue_update.recurrent_kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).queue_update.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).path_embedding.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).path_embedding.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).path_embedding.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).path_embedding.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).queue_embedding.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).queue_embedding.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).queue_embedding.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).queue_embedding.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).link_embedding.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).link_embedding.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).link_embedding.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).link_embedding.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).readout_path.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).readout_path.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).readout_path.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).readout_path.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).readout_path.layer_with_weights-2.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).readout_path.layer_with_weights-2.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).path_update.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).path_update.recurrent_kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).path_update.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).link_update.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).link_update.recurrent_kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).link_update.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).queue_update.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).queue_update.recurrent_kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).queue_update.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).path_embedding.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).path_embedding.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).path_embedding.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).path_embedding.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).queue_embedding.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).queue_embedding.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).queue_embedding.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).queue_embedding.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).link_embedding.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).link_embedding.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).link_embedding.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).link_embedding.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).readout_path.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).readout_path.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).readout_path.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).readout_path.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).readout_path.layer_with_weights-2.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).readout_path.layer_with_weights-2.bias
WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.

=== FERMI MODE B (metric=jitter) ===
Dataset directory : /scratch/gilbreth/mohame43/Routenet/RouteNet-Fermi/data/traffic_models/constant_bitrate/test/gbn-constant
Checkpoint dir    : /scratch/gilbreth/mohame43/Routenet/RouteNet-Fermi/traffic_models/jitter/ckpt_dir_mape_constant_bitrate
Sample index      : 0
Routes file       : /scratch/gilbreth/mohame43/Routenet/candidate_routes_0_5_test_gbn.txt

Loaded baseline sample (Fermi): n_paths = 272
BEST CHECKPOINT FOUND (Fermi): 30-111.35

====================================================================================================
[Fermi] Candidate 1: src=0, dst=5, path = 0->8->5
====================================================================================================
  idx src dst            route_nodes    traffic    packets   true_jitter_orig   pred_jitter
-------------------------------------------------------------------------------------------
    0   0   1                    0->2->1    948.479      0.950      0.025677      0.023590
    1   0   2                       0->2   1121.850      1.115      0.009654      0.009254
    2   0   3                 0->2->1->3    612.592      0.615      0.050070      0.048577
    3   0   4                    0->8->4    569.321      0.570      0.004469      0.004213
    4   0   5 *                  0->8->5   1163.940      1.154      0.230796      0.207292
    5   0   6                 0->8->7->6    444.808      0.441      0.009207      0.004790
    6   0   7                    0->8->7   1037.780      1.038      0.004595      0.003928
    7   0   8                       0->8    677.780      0.680      0.002231      0.002245
    8   0   9                 0->8->4->9   1195.660      1.193      0.016851      0.008269
    9   0  10                0->8->4->10    517.094      0.516      0.008711      0.006769
   10   0  11            0->8->4->10->11    154.845      0.151      0.011227      0.008593
   11   0  12            0->8->4->10->12   1226.900      1.229      0.010785      0.009894
   12   0  13        0->8->4->10->11->13   1135.800      1.129      0.015317      0.011246
   13   0  14        0->8->4->10->12->14    667.524      0.673      0.013207      0.011773
   14   0  15    0->8->4->10->12->14->15    659.054      0.659      0.267329      0.258275
   15   0  16        0->8->4->10->12->16    909.953      0.909      0.013012      0.010077
   16   1   0                    1->2->0    867.620      0.864      0.026005      0.025764
   17   1   2                       1->2    745.993      0.746      0.006808      0.005967
   18   1   3                       1->3    726.991      0.728      0.007345      0.006656
   19   1   4                       1->4    675.071      0.673      0.024739      0.022809
   20   1   5                 1->4->8->5   1024.670      1.018      0.263617      0.288484
   21   1   6             1->4->10->7->6    380.399      0.377      0.042140      0.031847
   22   1   7                1->4->10->7    149.510      0.149      0.038125      0.025892
   23   1   8                    1->4->8    301.178      0.300      0.029638      0.033271
   24   1   9                    1->3->9    841.467      0.837      0.041136      0.032369
   25   1  10                   1->4->10   1171.370      1.166      0.029732      0.018593
   26   1  11               1->4->10->11    540.795      0.541      0.033515      0.024875
   27   1  12               1->4->10->12   1167.830      1.166      0.032382      0.026705
   28   1  13           1->4->10->11->13    201.217      0.197      0.041590      0.043871
   29   1  14           1->4->10->12->14    880.941      0.879      0.035774      0.038443
   30   1  15       1->4->10->12->14->15    368.246      0.370      0.293246      0.456795
   31   1  16           1->4->10->12->16    988.449      0.991      0.035904      0.033139
   32   2   0                       2->0    547.554      0.546      0.010692      0.009007
   33   2   1                       2->1    171.327      0.172      0.007603      0.006440
   34   2   3                    2->1->3   1263.680      1.265      0.022579      0.023001
   35   2   4                       2->4    878.911      0.879      0.001136      0.001356
   36   2   5                 2->0->8->5    314.462      0.313      0.260493      0.227057
   37   2   6              2->0->8->7->6    797.873      0.797      0.029568      0.012883
   38   2   7                 2->0->8->7    660.396      0.664      0.022286      0.014912
   39   2   8                    2->0->8    849.800      0.845      0.016145      0.012720
   40   2   9                    2->4->9    257.141      0.255      0.013083      0.011302
   41   2  10                   2->4->10    491.164      0.493      0.005122      0.004652
   42   2  11               2->4->10->11    352.815      0.354      0.007125      0.003949
   43   2  12               2->4->10->12    897.034      0.890      0.007026      0.005201
   44   2  13           2->4->10->11->13    239.430      0.239      0.011548      0.008809
   45   2  14           2->4->10->12->14    898.549      0.903      0.009069      0.008376
   46   2  15       2->4->10->12->14->15   1252.950      1.251      0.261404      0.287220
   47   2  16           2->4->10->12->16    715.081      0.714      0.009081      0.007255
   48   3   0                 3->1->2->0    823.597      0.824      0.049490      0.044555
   49   3   1                       3->1    152.808      0.151      0.005959      0.005979
   50   3   2                    3->1->2    453.763      0.453      0.022049      0.020852
   51   3   4                       3->4    541.008      0.541      0.005799      0.006179
   52   3   5                 3->4->8->5    262.868      0.264      0.248154      0.289608
   53   3   6             3->9->10->7->6   1142.810      1.141      0.068111      0.059191
   54   3   7                3->9->10->7    206.722      0.202      0.061410      0.060088
   55   3   8                    3->4->8    775.584      0.778      0.008460      0.019498
   56   3   9                       3->9    434.569      0.433      0.025954      0.021672
   57   3  10                   3->9->10   1174.470      1.182      0.055470      0.047668
   58   3  11               3->9->10->11    785.866      0.790      0.059972      0.063443
   59   3  12                   3->9->12    442.083      0.447      0.030457      0.024431
   60   3  13           3->9->10->11->13    895.693      0.896      0.067292      0.065690
   61   3  14               3->9->12->14    875.484      0.872      0.032829      0.031309
   62   3  15           3->9->12->14->15    458.648      0.461      0.292180      0.271603
   63   3  16               3->9->12->16    587.836      0.585      0.032734      0.023997
   64   4   0                    4->8->0    151.870      0.151      0.002986      0.007491
   65   4   1                       4->1    556.229      0.553      0.017596      0.014060
   66   4   2                       4->2    908.232      0.907      0.001167      0.001571
   67   4   3                       4->3    927.194      0.937      0.009419      0.009103
   68   4   5                    4->8->5   1234.040      1.235      0.228698      0.263279
   69   4   6                4->10->7->6    995.673      0.998      0.008185      0.007156
   70   4   7                   4->10->7    811.217      0.809      0.005063      0.003895
   71   4   8                       4->8    863.404      0.863      0.000400      0.009747
   72   4   9                       4->9   1220.860      1.224      0.006888      0.006467
   73   4  10                      4->10    644.608      0.644      0.002775      0.002176
   74   4  11                  4->10->11    933.693      0.932      0.003960      0.002870
   75   4  12                  4->10->12   1008.940      1.010      0.004343      0.003250
   76   4  13              4->10->11->13    584.219      0.584      0.007183      0.004595
   77   4  14              4->10->12->14    416.655      0.415      0.005748      0.004768
   78   4  15          4->10->12->14->15   1141.380      1.142      0.251728      0.235571
   79   4  16              4->10->12->16    805.926      0.805      0.005716      0.003809
   80   5   0                    5->8->0    701.081      0.704      0.075732      0.061466
   81   5   1                 5->8->4->1    395.797      0.393      0.102007      0.089970
   82   5   2                 5->8->0->2    615.737      0.615      0.099637      0.103946
   83   5   3                 5->8->4->3    676.302      0.674      0.096755      0.082186
   84   5   4                    5->8->4    473.194      0.475      0.072430      0.055036
   85   5   6                       5->6    456.063      0.459      0.005530      0.005946
   86   5   7                    5->6->7    834.998      0.839      0.011115      0.008614
   87   5   8                       5->8   1135.260      1.138      0.071728      0.097830
   88   5   9                 5->8->4->9    474.199      0.471      0.092389      0.077919
   89   5  10                5->8->4->10    534.342      0.535      0.079575      0.061587
   90   5  11            5->8->4->10->11    792.053      0.788      0.083010      0.088249
   91   5  12            5->8->4->10->12   1242.630      1.242      0.082763      0.086043
   92   5  13        5->8->4->10->11->13    623.430      0.624      0.089884      0.141214
   93   5  14        5->8->4->10->12->14    288.628      0.290      0.083998      0.127339
   94   5  15    5->8->4->10->12->14->15    630.549      0.629      0.348071      0.518494
   95   5  16        5->8->4->10->12->16    431.004      0.427      0.085135      0.127256
   96   6   0                 6->7->8->0    935.537      0.937      0.128770      0.048048
   97   6   1              6->7->8->4->1    242.562      0.242      0.157323      0.053201
   98   6   2              6->7->8->0->2    948.429      0.949      0.147955      0.038504
   99   6   3              6->7->8->4->3   1093.590      1.095      0.146489      0.046307
  100   6   4                 6->7->8->4   1124.900      1.126      0.128390      0.045471
  101   6   5                       6->5    716.775      0.713      0.004899      0.004455
  102   6   7                       6->7    272.359      0.270      0.002058      0.002019
  103   6   8                    6->7->8   1115.040      1.123      0.124819      0.045433
  104   6   9                6->7->10->9    784.945      0.782      0.016380      0.008009
  105   6  10                   6->7->10    167.537      0.166      0.003428      0.002855
  106   6  11               6->7->10->11    935.920      0.930      0.005450      0.003152
  107   6  12               6->7->10->12    606.798      0.611      0.006068      0.003856
  108   6  13           6->7->10->11->13   1115.910      1.124      0.009659      0.007399
  109   6  14           6->7->10->12->14    146.525      0.147      0.008277      0.006733
  110   6  15       6->7->10->12->14->15    139.331      0.141      0.256490      0.279046
  111   6  16           6->7->10->12->16   1211.600      1.214      0.008064      0.005286
  112   7   0                    7->8->0    325.713      0.325      0.123343      0.055382
  113   7   1                 7->8->4->1    688.539      0.691      0.145251      0.054769
  114   7   2                 7->8->0->2    358.186      0.358      0.143210      0.055803
  115   7   3                 7->8->4->3    324.795      0.324      0.138520      0.044953
  116   7   4                    7->8->4    382.981      0.385      0.122056      0.044404
  117   7   5                    7->8->5   1207.220      1.214      0.397127      0.316871
  118   7   6                       7->6    937.767      0.938      0.001482      0.001907
  119   7   8                       7->8   1037.510      1.029      0.120348      0.032980
  120   7   9                   7->10->9    845.876      0.843      0.009809      0.008799
  121   7  10                      7->10    929.594      0.932      0.000506      0.000532
  122   7  11                  7->10->11   1201.930      1.202      0.001769      0.001391
  123   7  12                  7->10->12    905.131      0.909      0.002261      0.002105
  124   7  13              7->10->11->13    813.973      0.806      0.004975      0.003640
  125   7  14              7->10->12->14   1234.510      1.237      0.003630      0.003126
  126   7  15          7->10->12->14->15    582.758      0.586      0.252611      0.251366
  127   7  16              7->10->12->16    381.587      0.384      0.003781      0.002705
  128   8   0                       8->0   1220.630      1.214      0.001436      0.001998
  129   8   1                    8->4->1    648.319      0.648      0.020539      0.014156
  130   8   2                    8->0->2    390.292      0.389      0.016299      0.013798
  131   8   3                    8->4->3    227.204      0.226      0.014656      0.009870
  132   8   4                       8->4    632.425      0.627      0.001105      0.001271
  133   8   5                       8->5   1138.940      1.141      0.225471      0.216757
  134   8   6                    8->7->6    556.971      0.562      0.004054      0.004028
  135   8   7                       8->7    747.387      0.752      0.000978      0.001097
  136   8   9                    8->4->9    206.718      0.205      0.011479      0.008189
  137   8  10                   8->4->10    672.222      0.678      0.004701      0.003913
  138   8  11               8->4->10->11    749.357      0.745      0.006171      0.004140
  139   8  12               8->4->10->12    946.073      0.950      0.006483      0.005337
  140   8  13           8->4->10->11->13    377.863      0.377      0.009917      0.008449
  141   8  14           8->4->10->12->14   1174.270      1.170      0.007954      0.007829
  142   8  15       8->4->10->12->14->15    153.970      0.152      0.263884      0.255179
  143   8  16           8->4->10->12->16   1145.990      1.142      0.007930      0.006281
  144   9   0                 9->4->8->0    565.973      0.569      0.019500      0.020046
  145   9   1                    9->4->1    504.735      0.501      0.037900      0.033116
  146   9   2                    9->4->2    849.590      0.851      0.014932      0.015211
  147   9   3                       9->3    490.249      0.487      0.010762      0.009976
  148   9   4                       9->4   1045.420      1.046      0.009609      0.007990
  149   9   5                 9->4->8->5    911.084      0.910      0.247893      0.293933
  150   9   6                9->10->7->6    915.154      0.919      0.031016      0.026842
  151   9   7                   9->10->7   1036.660      1.035      0.024986      0.021044
  152   9   8                    9->4->8    969.301      0.968      0.012756      0.022463
  153   9  10                      9->10    492.369      0.489      0.022150      0.027969
  154   9  11                  9->10->11    721.391      0.724      0.024175      0.021784
  155   9  12                      9->12    948.349      0.946      0.001086      0.001341
  156   9  13              9->10->11->13    614.582      0.616      0.029822      0.028065
  157   9  14                  9->12->14    868.362      0.871      0.002606      0.004128
  158   9  15              9->12->14->15    444.801      0.447      0.247676      0.263437
  159   9  16                  9->12->16   1058.810      1.057      0.002476      0.003428
  160  10   0                10->7->8->0    264.322      0.263      0.125943      0.055525
  161  10   1                   10->4->1    448.921      0.446      0.020987      0.015154
  162  10   2                   10->4->2    953.241      0.952      0.002619      0.002893
  163  10   3                   10->4->3    661.730      0.661      0.013146      0.010302
  164  10   4                      10->4    338.532      0.339      0.000588      0.001341
  165  10   5                10->7->8->5    316.745      0.318      0.400472      0.342173
  166  10   6                   10->7->6    539.727      0.537      0.004090      0.004815
  167  10   7                      10->7    163.783      0.163      0.001754      0.001455
  168  10   8                   10->7->8    987.004      0.989      0.124048      0.045113
  169  10   9                      10->9    377.307      0.378      0.007350      0.006620
  170  10  11                     10->11    765.314      0.763      0.000659      0.001406
  171  10  12                     10->12    180.149      0.180      0.001229      0.001276
  172  10  13                 10->11->13    462.625      0.464      0.003086      0.003915
  173  10  14                 10->12->14    905.806      0.907      0.002132      0.003343
  174  10  15             10->12->14->15    146.921      0.147      0.245908      0.286057
  175  10  16                 10->12->16    140.998      0.138      0.002047      0.002281
  176  11   0            11->10->7->8->0   1035.050      1.031      0.129633      0.056015
  177  11   1               11->10->4->1    939.078      0.937      0.022889      0.017315
  178  11   2               11->10->4->2    197.893      0.201      0.004505      0.003050
  179  11   3               11->10->4->3    791.281      0.795      0.016282      0.010999
  180  11   4                  11->10->4   1140.280      1.136      0.001708      0.001403
  181  11   5            11->10->7->8->5    379.081      0.379      0.402858      0.340083
  182  11   6               11->10->7->6    395.637      0.394      0.006113      0.004276
  183  11   7                  11->10->7    694.183      0.694      0.002967      0.002354
  184  11   8               11->10->7->8   1047.870      1.047      0.125551      0.042191
  185  11   9                  11->10->9    939.800      0.943      0.009782      0.008169
  186  11  10                     11->10    464.462      0.465      0.000613      0.000587
  187  11  12                 11->10->12    352.046      0.351      0.002410      0.002105
  188  11  13                     11->13    141.723      0.143      0.001576      0.001885
  189  11  14                 11->13->14   1110.170      1.120      0.027792      0.023259
  190  11  15             11->13->14->15   1263.650      1.257      0.287947      0.252027
  191  11  16             11->10->12->16   1000.200      0.995      0.003772      0.002598
  192  12   0            12->10->7->8->0    866.855      0.863      0.129779      0.053896
  193  12   1               12->10->4->1    601.853      0.600      0.024182      0.021298
  194  12   2               12->10->4->2   1076.880      1.073      0.005220      0.003738
  195  12   3                   12->9->3    592.153      0.589      0.015412      0.013883
  196  12   4                  12->10->4    384.053      0.386      0.002470      0.002756
  197  12   5            12->10->7->8->5    593.124      0.595      0.406043      0.316996
  198  12   6               12->10->7->6   1248.090      1.247      0.006683      0.005487
  199  12   7                  12->10->7   1037.450      1.035      0.003733      0.003789
  200  12   8               12->10->7->8   1014.070      1.018      0.128078      0.040288
  201  12   9                      12->9    586.133      0.587      0.001347      0.001385
  202  12  10                     12->10    596.682      0.596      0.001365      0.001718
  203  12  11                 12->10->11   1195.730      1.193      0.002543      0.002666
  204  12  13                 12->14->13    191.091      0.189      0.010225      0.007247
  205  12  14                     12->14    140.612      0.144      0.000625      0.002096
  206  12  15                 12->14->15   1013.150      1.014      0.244456      0.258107
  207  12  16                     12->16   1138.830      1.143      0.000437      0.000736
  208  13   0        13->11->10->7->8->0    230.033      0.231      0.133917      0.050604
  209  13   1           13->11->10->4->1   1063.570      1.060      0.028852      0.019322
  210  13   2           13->11->10->4->2    607.098      0.612      0.008749      0.005844
  211  13   3           13->14->12->9->3   1086.120      1.082      0.054756      0.043057
  212  13   4              13->11->10->4    963.138      0.964      0.004678      0.002707
  213  13   5        13->11->10->7->8->5    415.767      0.422      0.412785      0.350811
  214  13   6           13->11->10->7->6    191.341      0.191      0.010502      0.007571
  215  13   7              13->11->10->7    682.482      0.689      0.005929      0.003753
  216  13   8           13->11->10->7->8    648.216      0.652      0.129988      0.060926
  217  13   9              13->14->12->9    628.156      0.636      0.032084      0.030129
  218  13  10                 13->11->10    455.354      0.454      0.002818      0.002756
  219  13  11                     13->11   1075.250      1.079      0.001231      0.001653
  220  13  12                 13->14->12   1028.340      1.035      0.025490      0.018320
  221  13  14                     13->14    888.582      0.887      0.023033      0.022834
  222  13  15                 13->14->15   1023.640      1.024      0.280391      0.225876
  223  13  16             13->14->15->16    910.336      0.912      0.494022      0.536974
  224  14   0        14->12->10->7->8->0    559.014      0.558      0.132995      0.053015
  225  14   1           14->12->10->4->1    137.234      0.139      0.030129      0.023961
  226  14   2           14->12->10->4->2    550.765      0.554      0.007702      0.005910
  227  14   3               14->12->9->3   1025.120      1.024      0.018136      0.015055
  228  14   4              14->12->10->4    178.961      0.179      0.004180      0.002863
  229  14   5        14->12->10->7->8->5    688.465      0.692      0.412837      0.393686
  230  14   6           14->12->10->7->6    722.821      0.725      0.008912      0.007394
  231  14   7              14->12->10->7    686.705      0.690      0.005431      0.003723
  232  14   8           14->12->10->7->8   1225.360      1.231      0.129021      0.049015
  233  14   9                  14->12->9    669.314      0.675      0.002770      0.003507
  234  14  10                 14->12->10    624.805      0.623      0.002532      0.002660
  235  14  11                 14->13->11    310.841      0.312      0.012372      0.010113
  236  14  12                     14->12    914.585      0.912      0.000480      0.000536
  237  14  13                     14->13    744.675      0.748      0.006768      0.006274
  238  14  15                     14->15    576.239      0.580      0.241916      0.246241
  239  14  16                 14->15->16    312.099      0.312      0.459604      0.505851
  240  15   0    15->16->12->10->7->8->0   1174.740      1.168      0.320568      0.217494
  241  15   1       15->16->12->10->4->1    233.933      0.234      0.221937      0.362637
  242  15   2       15->16->12->10->4->2   1139.060      1.142      0.190767      0.222214
  243  15   3           15->16->12->9->3   1049.520      1.048      0.208475      0.229794
  244  15   4          15->16->12->10->4    583.550      0.587      0.184284      0.191359
  245  15   5    15->16->12->10->7->8->5    627.906      0.618      0.600746      0.398335
  246  15   6       15->16->12->10->7->6    936.352      0.933      0.192348      0.215674
  247  15   7          15->16->12->10->7    945.856      0.952      0.186567      0.189691
  248  15   8       15->16->12->10->7->8    855.474      0.859      0.316388      0.256657
  249  15   9              15->16->12->9   1003.190      1.010      0.185086      0.166711
  250  15  10             15->16->12->10   1041.670      1.044      0.181606      0.198451
  251  15  11             15->14->13->11    398.599      0.396      0.031270      0.029876
  252  15  12                 15->16->12    806.143      0.808      0.181701      0.174757
  253  15  13                 15->14->13    776.439      0.769      0.022333      0.020940
  254  15  14                     15->14   1218.980      1.223      0.005926      0.005157
  255  15  16                     15->16   1118.710      1.121      0.177185      0.126558
  256  16   0        16->12->10->7->8->0   1171.710      1.173      0.132599      0.050622
  257  16   1           16->12->10->4->1    223.321      0.226      0.030163      0.024563
  258  16   2           16->12->10->4->2    855.198      0.851      0.007475      0.006317
  259  16   3               16->12->9->3   1213.990      1.220      0.017929      0.012705
  260  16   4              16->12->10->4    617.008      0.616      0.004184      0.003067
  261  16   5        16->12->10->7->8->5    967.048      0.965      0.408313      0.334676
  262  16   6           16->12->10->7->6    165.088      0.166      0.009196      0.007939
  263  16   7              16->12->10->7    267.560      0.266      0.005476      0.003871
  264  16   8           16->12->10->7->8    538.336      0.538      0.129271      0.048213
  265  16   9                  16->12->9    733.836      0.740      0.002906      0.003359
  266  16  10                 16->12->10    594.522      0.589      0.002687      0.002873
  267  16  11             16->12->10->11    913.926      0.910      0.004285      0.003075
  268  16  12                     16->12   1213.680      1.215      0.000683      0.000794
  269  16  13             16->12->14->13    234.133      0.234      0.014179      0.011148
  270  16  14                 16->12->14    506.459      0.504      0.001915      0.002566
  271  16  15                     16->15    400.971      0.401      0.004899      0.004505
