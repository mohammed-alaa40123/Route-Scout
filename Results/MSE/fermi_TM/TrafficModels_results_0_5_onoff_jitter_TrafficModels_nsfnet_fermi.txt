2025-12-18 19:28:57.693527: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/spack/gilbreth-r9/apps/cuda/12.6.0-gcc-11.5.0-a7cv7sp/lib64:/apps/spack/gilbreth-r9/apps/libxml2/2.10.3-gcc-11.5.0-wroyiwb/lib:/apps/spack/gilbreth-r9/apps/libiconv/1.17-gcc-11.5.0-36njf44/lib:/apps/spack/gilbreth-r9/apps/libfabric/1.19.1-gcc-11.5.0-b5idhcg/lib64:/apps/spack/gilbreth-r9/apps/libfabric/1.19.1-gcc-11.5.0-b5idhcg/lib:/apps/spack/gilbreth-r9/apps/openmpi/4.1.6-gcc-11.5.0-w7wc45k/lib
2025-12-18 19:28:57.694042: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2025-12-18 19:29:00.090456: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2025-12-18 19:29:00.090532: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: gilbreth-fe03.rcac.purdue.edu
2025-12-18 19:29:00.090547: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: gilbreth-fe03.rcac.purdue.edu
2025-12-18 19:29:00.090626: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 565.57.1
2025-12-18 19:29:00.090654: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 565.57.1
2025-12-18 19:29:00.090663: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 565.57.1
2025-12-18 19:29:00.091067: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-12-18 19:29:04.818681: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
2025-12-18 19:29:05.161173: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:907] Skipping loop optimization for Merge node with control input: Assert_1/AssertGuard/branch_executed/_1027
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).path_update.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).path_update.recurrent_kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).path_update.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).link_update.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).link_update.recurrent_kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).link_update.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).queue_update.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).queue_update.recurrent_kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).queue_update.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).path_embedding.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).path_embedding.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).path_embedding.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).path_embedding.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).queue_embedding.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).queue_embedding.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).queue_embedding.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).queue_embedding.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).link_embedding.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).link_embedding.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).link_embedding.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).link_embedding.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).readout_path.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).readout_path.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).readout_path.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).readout_path.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).readout_path.layer_with_weights-2.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).readout_path.layer_with_weights-2.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).path_update.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).path_update.recurrent_kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).path_update.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).link_update.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).link_update.recurrent_kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).link_update.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).queue_update.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).queue_update.recurrent_kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).queue_update.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).path_embedding.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).path_embedding.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).path_embedding.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).path_embedding.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).queue_embedding.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).queue_embedding.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).queue_embedding.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).queue_embedding.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).link_embedding.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).link_embedding.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).link_embedding.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).link_embedding.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).readout_path.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).readout_path.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).readout_path.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).readout_path.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).readout_path.layer_with_weights-2.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).readout_path.layer_with_weights-2.bias
WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.

=== FERMI MODE B (metric=jitter) ===
Dataset directory : /scratch/gilbreth/mohame43/Routenet/RouteNet-Fermi/data/traffic_models/onoff/train/nsfnet-onoff
Checkpoint dir    : /scratch/gilbreth/mohame43/Routenet/RouteNet-Fermi/traffic_models/jitter/ckpt_dir_mape_onoff
Sample index      : 0
Routes file       : /scratch/gilbreth/mohame43/Routenet/candidate_routes_0_5_train_nsfnet.txt

Loaded baseline sample (Fermi): n_paths = 182
BEST CHECKPOINT FOUND (Fermi): 30-30.34

====================================================================================================
[Fermi] Candidate 1: src=0, dst=5, path = 0->2->5
====================================================================================================
  idx src dst            route_nodes    traffic    packets   true_jitter_orig   pred_jitter
-------------------------------------------------------------------------------------------
    0   0   1                       0->1   1017.530      1.014      0.012985      0.013697
    1   0   2                       0->2   1183.280      1.182      0.018836      0.021344
    2   0   3                       0->3    686.979      0.687      0.075843      0.064626
    3   0   4                    0->3->4    632.110      0.631      0.080951      0.080583
    4   0   5 *                  0->2->5   1227.150      1.226      0.024075      0.029657
    5   0   6                 0->1->7->6    512.160      0.510      0.378329      0.395424
    6   0   7                    0->1->7   1113.530      1.114      0.347736      0.335079
    7   0   8                    0->3->8    748.982      0.748      0.081668      0.080885
    8   0   9                 0->3->8->9   1259.960      1.259      0.120168      0.108761
    9   0  10                0->1->7->10    586.115      0.585      0.350553      0.379445
   10   0  11                0->3->8->11    216.680      0.217      0.098205      0.085570
   11   0  12                0->2->5->12   1299.820      1.297      1.169280      1.142726
   12   0  13                0->2->5->13   1206.610      1.205      0.207163      0.175908
   13   1   0                       1->0    739.737      0.741      0.015019      0.015220
   14   1   2                       1->2    728.732      0.728      0.008774      0.008790
   15   1   3                    1->0->3    968.666      0.970      0.109004      0.096606
   16   1   4                 1->0->3->4    929.965      0.927      0.115553      0.103576
   17   1   5                    1->2->5    809.827      0.815      0.014272      0.013291
   18   1   6                    1->7->6    807.628      0.807      0.349103      0.383133
   19   1   7                       1->7    736.331      0.735      0.307066      0.314099
   20   1   8                 1->0->3->8   1085.210      1.090      0.117457      0.099770
   21   1   9                1->7->10->9    448.837      0.448      0.415983      0.473076
   22   1  10                   1->7->10    220.927      0.219      0.291054      0.304503
   23   1  11               1->7->10->11    369.811      0.369      0.325887      0.397960
   24   1  12                1->2->5->12    900.260      0.899      1.160100      1.137727
   25   1  13               1->7->10->13   1233.180      1.237      0.402863      0.448558
   26   2   0                       2->0    608.018      0.609      0.007315      0.007323
   27   2   1                       2->1   1231.150      1.232      0.009266      0.008915
   28   2   3                    2->0->3    259.400      0.261      0.088149      0.086560
   29   2   4                    2->5->4    956.283      0.956      0.004501      0.003900
   30   2   5                       2->5    442.043      0.444      0.001784      0.001594
   31   2   6                 2->1->7->6   1058.500      1.057      0.365588      0.411477
   32   2   7                    2->1->7    606.633      0.612      0.327934      0.349124
   33   2   8                 2->0->3->8    239.303      0.238      0.097138      0.085293
   34   2   9                2->5->12->9   1334.950      1.336      1.187430      1.132189
   35   2  10               2->5->13->10    953.860      0.955      1.134170      1.080014
   36   2  11               2->5->12->11    382.915      0.384      1.175070      1.137379
   37   2  12                   2->5->12    874.039      0.872      1.136090      1.086198
   38   2  13                   2->5->13    738.716      0.737      0.169123      0.141895
   39   3   0                       3->0    908.656      0.912      1.151220      1.049369
   40   3   1                    3->0->1    322.595      0.323      1.172270      1.100573
   41   3   2                    3->0->2    555.623      0.558      1.173440      1.082756
   42   3   4                       3->4    424.354      0.422      0.001219      0.001239
   43   3   5                    3->4->5    959.143      0.955      0.003761      0.004917
   44   3   6                    3->4->6    310.360      0.310      0.015382      0.014721
   45   3   7                 3->0->1->7    975.704      0.975      1.498530      1.499603
   46   3   8                       3->8   1311.040      1.316      0.001436      0.001299
   47   3   9                    3->8->9    779.022      0.780      0.030517      0.031531
   48   3  10                3->8->9->10    896.837      0.895      0.639512      0.600542
   49   3  11                   3->8->11    213.867      0.215      0.013854      0.012066
   50   3  12               3->8->11->12    516.074      0.516      0.042928      0.035848
   51   3  13                3->4->5->13    609.452      0.609      0.175609      0.167314
   52   4   0                    4->3->0    330.252      0.330      1.153440      1.170614
   53   4   1                 4->3->0->1   1211.810      1.208      1.175070      1.160809
   54   4   2                    4->5->2    267.929      0.267      0.003990      0.004039
   55   4   3                       4->3    841.911      0.840      0.001369      0.001135
   56   4   5                       4->5    497.207      0.498      0.001139      0.001011
   57   4   6                       4->6   1237.550      1.242      0.008865      0.009567
   58   4   7                    4->6->7    862.146      0.860      0.039482      0.037519
   59   4   8                    4->3->8    515.735      0.515      0.004468      0.004153
   60   4   9                 4->3->8->9    967.110      0.963      0.036436      0.035476
   61   4  10               4->5->13->10    934.376      0.937      1.128760      1.185420
   62   4  11                4->3->8->11    525.474      0.525      0.019502      0.017688
   63   4  12                   4->5->12    655.925      0.654      1.151750      1.145144
   64   4  13                   4->5->13    219.955      0.219      0.152021      0.162609
   65   5   0                    5->2->0    621.951      0.623      0.012391      0.012439
   66   5   1                    5->2->1    971.692      0.973      0.014643      0.014447
   67   5   2                       5->2   1003.340      1.004      0.001197      0.001379
   68   5   3                    5->4->3   1314.600      1.311      0.004046      0.003956
   69   5   4                       5->4   1070.990      1.070      0.001204      0.001089
   70   5   6                    5->4->6    870.699      0.872      0.014452      0.014133
   71   5   7               5->13->10->7    933.846      0.936      1.121970      1.165803
   72   5   8                5->12->9->8   1292.360      1.289      1.826520      2.031660
   73   5   9                   5->12->9    707.663      0.711      1.200550      1.106456
   74   5  10                  5->13->10    985.982      0.990      1.138050      1.011388
   75   5  11                  5->12->11   1080.530      1.079      1.145260      1.062394
   76   5  12                      5->12    647.459      0.652      1.131840      1.072768
   77   5  13                      5->13    486.015      0.484      0.157236      0.142111
   78   6   0                 6->4->3->0   1223.640      1.219      1.167860      1.141918
   79   6   1                    6->7->1    872.333      0.872      0.318162      0.318924
   80   6   2                 6->7->1->2    771.597      0.772      0.347464      0.336158
   81   6   3                    6->4->3    458.573      0.459      0.017788      0.015953
   82   6   4                       6->4    684.689      0.684      0.012338      0.012673
   83   6   5                    6->4->5    742.455      0.743      0.017056      0.016058
   84   6   7                       6->7    539.680      0.542      0.020675      0.019355
   85   6   8                 6->4->3->8    525.355      0.528      0.024136      0.021051
   86   6   9                6->7->10->9    913.791      0.913      0.143102      0.115528
   87   6  10                   6->7->10   1200.950      1.199      0.026574      0.025066
   88   6  11               6->7->10->11    536.661      0.539      0.049624      0.043989
   89   6  12                6->4->5->12    601.157      0.601      1.166020      1.130926
   90   6  13               6->7->10->13    865.416      0.866      0.095369      0.094255
   91   7   0                    7->1->0   1322.570      1.318      0.325866      0.308898
   92   7   1                       7->1    684.585      0.686      0.286211      0.287411
   93   7   2                    7->1->2    362.737      0.363      0.293814      0.303873
   94   7   3                 7->1->0->3    694.994      0.696      0.417562      0.417712
   95   7   4                    7->6->4    499.749      0.498      0.038770      0.036529
   96   7   5               7->10->13->5   1004.750      1.008      0.086829      0.079321
   97   7   6                       7->6    307.725      0.309      0.016871      0.016394
   98   7   8                7->10->9->8   1011.130      1.012      0.789815      0.773329
   99   7   9                   7->10->9   1176.050      1.173      0.108514      0.089316
  100   7  10                      7->10   1192.190      1.194      0.002005      0.002546
  101   7  11                  7->10->11    779.438      0.784      0.016309      0.015279
  102   7  12              7->10->11->12    339.664      0.339      0.046535      0.041340
  103   7  13                  7->10->13   1190.060      1.186      0.066911      0.072204
  104   8   0                    8->3->0    843.696      0.845      1.161680      1.176857
  105   8   1                 8->3->0->1    233.138      0.234      1.180190      1.093007
  106   8   2                 8->3->0->2    996.392      0.997      1.188810      1.143314
  107   8   3                       8->3    687.051      0.686      0.001692      0.002358
  108   8   4                    8->3->4   1193.090      1.193      0.004275      0.004665
  109   8   5               8->11->12->5    212.946      0.214      0.892321      0.839096
  110   8   6                 8->3->4->6    205.357      0.206      0.022349      0.020034
  111   8   7                8->9->10->7   1274.070      1.272      0.642042      0.639530
  112   8   9                       8->9    391.383      0.391      0.024223      0.023629
  113   8  10                   8->9->10    759.829      0.761      0.629398      0.561234
  114   8  11                      8->11    426.362      0.426      0.007630      0.007314
  115   8  12                  8->11->12    388.892      0.391      0.035390      0.032594
  116   8  13               8->9->10->13    455.604      0.457      0.692581      0.637934
  117   9   0                 9->8->3->0   1288.240      1.286      1.773090      2.015244
  118   9   1                9->10->7->1   1014.170      1.010      0.880404      0.888760
  119   9   2                9->12->5->2   1097.490      1.100      0.889303      0.952961
  120   9   3                    9->8->3    904.003      0.904      0.618435      0.610720
  121   9   4                 9->8->3->4   1003.810      1.011      0.595481      0.648997
  122   9   5                   9->12->5   1275.100      1.274      0.883735      1.054685
  123   9   6                9->10->7->6    970.936      0.973      0.615294      0.625928
  124   9   7                   9->10->7    865.839      0.866      0.582305      0.572130
  125   9   8                       9->8   1289.020      1.292      0.572989      0.704848
  126   9  10                      9->10    652.497      0.654      0.542871      0.545685
  127   9  11                   9->8->11    446.651      0.449      0.651170      0.675759
  128   9  12                      9->12   1298.810      1.296      0.007072      0.009177
  129   9  13                  9->10->13    722.654      0.721      0.638278      0.609187
  130  10   0                10->7->1->0    455.652      0.456      0.310484      0.342897
  131  10   1                   10->7->1    291.186      0.293      0.280835      0.274932
  132  10   2                10->7->1->2    689.195      0.690      0.305031      0.306239
  133  10   3                10->9->8->3   1211.720      1.214      0.774428      0.813767
  134  10   4                10->7->6->4    626.183      0.629      0.046824      0.039095
  135  10   5                  10->13->5    822.780      0.820      0.077650      0.080725
  136  10   6                   10->7->6    272.844      0.275      0.022743      0.023056
  137  10   7                      10->7    754.960      0.750      0.002047      0.002470
  138  10   8                   10->9->8    811.265      0.811      0.790754      0.725623
  139  10   9                      10->9   1018.540      1.021      0.104341      0.082258
  140  10  11                     10->11    443.599      0.443      0.011053      0.010646
  141  10  12                 10->11->12   1226.160      1.227      0.035595      0.031594
  142  10  13                     10->13    219.882      0.219      0.050555      0.065035
  143  11   0                11->8->3->0   1219.580      1.221      1.169400      1.144526
  144  11   1               11->10->7->1    637.538      0.639      0.313128      0.361673
  145  11   2               11->12->5->2    560.925      0.562      0.895540      0.947460
  146  11   3                   11->8->3    925.490      0.926      0.013180      0.012977
  147  11   4                11->8->3->4    547.537      0.549      0.019742      0.017700
  148  11   5                  11->12->5   1101.580      1.102      0.894183      0.877687
  149  11   6               11->10->7->6    986.031      0.984      0.042087      0.037277
  150  11   7                  11->10->7    985.241      0.982      0.014307      0.013586
  151  11   8                      11->8   1112.360      1.111      0.007629      0.008224
  152  11   9                  11->12->9   1028.740      1.031      0.068123      0.074904
  153  11  10                     11->10    551.305      0.550      0.009176      0.009136
  154  11  12                     11->12    789.221      0.790      0.016436      0.018217
  155  11  13                 11->10->13   1013.880      1.015      0.075603      0.081038
  156  12   0                12->5->2->0    683.473      0.685      0.875566      0.896028
  157  12   1                12->5->2->1    927.442      0.932      0.883956      0.944081
  158  12   2                   12->5->2    522.113      0.522      0.855464      0.879926
  159  12   3                12->9->8->3   1104.340      1.105      0.660679      0.724782
  160  12   4                   12->5->4    331.599      0.332      0.837162      0.883474
  161  12   5                      12->5    513.949      0.512      0.840457      0.936630
  162  12   6                12->5->4->6   1031.600      1.033      0.878873      0.913444
  163  12   7               12->9->10->7    720.333      0.722      0.637276      0.676021
  164  12   8                   12->9->8    406.813      0.406      0.709846      0.712984
  165  12   9                      12->9    388.417      0.385      0.039393      0.045590
  166  12  10                  12->9->10    606.350      0.609      0.615396      0.605391
  167  12  11                     12->11    231.549      0.230      0.005969      0.004729
  168  12  13                  12->5->13   1048.360      1.053      1.046670      1.019664
  169  13   0                13->5->2->0    444.134      0.444      0.036502      0.028921
  170  13   1               13->10->7->1    834.782      0.835      1.134640      1.233255
  171  13   2                   13->5->2    249.107      0.249      0.016577      0.015003
  172  13   3                13->5->4->3    532.186      0.533      0.021997      0.019120
  173  13   4                   13->5->4    979.162      0.977      0.014842      0.013970
  174  13   5                      13->5    213.834      0.215      0.011355      0.012283
  175  13   6               13->10->7->6    207.585      0.206      0.815257      0.961864
  176  13   7                  13->10->7   1102.050      1.103      0.824697      0.951559
  177  13   8               13->10->9->8    993.989      0.996      1.615550      1.806782
  178  13   9                  13->10->9    266.341      0.267      0.880898      0.937926
  179  13  10                     13->10    861.207      0.859      0.810203      0.915047
  180  13  11                 13->10->11   1199.620      1.198      0.839678      0.853166
  181  13  12                  13->5->12    443.778      0.445      1.161360      1.049151
