2025-12-18 19:29:55.409647: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/spack/gilbreth-r9/apps/cuda/12.6.0-gcc-11.5.0-a7cv7sp/lib64:/apps/spack/gilbreth-r9/apps/libxml2/2.10.3-gcc-11.5.0-wroyiwb/lib:/apps/spack/gilbreth-r9/apps/libiconv/1.17-gcc-11.5.0-36njf44/lib:/apps/spack/gilbreth-r9/apps/libfabric/1.19.1-gcc-11.5.0-b5idhcg/lib64:/apps/spack/gilbreth-r9/apps/libfabric/1.19.1-gcc-11.5.0-b5idhcg/lib:/apps/spack/gilbreth-r9/apps/openmpi/4.1.6-gcc-11.5.0-w7wc45k/lib
2025-12-18 19:29:55.410153: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2025-12-18 19:29:57.709898: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2025-12-18 19:29:57.710954: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: gilbreth-fe03.rcac.purdue.edu
2025-12-18 19:29:57.710973: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: gilbreth-fe03.rcac.purdue.edu
2025-12-18 19:29:57.711054: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 565.57.1
2025-12-18 19:29:57.711082: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 565.57.1
2025-12-18 19:29:57.711091: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 565.57.1
2025-12-18 19:29:57.711537: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-12-18 19:30:01.952826: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
2025-12-18 19:30:02.295910: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:907] Skipping loop optimization for Merge node with control input: Assert_1/AssertGuard/branch_executed/_1027
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).path_update.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).path_update.recurrent_kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).path_update.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).link_update.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).link_update.recurrent_kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).link_update.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).queue_update.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).queue_update.recurrent_kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).queue_update.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).path_embedding.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).path_embedding.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).path_embedding.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).path_embedding.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).queue_embedding.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).queue_embedding.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).queue_embedding.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).queue_embedding.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).link_embedding.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).link_embedding.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).link_embedding.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).link_embedding.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).readout_path.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).readout_path.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).readout_path.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).readout_path.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).readout_path.layer_with_weights-2.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).readout_path.layer_with_weights-2.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).path_update.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).path_update.recurrent_kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).path_update.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).link_update.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).link_update.recurrent_kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).link_update.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).queue_update.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).queue_update.recurrent_kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).queue_update.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).path_embedding.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).path_embedding.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).path_embedding.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).path_embedding.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).queue_embedding.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).queue_embedding.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).queue_embedding.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).queue_embedding.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).link_embedding.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).link_embedding.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).link_embedding.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).link_embedding.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).readout_path.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).readout_path.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).readout_path.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).readout_path.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).readout_path.layer_with_weights-2.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).readout_path.layer_with_weights-2.bias
WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.

=== FERMI MODE B (metric=jitter) ===
Dataset directory : /scratch/gilbreth/mohame43/Routenet/RouteNet-Fermi/data/traffic_models/all_multiplexed/test/gbn-multiplexed
Checkpoint dir    : /scratch/gilbreth/mohame43/Routenet/RouteNet-Fermi/traffic_models/jitter/ckpt_dir_mape_all_multiplexed
Sample index      : 0
Routes file       : /scratch/gilbreth/mohame43/Routenet/candidate_routes_0_5_test_gbn.txt

Loaded baseline sample (Fermi): n_paths = 272
BEST CHECKPOINT FOUND (Fermi): 30-15.26

====================================================================================================
[Fermi] Candidate 1: src=0, dst=5, path = 0->8->5
====================================================================================================
  idx src dst            route_nodes    traffic    packets   true_jitter_orig   pred_jitter
-------------------------------------------------------------------------------------------
    0   0   1                    0->2->1    823.724      0.819      0.261858      0.276276
    1   0   2                       0->2   1101.150      1.102      0.226121      0.257258
    2   0   3                 0->2->1->3    634.654      0.634      1.167490      1.334715
    3   0   4                    0->8->4   1244.150      1.239      0.023897      0.027619
    4   0   5 *                  0->8->5    660.230      0.662      0.571318      0.796869
    5   0   6                 0->8->7->6    582.412      0.579      0.022886      0.023367
    6   0   7                    0->8->7    209.409      0.210      0.017195      0.018123
    7   0   8                       0->8    872.791      0.873      0.014725      0.014871
    8   0   9                 0->8->4->9   1088.960      1.088      0.040490      0.042933
    9   0  10                0->8->4->10   1260.020      1.255      0.048130      0.058187
   10   0  11            0->8->4->10->11   1048.670      1.051      0.049153      0.060148
   11   0  12            0->8->4->10->12    907.441      0.909      0.055828      0.067353
   12   0  13        0->8->4->10->11->13    797.356      0.799      0.059435      0.074540
   13   0  14        0->8->4->10->12->14   1212.870      1.214      0.058595      0.067103
   14   0  15    0->8->4->10->12->14->15    608.601      0.608      1.906060      1.672445
   15   0  16        0->8->4->10->12->16    343.418      0.343      0.058123      0.066849
   16   1   0                    1->2->0    855.606      0.855      0.436911      0.460987
   17   1   2                       1->2    841.367      0.838      0.008821      0.010227
   18   1   3                       1->3    964.023      0.965      0.022825      0.012957
   19   1   4                       1->4   1160.190      1.158      0.126030      0.186994
   20   1   5                 1->4->8->5   1238.740      1.243      0.829640      0.804763
   21   1   6             1->4->10->7->6    369.500      0.368      0.168154      0.206837
   22   1   7                1->4->10->7    492.283      0.490      0.165913      0.199840
   23   1   8                    1->4->8    846.446      0.839      0.148620      0.196406
   24   1   9                    1->3->9   1251.720      1.256      0.376575      0.409354
   25   1  10                   1->4->10    358.368      0.360      0.156094      0.181212
   26   1  11               1->4->10->11    604.132      0.604      0.142768      0.202808
   27   1  12               1->4->10->12    471.726      0.473      0.156297      0.197603
   28   1  13           1->4->10->11->13    290.113      0.287      0.156823      0.226061
   29   1  14           1->4->10->12->14   1212.100      1.220      0.146020      0.200871
   30   1  15       1->4->10->12->14->15    842.990      0.843      1.549420      1.700141
   31   1  16           1->4->10->12->16    278.802      0.279      0.159835      0.223428
   32   2   0                       2->0    713.278      0.709      0.855783      0.857169
   33   2   1                       2->1    826.881      0.823      0.008780      0.010377
   34   2   3                    2->1->3    150.248      0.151      0.039625      0.033018
   35   2   4                       2->4    435.527      0.441      0.041839      0.054559
   36   2   5                 2->0->8->5    508.007      0.511      1.560530      1.946600
   37   2   6              2->0->8->7->6    265.581      0.265      0.434706      0.477912
   38   2   7                 2->0->8->7   1179.850      1.184      0.434143      0.454408
   39   2   8                    2->0->8    776.827      0.778      0.876846      0.990347
   40   2   9                    2->4->9    484.576      0.486      0.015896      0.019373
   41   2  10                   2->4->10   1190.100      1.187      0.015561      0.022129
   42   2  11               2->4->10->11   1260.820      1.264      0.021082      0.028448
   43   2  12               2->4->10->12   1098.220      1.097      0.023365      0.029017
   44   2  13           2->4->10->11->13   1095.210      1.093      0.029032      0.036199
   45   2  14           2->4->10->12->14    586.161      0.585      0.025384      0.033384
   46   2  15       2->4->10->12->14->15    318.984      0.319      1.373970      1.414637
   47   2  16           2->4->10->12->16    910.177      0.907      0.026353      0.034045
   48   3   0                 3->1->2->0    939.528      0.937      0.440791      0.497048
   49   3   1                       3->1    416.766      0.415      0.007116      0.007407
   50   3   2                    3->1->2    584.776      0.584      0.023244      0.024880
   51   3   4                       3->4    887.664      0.890      0.484069      0.480721
   52   3   5                 3->4->8->5   1216.650      1.223      0.783058      0.723621
   53   3   6             3->9->10->7->6   1100.670      1.103      0.728756      0.668356
   54   3   7                3->9->10->7    472.679      0.470      0.710592      0.670988
   55   3   8                    3->4->8    581.998      0.584      0.088557      0.092714
   56   3   9                       3->9    416.041      0.415      0.240346      0.286144
   57   3  10                   3->9->10    805.898      0.805      0.667566      0.706185
   58   3  11               3->9->10->11    704.304      0.704      0.671634      0.689203
   59   3  12                   3->9->12    864.846      0.869      0.242904      0.264428
   60   3  13           3->9->10->11->13    888.858      0.887      0.727454      0.734111
   61   3  14               3->9->12->14    570.520      0.567      0.252443      0.280200
   62   3  15           3->9->12->14->15    620.956      0.621      1.572410      1.824007
   63   3  16               3->9->12->16    233.185      0.234      0.248970      0.272516
   64   4   0                    4->8->0    762.006      0.763      0.003339      0.004624
   65   4   1                       4->1   1300.890      1.302      1.286560      1.261214
   66   4   2                       4->2   1167.280      1.164      0.029488      0.028863
   67   4   3                       4->3    223.163      0.222      0.665801      0.715737
   68   4   5                    4->8->5   1155.840      1.154      0.682030      0.725412
   69   4   6                4->10->7->6    243.068      0.243      0.033877      0.037965
   70   4   7                   4->10->7    991.031      0.987      0.035217      0.036442
   71   4   8                       4->8   1195.950      1.195      0.000452      0.000668
   72   4   9                       4->9    320.199      0.320      0.010019      0.011908
   73   4  10                      4->10   1119.380      1.123      0.011769      0.015910
   74   4  11                  4->10->11    596.827      0.596      0.013648      0.020815
   75   4  12                  4->10->12    929.502      0.930      0.018274      0.023931
   76   4  13              4->10->11->13    609.513      0.611      0.022579      0.029618
   77   4  14              4->10->12->14    596.939      0.598      0.020870      0.025820
   78   4  15          4->10->12->14->15    141.527      0.141      1.375670      1.339617
   79   4  16              4->10->12->16    324.049      0.325      0.020082      0.025651
   80   5   0                    5->8->0    666.930      0.667      1.017930      1.071547
   81   5   1                 5->8->4->1    334.311      0.335      2.523610      2.439561
   82   5   2                 5->8->0->2   1045.030      1.045      1.477610      1.568188
   83   5   3                 5->8->4->3    825.124      0.824      1.297730      1.328984
   84   5   4                    5->8->4   1032.130      1.029      1.256880      1.242851
   85   5   6                       5->6    840.832      0.843      0.005380      0.005970
   86   5   7                    5->6->7    452.160      0.453      0.014972      0.016005
   87   5   8                       5->8    790.611      0.791      1.253960      1.224604
   88   5   9                 5->8->4->9   1236.040      1.237      1.277920      1.293266
   89   5  10                5->8->4->10    580.805      0.586      1.290620      1.268823
   90   5  11            5->8->4->10->11    318.377      0.320      1.113580      1.113464
   91   5  12            5->8->4->10->12    818.709      0.820      1.308540      1.312461
   92   5  13        5->8->4->10->11->13    908.180      0.911      1.306070      1.388505
   93   5  14        5->8->4->10->12->14    194.055      0.193      1.303330      1.357067
   94   5  15    5->8->4->10->12->14->15   1252.730      1.254      2.869940      3.050707
   95   5  16        5->8->4->10->12->16    333.332      0.334      1.301750      1.335211
   96   6   0                 6->7->8->0    908.211      0.909      0.230437      0.206180
   97   6   1              6->7->8->4->1    897.087      0.901      1.547590      1.496926
   98   6   2              6->7->8->0->2   1214.090      1.214      0.519900      0.511976
   99   6   3              6->7->8->4->3    652.668      0.653      0.286686      0.315773
  100   6   4                 6->7->8->4    425.332      0.427      0.231035      0.226698
  101   6   5                       6->5   1005.410      1.005      0.005928      0.006949
  102   6   7                       6->7   1167.830      1.168      0.005041      0.005837
  103   6   8                    6->7->8    500.276      0.501      0.223567      0.197813
  104   6   9                6->7->10->9   1133.280      1.136      0.142147      0.132020
  105   6  10                   6->7->10    344.077      0.346      0.063343      0.081250
  106   6  11               6->7->10->11    723.048      0.723      0.009050      0.010846
  107   6  12               6->7->10->12   1259.280      1.259      0.013792      0.016396
  108   6  13           6->7->10->11->13    437.150      0.434      0.018564      0.021805
  109   6  14           6->7->10->12->14    945.373      0.946      0.015426      0.018342
  110   6  15       6->7->10->12->14->15    962.034      0.964      1.338310      1.399819
  111   6  16           6->7->10->12->16    373.740      0.376      0.015231      0.019249
  112   7   0                    7->8->0   1124.330      1.122      0.218554      0.194669
  113   7   1                 7->8->4->1    453.090      0.452      1.524850      1.515853
  114   7   2                 7->8->0->2    943.734      0.938      0.462082      0.447710
  115   7   3                 7->8->4->3    780.391      0.778      0.273511      0.300924
  116   7   4                    7->8->4    283.556      0.286      0.179807      0.290127
  117   7   5                    7->8->5    533.181      0.530      0.851351      0.863776
  118   7   6                       7->6    204.035      0.207      0.001799      0.002517
  119   7   8                       7->8   1167.430      1.167      0.217074      0.220293
  120   7   9                   7->10->9    760.903      0.761      0.134746      0.125201
  121   7  10                      7->10    178.979      0.180      0.000503      0.000518
  122   7  11                  7->10->11    462.253      0.464      0.001820      0.001892
  123   7  12                  7->10->12    241.888      0.243      0.006037      0.006248
  124   7  13              7->10->11->13    766.531      0.767      0.010358      0.010305
  125   7  14              7->10->12->14   1086.280      1.089      0.008147      0.007558
  126   7  15          7->10->12->14->15    941.744      0.939      1.332980      1.325329
  127   7  16              7->10->12->16    202.990      0.202      0.006893      0.007544
  128   8   0                       8->0    798.975      0.797      0.002113      0.002984
  129   8   1                    8->4->1   1133.480      1.136      1.287740      1.299950
  130   8   2                    8->0->2    382.171      0.379      0.230631      0.238126
  131   8   3                    8->4->3    391.200      0.392      0.051138      0.072116
  132   8   4                       8->4    953.954      0.959      0.005165      0.006225
  133   8   5                       8->5    898.637      0.894      0.508104      0.629550
  134   8   6                    8->7->6   1096.980      1.099      0.004220      0.004722
  135   8   7                       8->7   1019.580      1.017      0.000971      0.001006
  136   8   9                    8->4->9   1122.070      1.120      0.015607      0.018717
  137   8  10                   8->4->10   1259.330      1.261      0.023265      0.028578
  138   8  11               8->4->10->11    133.491      0.132      0.025507      0.030988
  139   8  12               8->4->10->12    598.790      0.600      0.032559      0.037461
  140   8  13           8->4->10->11->13   1136.630      1.141      0.036071      0.045905
  141   8  14           8->4->10->12->14    266.650      0.267      0.033301      0.037706
  142   8  15       8->4->10->12->14->15    634.286      0.634      1.445210      1.412615
  143   8  16           8->4->10->12->16   1040.970      1.039      0.094509      0.098715
  144   9   0                 9->4->8->0    540.878      0.541      0.021830      0.023447
  145   9   1                    9->4->1    532.220      0.531      1.314190      1.330144
  146   9   2                    9->4->2    503.813      0.503      0.021130      0.025107
  147   9   3                       9->3    145.289      0.144      0.010162      0.012228
  148   9   4                       9->4   1014.670      1.014      0.010675      0.012441
  149   9   5                 9->4->8->5   1229.280      1.228      0.556994      0.620353
  150   9   6                9->10->7->6    895.317      0.895      0.452785      0.404657
  151   9   7                   9->10->7    630.164      0.631      1.100300      0.925382
  152   9   8                    9->4->8   1111.900      1.118      0.014717      0.017959
  153   9  10                      9->10    491.842      0.492      0.403257      0.422468
  154   9  11                  9->10->11    670.124      0.669      0.411244      0.426569
  155   9  12                      9->12    430.292      0.429      0.000976      0.001070
  156   9  13              9->10->11->13    914.824      0.914      0.413546      0.417283
  157   9  14                  9->12->14    460.782      0.461      0.002954      0.003181
  158   9  15              9->12->14->15    335.251      0.336      1.320410      1.321885
  159   9  16                  9->12->16    192.720      0.192      0.002333      0.002465
  160  10   0                10->7->8->0   1026.140      1.023      0.270707      0.206527
  161  10   1                   10->4->1    427.217      0.427      1.275980      1.276665
  162  10   2                   10->4->2    801.559      0.804      0.008570      0.007724
  163  10   3                   10->4->3    613.696      0.613      0.051175      0.053803
  164  10   4                      10->4   1184.300      1.189      0.001483      0.001320
  165  10   5                10->7->8->5   1045.040      1.048      1.090130      0.836410
  166  10   6                   10->7->6    654.741      0.658      0.064206      0.073740
  167  10   7                      10->7    482.101      0.484      0.018405      0.010805
  168  10   8                   10->7->8   1233.410      1.234      0.266323      0.217305
  169  10   9                      10->9    669.844      0.675      0.122883      0.123647
  170  10  11                     10->11    621.281      0.623      0.000700      0.000693
  171  10  12                     10->12    311.760      0.312      0.004353      0.004641
  172  10  13                 10->11->13    876.586      0.874      0.008755      0.008046
  173  10  14                 10->12->14    550.180      0.548      0.006352      0.005935
  174  10  15             10->12->14->15    391.566      0.388      1.372500      1.293394
  175  10  16                 10->12->16    213.163      0.214      0.036491      0.039615
  176  11   0            11->10->7->8->0    643.051      0.644      0.373120      0.344809
  177  11   1               11->10->4->1   1134.670      1.133      1.217240      1.343700
  178  11   2               11->10->4->2    963.479      0.967      0.018376      0.016339
  179  11   3               11->10->4->3    157.748      0.158      0.062344      0.059774
  180  11   4                  11->10->4   1003.110      1.005      0.028095      0.022487
  181  11   5            11->10->7->8->5   1042.580      1.045      1.043850      0.886905
  182  11   6               11->10->7->6    652.006      0.654      0.032522      0.022731
  183  11   7                  11->10->7    843.255      0.843      0.029087      0.019789
  184  11   8               11->10->7->8    311.251      0.309      0.279239      0.217313
  185  11   9                  11->10->9   1071.450      1.069      0.604717      0.683182
  186  11  10                     11->10    853.105      0.851      0.006302      0.004061
  187  11  12                 11->10->12   1201.510      1.203      0.036707      0.032222
  188  11  13                     11->13    960.241      0.963      0.040262      0.045821
  189  11  14                 11->13->14    166.900      0.166      0.290688      0.257675
  190  11  15             11->13->14->15    261.062      0.259      3.583650      2.648080
  191  11  16             11->10->12->16    604.783      0.604      0.061406      0.045534
  192  12   0            12->10->7->8->0    342.494      0.341      0.286767      0.222222
  193  12   1               12->10->4->1   1197.510      1.201      1.282530      1.308007
  194  12   2               12->10->4->2    847.608      0.847      0.071679      0.073385
  195  12   3                   12->9->3   1202.420      1.197      0.016930      0.019378
  196  12   4                  12->10->4    227.620      0.229      0.009308      0.008526
  197  12   5            12->10->7->8->5   1169.050      1.172      0.942519      0.731962
  198  12   6               12->10->7->6    682.776      0.682      0.033347      0.025130
  199  12   7                  12->10->7    383.449      0.382      0.028293      0.020962
  200  12   8               12->10->7->8    560.205      0.560      0.283661      0.206615
  201  12   9                      12->9    837.566      0.839      0.002027      0.001539
  202  12  10                     12->10   1260.180      1.267      0.006740      0.006361
  203  12  11                 12->10->11   1090.090      1.093      0.008432      0.007484
  204  12  13                 12->14->13    551.751      0.551      0.121887      0.140682
  205  12  14                     12->14   1126.890      1.129      0.000993      0.000699
  206  12  15                 12->14->15    567.370      0.565      1.319640      1.307117
  207  12  16                     12->16    723.278      0.723      0.000379      0.000363
  208  13   0        13->11->10->7->8->0    759.038      0.760      0.432490      0.490340
  209  13   1           13->11->10->4->1   1071.230      1.074      1.324240      1.351371
  210  13   2           13->11->10->4->2    638.115      0.640      0.129989      0.117710
  211  13   3           13->14->12->9->3   1123.570      1.122      0.394026      0.276611
  212  13   4              13->11->10->4    978.811      0.979      0.020597      0.018683
  213  13   5        13->11->10->7->8->5    526.048      0.528      1.137550      0.956288
  214  13   6           13->11->10->7->6    422.395      0.424      0.046955      0.035993
  215  13   7              13->11->10->7   1242.590      1.242      0.041559      0.031210
  216  13   8           13->11->10->7->8    470.581      0.469      0.302338      0.227141
  217  13   9              13->14->12->9    410.444      0.407      0.280058      0.240022
  218  13  10                 13->11->10   1217.410      1.222      0.015381      0.013616
  219  13  11                     13->11    610.987      0.610      0.006964      0.007151
  220  13  12                 13->14->12    972.562      0.973      0.745265      0.688913
  221  13  14                     13->14    920.625      0.922      0.285134      0.266749
  222  13  15                 13->14->15    592.289      0.595      1.755400      1.619964
  223  13  16             13->14->15->16    324.716      0.325      2.325620      2.538522
  224  14   0        14->12->10->7->8->0    478.905      0.478      0.294804      0.230895
  225  14   1           14->12->10->4->1    456.774      0.455      1.293250      1.375015
  226  14   2           14->12->10->4->2    360.986      0.359      0.023413      0.020909
  227  14   3               14->12->9->3    726.167      0.726      0.019648      0.019891
  228  14   4              14->12->10->4    993.496      0.992      0.012602      0.010545
  229  14   5        14->12->10->7->8->5   1241.270      1.241      1.119990      0.919278
  230  14   6           14->12->10->7->6    950.698      0.944      0.039120      0.030596
  231  14   7              14->12->10->7    265.600      0.266      0.033116      0.023344
  232  14   8           14->12->10->7->8    637.712      0.638      0.409763      0.326911
  233  14   9                  14->12->9    272.381      0.273      0.078550      0.059068
  234  14  10                 14->12->10    242.992      0.243      0.009867      0.008362
  235  14  11                 14->13->11    275.129      0.275      0.122369      0.148133
  236  14  12                     14->12    486.485      0.490      0.000905      0.000901
  237  14  13                     14->13    374.648      0.376      0.125271      0.127537
  238  14  15                     14->15    765.337      0.763      1.213060      1.187068
  239  14  16                 14->15->16    443.972      0.446      2.475650      2.396828
  240  15   0    15->16->12->10->7->8->0    638.136      0.635      0.910103      0.848111
  241  15   1       15->16->12->10->4->1    945.967      0.943      1.856340      2.120796
  242  15   2       15->16->12->10->4->2    534.136      0.534      1.146520      1.179733
  243  15   3           15->16->12->9->3    162.012      0.160      0.589327      0.621963
  244  15   4          15->16->12->10->4    560.931      0.562      0.627141      0.580471
  245  15   5    15->16->12->10->7->8->5    320.368      0.319      2.716740      2.542395
  246  15   6       15->16->12->10->7->6    423.502      0.419      0.614164      0.580178
  247  15   7          15->16->12->10->7    249.592      0.249      0.613830      0.574098
  248  15   8       15->16->12->10->7->8    613.144      0.614      0.940185      0.822233
  249  15   9              15->16->12->9   1152.230      1.155      0.560914      0.511920
  250  15  10             15->16->12->10    556.351      0.559      0.619200      0.557594
  251  15  11             15->14->13->11    519.430      0.518      0.291038      0.291900
  252  15  12                 15->16->12    657.741      0.658      0.550045      0.527312
  253  15  13                 15->14->13    765.461      0.769      1.006110      1.114245
  254  15  14                     15->14    252.132      0.251      0.590672      0.728715
  255  15  16                     15->16    307.867      0.305      0.546509      0.532523
  256  16   0        16->12->10->7->8->0   1244.940      1.244      0.295477      0.255618
  257  16   1           16->12->10->4->1    314.115      0.314      1.306190      1.353821
  258  16   2           16->12->10->4->2    746.152      0.744      0.085011      0.102921
  259  16   3               16->12->9->3    626.423      0.627      0.020422      0.022244
  260  16   4              16->12->10->4    863.726      0.864      0.014942      0.013360
  261  16   5        16->12->10->7->8->5   1065.580      1.064      1.107360      0.912560
  262  16   6           16->12->10->7->6    516.318      0.514      0.039906      0.032073
  263  16   7              16->12->10->7    223.309      0.222      0.156541      0.124525
  264  16   8           16->12->10->7->8   1260.900      1.263      0.290766      0.226527
  265  16   9                  16->12->9    231.741      0.232      0.005058      0.005144
  266  16  10                 16->12->10    804.757      0.805      0.011695      0.011213
  267  16  11             16->12->10->11    837.993      0.840      0.012728      0.011743
  268  16  12                     16->12    801.612      0.803      0.002308      0.002305
  269  16  13             16->12->14->13    754.715      0.755      0.647988      0.884525
  270  16  14                 16->12->14    905.777      0.911      0.004182      0.003438
  271  16  15                     16->15    266.153      0.267      0.004951      0.005849
