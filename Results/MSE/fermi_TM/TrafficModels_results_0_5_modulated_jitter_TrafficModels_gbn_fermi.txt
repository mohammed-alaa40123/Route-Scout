2025-12-18 19:30:28.583563: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/spack/gilbreth-r9/apps/cuda/12.6.0-gcc-11.5.0-a7cv7sp/lib64:/apps/spack/gilbreth-r9/apps/libxml2/2.10.3-gcc-11.5.0-wroyiwb/lib:/apps/spack/gilbreth-r9/apps/libiconv/1.17-gcc-11.5.0-36njf44/lib:/apps/spack/gilbreth-r9/apps/libfabric/1.19.1-gcc-11.5.0-b5idhcg/lib64:/apps/spack/gilbreth-r9/apps/libfabric/1.19.1-gcc-11.5.0-b5idhcg/lib:/apps/spack/gilbreth-r9/apps/openmpi/4.1.6-gcc-11.5.0-w7wc45k/lib
2025-12-18 19:30:28.583999: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2025-12-18 19:30:31.013733: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2025-12-18 19:30:31.013806: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: gilbreth-fe03.rcac.purdue.edu
2025-12-18 19:30:31.013821: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: gilbreth-fe03.rcac.purdue.edu
2025-12-18 19:30:31.013909: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 565.57.1
2025-12-18 19:30:31.013937: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 565.57.1
2025-12-18 19:30:31.013947: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 565.57.1
2025-12-18 19:30:31.014596: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-12-18 19:30:35.813975: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
2025-12-18 19:30:36.705278: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:907] Skipping loop optimization for Merge node with control input: Assert_1/AssertGuard/branch_executed/_1027
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).path_update.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).path_update.recurrent_kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).path_update.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).link_update.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).link_update.recurrent_kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).link_update.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).queue_update.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).queue_update.recurrent_kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).queue_update.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).path_embedding.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).path_embedding.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).path_embedding.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).path_embedding.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).queue_embedding.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).queue_embedding.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).queue_embedding.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).queue_embedding.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).link_embedding.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).link_embedding.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).link_embedding.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).link_embedding.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).readout_path.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).readout_path.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).readout_path.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).readout_path.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).readout_path.layer_with_weights-2.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).readout_path.layer_with_weights-2.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).path_update.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).path_update.recurrent_kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).path_update.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).link_update.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).link_update.recurrent_kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).link_update.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).queue_update.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).queue_update.recurrent_kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).queue_update.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).path_embedding.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).path_embedding.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).path_embedding.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).path_embedding.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).queue_embedding.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).queue_embedding.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).queue_embedding.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).queue_embedding.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).link_embedding.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).link_embedding.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).link_embedding.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).link_embedding.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).readout_path.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).readout_path.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).readout_path.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).readout_path.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).readout_path.layer_with_weights-2.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).readout_path.layer_with_weights-2.bias
WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.

=== FERMI MODE B (metric=jitter) ===
Dataset directory : /scratch/gilbreth/mohame43/Routenet/RouteNet-Fermi/data/traffic_models/modulated/test/gbn-modulated
Checkpoint dir    : /scratch/gilbreth/mohame43/Routenet/RouteNet-Fermi/traffic_models/jitter/ckpt_dir_mape_modulated
Sample index      : 0
Routes file       : /scratch/gilbreth/mohame43/Routenet/candidate_routes_0_5_test_gbn.txt

Loaded baseline sample (Fermi): n_paths = 272
BEST CHECKPOINT FOUND (Fermi): 30-13.57

====================================================================================================
[Fermi] Candidate 1: src=0, dst=5, path = 0->8->5
====================================================================================================
  idx src dst            route_nodes    traffic    packets   true_jitter_orig   pred_jitter
-------------------------------------------------------------------------------------------
    0   0   1                    0->2->1    950.317      0.953      1.370080      1.818077
    1   0   2                       0->2   1118.710      1.119      0.840610      0.840322
    2   0   3                 0->2->1->3    610.118      0.611      2.111490      3.354820
    3   0   4                    0->8->4    567.984      0.564      0.174996      0.208710
    4   0   5 *                  0->8->5   1145.850      1.146      1.695760      1.666367
    5   0   6                 0->8->7->6    438.206      0.441      0.217693      0.221190
    6   0   7                    0->8->7   1047.980      1.050      0.118970      0.129280
    7   0   8                       0->8    696.593      0.698      0.124676      0.116458
    8   0   9                 0->8->4->9   1188.900      1.191      0.781145      0.939490
    9   0  10                0->8->4->10    534.550      0.535      0.292383      0.293310
   10   0  11            0->8->4->10->11    145.898      0.146      0.385018      0.345434
   11   0  12            0->8->4->10->12   1221.160      1.225      0.344835      0.330072
   12   0  13        0->8->4->10->11->13   1126.200      1.129      0.361527      0.319405
   13   0  14        0->8->4->10->12->14    670.521      0.673      0.384076      0.374231
   14   0  15    0->8->4->10->12->14->15    645.570      0.642      2.303000      2.170479
   15   0  16        0->8->4->10->12->16    908.015      0.911      0.345164      0.360880
   16   1   0                    1->2->0    860.701      0.864      2.014000      2.074246
   17   1   2                       1->2    742.714      0.746      0.699505      0.662598
   18   1   3                       1->3    728.429      0.727      0.690675      0.730042
   19   1   4                       1->4    660.905      0.661      1.199350      1.200332
   20   1   5                 1->4->8->5   1022.870      1.021      3.197960      3.036426
   21   1   6             1->4->10->7->6    393.487      0.393      1.464620      1.643885
   22   1   7                1->4->10->7    140.037      0.139      1.463790      1.497625
   23   1   8                    1->4->8    293.544      0.292      1.237210      1.310856
   24   1   9                    1->3->9    845.023      0.843      2.642480      2.552258
   25   1  10                   1->4->10   1147.010      1.148      1.286960      1.284279
   26   1  11               1->4->10->11    524.469      0.528      1.364260      1.351269
   27   1  12               1->4->10->12   1172.390      1.170      1.346290      1.360468
   28   1  13           1->4->10->11->13    188.123      0.188      1.407640      1.426570
   29   1  14           1->4->10->12->14    882.044      0.882      1.368030      1.519217
   30   1  15       1->4->10->12->14->15    362.496      0.363      3.522030      3.468709
   31   1  16           1->4->10->12->16    990.600      0.992      1.374780      1.408241
   32   2   0                       2->0    552.180      0.552      1.017570      0.933274
   33   2   1                       2->1    205.762      0.208      0.972626      0.868373
   34   2   3                    2->1->3   1261.990      1.264      1.159980      1.559358
   35   2   4                       2->4    894.711      0.891      0.050550      0.053395
   36   2   5                 2->0->8->5    311.665      0.313      3.187490      3.351012
   37   2   6              2->0->8->7->6    796.668      0.797      1.110050      1.141276
   38   2   7                 2->0->8->7    664.070      0.666      1.108530      1.040615
   39   2   8                    2->0->8    845.887      0.845      1.092200      0.967447
   40   2   9                    2->4->9    237.873      0.239      0.959427      1.053776
   41   2  10                   2->4->10    500.869      0.503      0.181644      0.151379
   42   2  11               2->4->10->11    336.106      0.338      0.251396      0.205316
   43   2  12               2->4->10->12    891.259      0.888      0.210658      0.208663
   44   2  13           2->4->10->11->13    247.171      0.247      0.295598      0.266827
   45   2  14           2->4->10->12->14    885.708      0.890      0.234739      0.211013
   46   2  15       2->4->10->12->14->15   1251.670      1.251      2.070640      2.067426
   47   2  16           2->4->10->12->16    692.698      0.699      0.236112      0.212051
   48   3   0                 3->1->2->0    828.425      0.830      2.694820      3.087771
   49   3   1                       3->1    150.515      0.151      0.714360      0.669108
   50   3   2                    3->1->2    450.810      0.454      1.385830      1.732868
   51   3   4                       3->4    567.009      0.567      0.512542      0.557919
   52   3   5                 3->4->8->5    255.229      0.255      2.497920      2.236325
   53   3   6             3->9->10->7->6   1161.060      1.156      3.294990      3.217299
   54   3   7                3->9->10->7    212.729      0.213      3.301670      3.127006
   55   3   8                    3->4->8    775.146      0.774      0.505296      0.669396
   56   3   9                       3->9    429.252      0.430      1.260480      1.218758
   57   3  10                   3->9->10   1155.740      1.161      2.960790      2.996172
   58   3  11               3->9->10->11    792.514      0.791      3.150370      2.983476
   59   3  12                   3->9->12    437.195      0.438      1.261810      1.349171
   60   3  13           3->9->10->11->13    887.828      0.887      3.126550      3.090071
   61   3  14               3->9->12->14    863.102      0.864      1.237780      1.442556
   62   3  15           3->9->12->14->15    444.552      0.447      3.273380      3.694934
   63   3  16               3->9->12->16    586.714      0.587      1.225560      1.371962
   64   4   0                    4->8->0    146.469      0.147      0.161677      0.130127
   65   4   1                       4->1    545.385      0.544      1.150530      1.161542
   66   4   2                       4->2    902.845      0.904      0.036815      0.047871
   67   4   3                       4->3    934.537      0.936      0.797070      0.827505
   68   4   5                    4->8->5   1246.070      1.245      1.521080      1.410577
   69   4   6                4->10->7->6    987.027      0.990      0.204123      0.210279
   70   4   7                   4->10->7    820.666      0.818      0.137200      0.141941
   71   4   8                       4->8    862.528      0.861      0.006372      0.009809
   72   4   9                       4->9   1212.960      1.209      0.584188      0.597246
   73   4  10                      4->10    654.485      0.654      0.075436      0.063540
   74   4  11                  4->10->11    931.661      0.930      0.082570      0.076299
   75   4  12                  4->10->12   1026.650      1.029      0.128723      0.115578
   76   4  13              4->10->11->13    572.555      0.572      0.148173      0.181111
   77   4  14              4->10->12->14    409.165      0.406      0.134196      0.143979
   78   4  15          4->10->12->14->15   1129.480      1.134      1.879720      1.757249
   79   4  16              4->10->12->16    818.681      0.823      0.132440      0.123439
   80   5   0                    5->8->0    704.299      0.707      1.309840      1.450394
   81   5   1                 5->8->4->1    388.018      0.387      2.916640      2.844143
   82   5   2                 5->8->0->2    609.497      0.609      2.256760      2.542680
   83   5   3                 5->8->4->3    669.773      0.670      2.237480      2.323438
   84   5   4                    5->8->4    475.667      0.477      1.273510      1.362374
   85   5   6                       5->6    457.115      0.456      0.611272      0.591407
   86   5   7                    5->6->7    848.175      0.843      0.660145      0.726825
   87   5   8                       5->8   1145.350      1.145      1.233500      1.262481
   88   5   9                 5->8->4->9    473.911      0.472      1.928550      1.917309
   89   5  10                5->8->4->10    540.779      0.543      1.437580      1.431739
   90   5  11            5->8->4->10->11    788.199      0.787      1.424360      1.440484
   91   5  12            5->8->4->10->12   1243.110      1.246      1.454440      1.466108
   92   5  13        5->8->4->10->11->13    630.739      0.627      1.499580      1.362614
   93   5  14        5->8->4->10->12->14    276.221      0.276      1.577600      1.461876
   94   5  15    5->8->4->10->12->14->15    632.852      0.632      3.526900      2.902171
   95   5  16        5->8->4->10->12->16    421.213      0.421      1.520060      1.409322
   96   6   0                 6->7->8->0    939.551      0.940      0.468021      0.442995
   97   6   1              6->7->8->4->1    227.058      0.226      1.926630      2.225216
   98   6   2              6->7->8->0->2    952.916      0.951      1.512450      1.529019
   99   6   3              6->7->8->4->3   1096.930      1.097      1.409160      1.482988
  100   6   4                 6->7->8->4   1128.280      1.128      0.472659      0.480560
  101   6   5                       6->5    699.235      0.696      0.407469      0.427689
  102   6   7                       6->7    270.972      0.270      0.121787      0.128368
  103   6   8                    6->7->8   1135.930      1.134      0.430276      0.339962
  104   6   9                6->7->10->9    781.868      0.782      0.851338      0.873967
  105   6  10                   6->7->10    168.575      0.170      0.131191      0.188505
  106   6  11               6->7->10->11    917.001      0.922      0.125486      0.171914
  107   6  12               6->7->10->12    612.674      0.613      0.195442      0.229823
  108   6  13           6->7->10->11->13   1123.100      1.126      0.200810      0.212082
  109   6  14           6->7->10->12->14    138.757      0.139      0.230471      0.267782
  110   6  15       6->7->10->12->14->15    148.751      0.148      2.198050      2.281845
  111   6  16           6->7->10->12->16   1225.200      1.224      0.174695      0.185400
  112   7   0                    7->8->0    326.628      0.326      0.263295      0.325457
  113   7   1                 7->8->4->1    702.506      0.703      1.502670      1.714946
  114   7   2                 7->8->0->2    367.311      0.368      1.283040      1.526526
  115   7   3                 7->8->4->3    326.469      0.325      1.049960      1.249359
  116   7   4                    7->8->4    367.260      0.367      0.275322      0.311688
  117   7   5                    7->8->5   1230.700      1.228      1.960880      1.792298
  118   7   6                       7->6    931.821      0.936      0.060063      0.077877
  119   7   8                       7->8   1023.750      1.022      0.237723      0.262043
  120   7   9                   7->10->9    844.141      0.844      0.750335      0.730155
  121   7  10                      7->10    929.098      0.928      0.011822      0.015816
  122   7  11                  7->10->11   1204.150      1.203      0.025056      0.032238
  123   7  12                  7->10->12    919.858      0.919      0.072155      0.073131
  124   7  13              7->10->11->13    810.317      0.806      0.087907      0.134861
  125   7  14              7->10->12->14   1227.610      1.229      0.079448      0.073733
  126   7  15          7->10->12->14->15    574.865      0.577      1.892060      1.742054
  127   7  16              7->10->12->16    338.168      0.340      0.103486      0.095615
  128   8   0                       8->0   1203.050      1.206      0.054721      0.057499
  129   8   1                    8->4->1    654.582      0.656      1.274970      1.280935
  130   8   2                    8->0->2    369.867      0.368      1.367930      1.222782
  131   8   3                    8->4->3    211.631      0.211      1.045130      1.082819
  132   8   4                       8->4    625.853      0.627      0.037633      0.054967
  133   8   5                       8->5   1141.250      1.141      1.423100      1.353384
  134   8   6                    8->7->6    569.340      0.570      0.147262      0.086527
  135   8   7                       8->7    748.968      0.748      0.035171      0.037634
  136   8   9                    8->4->9    232.834      0.232      1.087910      0.989441
  137   8  10                   8->4->10    679.626      0.681      0.135727      0.120231
  138   8  11               8->4->10->11    728.494      0.731      0.148261      0.151589
  139   8  12               8->4->10->12    949.192      0.948      0.196494      0.181105
  140   8  13           8->4->10->11->13    356.586      0.362      0.216928      0.224512
  141   8  14           8->4->10->12->14   1152.260      1.152      0.202084      0.184490
  142   8  15       8->4->10->12->14->15    144.552      0.145      2.053010      1.842695
  143   8  16           8->4->10->12->16   1138.660      1.141      0.187262      0.177133
  144   9   0                 9->4->8->0    579.314      0.578      0.982289      1.040573
  145   9   1                    9->4->1    495.839      0.495      2.728570      2.511300
  146   9   2                    9->4->2    861.721      0.859      0.965097      1.000446
  147   9   3                       9->3    496.149      0.494      1.052680      0.930848
  148   9   4                       9->4   1043.450      1.046      0.919739      0.911923
  149   9   5                 9->4->8->5    911.896      0.913      2.974100      2.654807
  150   9   6                9->10->7->6    921.247      0.921      1.383490      1.519902
  151   9   7                   9->10->7   1030.570      1.029      1.227640      1.316443
  152   9   8                    9->4->8    956.031      0.963      0.903692      0.952492
  153   9  10                      9->10    490.272      0.490      1.265710      1.239736
  154   9  11                  9->10->11    741.895      0.741      1.236050      1.322749
  155   9  12                      9->12    945.898      0.952      0.042027      0.041626
  156   9  13              9->10->11->13    615.952      0.618      1.317690      1.505301
  157   9  14                  9->12->14    867.576      0.867      0.063055      0.070493
  158   9  15              9->12->14->15    440.762      0.440      1.793390      1.808625
  159   9  16                  9->12->16   1072.440      1.069      0.037666      0.057613
  160  10   0                10->7->8->0    296.665      0.297      0.449694      0.423021
  161  10   1                   10->4->1    437.905      0.437      1.338020      1.287800
  162  10   2                   10->4->2    955.584      0.953      0.057207      0.069147
  163  10   3                   10->4->3    673.247      0.675      0.970239      0.963288
  164  10   4                      10->4    334.055      0.335      0.025016      0.022211
  165  10   5                10->7->8->5    323.301      0.321      2.404910      1.916402
  166  10   6                   10->7->6    541.158      0.541      0.110779      0.156839
  167  10   7                      10->7    165.106      0.165      0.063474      0.064701
  168  10   8                   10->7->8    977.023      0.981      0.381858      0.295319
  169  10   9                      10->9    386.267      0.382      0.707951      0.755019
  170  10  11                     10->11    773.123      0.771      0.018882      0.020885
  171  10  12                     10->12    182.134      0.181      0.059069      0.053060
  172  10  13                 10->11->13    465.600      0.468      0.115533      0.104377
  173  10  14                 10->12->14    903.096      0.905      0.059750      0.050393
  174  10  15             10->12->14->15    139.924      0.142      1.778450      1.693812
  175  10  16                 10->12->16    111.827      0.112      0.065844      0.065530
  176  11   0            11->10->7->8->0   1030.310      1.030      0.486956      0.419244
  177  11   1               11->10->4->1    943.766      0.937      1.326700      1.367796
  178  11   2               11->10->4->2    208.255      0.206      0.186033      0.144473
  179  11   3               11->10->4->3    799.802      0.801      0.921633      1.049749
  180  11   4                  11->10->4   1143.420      1.140      0.033777      0.036036
  181  11   5            11->10->7->8->5    379.056      0.380      2.430860      2.061579
  182  11   6               11->10->7->6    397.771      0.396      0.195615      0.213250
  183  11   7                  11->10->7    702.244      0.700      0.104317      0.100179
  184  11   8               11->10->7->8   1052.070      1.051      0.443871      0.404513
  185  11   9                  11->10->9    938.564      0.944      0.701624      0.668550
  186  11  10                     11->10    459.708      0.460      0.025821      0.024086
  187  11  12                 11->10->12    367.497      0.366      0.084757      0.101339
  188  11  13                     11->13    128.530      0.130      0.094497      0.093027
  189  11  14                 11->13->14   1111.330      1.108      1.503680      1.399883
  190  11  15             11->13->14->15   1262.440      1.259      3.543290      3.323842
  191  11  16             11->10->12->16   1002.250      1.003      0.078703      0.082150
  192  12   0            12->10->7->8->0    868.500      0.873      0.530614      0.439181
  193  12   1               12->10->4->1    605.537      0.603      1.447250      1.375761
  194  12   2               12->10->4->2   1067.520      1.069      0.101009      0.106662
  195  12   3                   12->9->3    595.678      0.595      1.183380      1.145036
  196  12   4                  12->10->4    413.914      0.413      0.071229      0.073816
  197  12   5            12->10->7->8->5    595.664      0.594      2.362720      2.055289
  198  12   6               12->10->7->6   1244.390      1.245      0.162873      0.184319
  199  12   7                  12->10->7   1037.000      1.037      0.132214      0.123708
  200  12   8               12->10->7->8   1017.850      1.017      0.488094      0.429017
  201  12   9                      12->9    596.075      0.599      0.056731      0.051031
  202  12  10                     12->10    583.298      0.588      0.051140      0.057076
  203  12  11                 12->10->11   1186.350      1.188      0.060889      0.061508
  204  12  13                 12->14->13    175.833      0.175      0.851138      0.948521
  205  12  14                     12->14    137.777      0.136      0.028571      0.023889
  206  12  15                 12->14->15   1029.580      1.027      1.627360      1.425005
  207  12  16                     12->16   1134.090      1.137      0.007228      0.009179
  208  13   0        13->11->10->7->8->0    230.598      0.229      0.636506      0.586941
  209  13   1           13->11->10->4->1   1065.930      1.065      1.342310      1.564899
  210  13   2           13->11->10->4->2    605.809      0.606      0.160878      0.173584
  211  13   3           13->14->12->9->3   1077.120      1.078      2.463600      2.491279
  212  13   4              13->11->10->4    952.127      0.957      0.096774      0.117097
  213  13   5        13->11->10->7->8->5    428.307      0.427      2.747680      2.304571
  214  13   6           13->11->10->7->6    186.644      0.185      0.320344      0.310829
  215  13   7              13->11->10->7    687.504      0.693      0.174917      0.188965
  216  13   8           13->11->10->7->8    646.771      0.646      0.566016      0.468039
  217  13   9              13->14->12->9    642.845      0.644      1.260930      1.445654
  218  13  10                 13->11->10    448.158      0.451      0.072410      0.113166
  219  13  11                     13->11   1078.550      1.079      0.051178      0.061416
  220  13  12                 13->14->12   1032.260      1.035      1.223860      1.274186
  221  13  14                     13->14    887.064      0.891      1.210940      1.211948
  222  13  15                 13->14->15   1036.490      1.035      3.364840      2.915788
  223  13  16             13->14->15->16    914.502      0.914      4.761270      5.064684
  224  14   0        14->12->10->7->8->0    553.605      0.552      0.641258      0.576727
  225  14   1           14->12->10->4->1    143.725      0.144      1.601170      1.651071
  226  14   2           14->12->10->4->2    554.881      0.555      0.163327      0.171406
  227  14   3               14->12->9->3   1027.370      1.022      1.237240      1.151993
  228  14   4              14->12->10->4    184.853      0.187      0.126305      0.143685
  229  14   5        14->12->10->7->8->5    670.482      0.672      2.513550      2.107529
  230  14   6           14->12->10->7->6    730.276      0.730      0.246470      0.254518
  231  14   7              14->12->10->7    684.878      0.685      0.179212      0.170725
  232  14   8           14->12->10->7->8   1237.740      1.237      0.526628      0.486860
  233  14   9                  14->12->9    664.123      0.664      0.072386      0.063019
  234  14  10                 14->12->10    616.590      0.616      0.086525      0.090219
  235  14  11                 14->13->11    308.304      0.310      0.738936      1.009588
  236  14  12                     14->12    903.155      0.904      0.010111      0.014588
  237  14  13                     14->13    759.215      0.758      0.608942      0.668368
  238  14  15                     14->15    567.689      0.570      1.423020      1.378909
  239  14  16                 14->15->16    308.602      0.310      2.894610      3.022945
  240  15   0    15->16->12->10->7->8->0   1167.140      1.170      1.750200      1.541837
  241  15   1       15->16->12->10->4->1    232.966      0.232      2.564040      2.656471
  242  15   2       15->16->12->10->4->2   1147.220      1.148      1.280440      1.607384
  243  15   3           15->16->12->9->3   1063.950      1.060      2.283810      2.393009
  244  15   4          15->16->12->10->4    585.660      0.585      1.182780      1.583975
  245  15   5    15->16->12->10->7->8->5    612.161      0.610      3.768670      3.604734
  246  15   6       15->16->12->10->7->6    922.733      0.924      1.373390      1.609564
  247  15   7          15->16->12->10->7    947.471      0.949      1.331410      1.563739
  248  15   8       15->16->12->10->7->8    864.045      0.864      1.769980      1.713483
  249  15   9              15->16->12->9    984.144      0.985      1.214790      1.524893
  250  15  10             15->16->12->10   1045.890      1.043      1.286990      1.487070
  251  15  11             15->14->13->11    400.315      0.402      1.359880      1.647188
  252  15  12                 15->16->12    799.937      0.802      1.209840      1.331639
  253  15  13                 15->14->13    781.134      0.779      1.182450      1.529723
  254  15  14                     15->14   1226.120      1.226      0.504380      0.519293
  255  15  16                     15->16   1132.940      1.130      1.178470      1.274895
  256  16   0        16->12->10->7->8->0   1176.980      1.174      0.568768      0.542720
  257  16   1           16->12->10->4->1    224.844      0.226      1.683030      1.645559
  258  16   2           16->12->10->4->2    841.559      0.839      0.140396      0.177882
  259  16   3               16->12->9->3   1214.000      1.210      1.148540      1.129655
  260  16   4              16->12->10->4    614.830      0.614      0.115371      0.137207
  261  16   5        16->12->10->7->8->5    963.350      0.959      2.476410      2.180495
  262  16   6           16->12->10->7->6    172.735      0.174      0.352165      0.327085
  263  16   7              16->12->10->7    269.490      0.270      0.207019      0.219290
  264  16   8           16->12->10->7->8    535.624      0.539      0.562523      0.566794
  265  16   9                  16->12->9    720.403      0.723      0.115960      0.083485
  266  16  10                 16->12->10    583.240      0.583      0.097323      0.119510
  267  16  11             16->12->10->11    902.361      0.904      0.106945      0.134440
  268  16  12                     16->12   1231.400      1.229      0.019245      0.035072
  269  16  13             16->12->14->13    211.276      0.211      0.981069      0.899748
  270  16  14                 16->12->14    496.268      0.498      0.049683      0.049400
  271  16  15                     16->15    426.713      0.428      0.588342      0.543074
