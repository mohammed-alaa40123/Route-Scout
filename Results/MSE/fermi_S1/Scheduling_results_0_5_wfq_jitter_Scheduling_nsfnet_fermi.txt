2025-12-18 19:43:18.213184: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/spack/gilbreth-r9/apps/cuda/12.6.0-gcc-11.5.0-a7cv7sp/lib64:/apps/spack/gilbreth-r9/apps/libxml2/2.10.3-gcc-11.5.0-wroyiwb/lib:/apps/spack/gilbreth-r9/apps/libiconv/1.17-gcc-11.5.0-36njf44/lib:/apps/spack/gilbreth-r9/apps/libfabric/1.19.1-gcc-11.5.0-b5idhcg/lib64:/apps/spack/gilbreth-r9/apps/libfabric/1.19.1-gcc-11.5.0-b5idhcg/lib:/apps/spack/gilbreth-r9/apps/openmpi/4.1.6-gcc-11.5.0-w7wc45k/lib
2025-12-18 19:43:18.213810: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2025-12-18 19:43:23.084694: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2025-12-18 19:43:23.084768: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: gilbreth-fe03.rcac.purdue.edu
2025-12-18 19:43:23.084783: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: gilbreth-fe03.rcac.purdue.edu
2025-12-18 19:43:23.084856: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 565.57.1
2025-12-18 19:43:23.084891: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 565.57.1
2025-12-18 19:43:23.084903: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 565.57.1
2025-12-18 19:43:23.085390: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-12-18 19:43:27.915219: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
2025-12-18 19:43:28.253374: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:907] Skipping loop optimization for Merge node with control input: Assert_1/AssertGuard/branch_executed/_1027
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).path_update.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).path_update.recurrent_kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).path_update.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).link_update.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).link_update.recurrent_kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).link_update.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).queue_update.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).queue_update.recurrent_kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).queue_update.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).path_embedding.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).path_embedding.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).path_embedding.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).path_embedding.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).queue_embedding.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).queue_embedding.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).queue_embedding.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).queue_embedding.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).link_embedding.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).link_embedding.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).link_embedding.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).link_embedding.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).readout_path.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).readout_path.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).readout_path.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).readout_path.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).readout_path.layer_with_weights-2.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).readout_path.layer_with_weights-2.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).path_update.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).path_update.recurrent_kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).path_update.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).link_update.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).link_update.recurrent_kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).link_update.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).queue_update.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).queue_update.recurrent_kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).queue_update.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).path_embedding.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).path_embedding.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).path_embedding.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).path_embedding.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).queue_embedding.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).queue_embedding.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).queue_embedding.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).queue_embedding.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).link_embedding.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).link_embedding.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).link_embedding.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).link_embedding.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).readout_path.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).readout_path.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).readout_path.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).readout_path.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).readout_path.layer_with_weights-2.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).readout_path.layer_with_weights-2.bias
WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.

=== FERMI SCHEDULING MODE B (metric=jitter) ===
Dataset directory : /scratch/gilbreth/mohame43/Routenet/RouteNet-Fermi/data/scheduling/train
Checkpoint dir    : /scratch/gilbreth/mohame43/Routenet/RouteNet-Fermi/scheduling/jitter/ckpt_dir
Sample index      : 1
Routes file       : /scratch/gilbreth/mohame43/Routenet/candidate_routes_scheduling_0_5_train_nsfnet.txt

[Fermi-Sched] Selected sample_index=1 (graph=graph-nsfnet-wfq-drr-sp-91.txt, dataset_file=/scratch/gilbreth/mohame43/Routenet/RouteNet-Fermi/data/scheduling/train/gnnet_data_set_0_99.tar.gz)
Baseline sample (Fermi-Sched) uses topology graph: graph-nsfnet-wfq-drr-sp-91.txt
Loaded baseline sample (Fermi-Sched): n_paths = 182
BEST CHECKPOINT FOUND (Fermi-Sched): 09-29.42028

========================================================================================================================
[Fermi-Sched] Candidate 1: src=0, dst=5, path = 0->2->5
========================================================================================================================
  idx src dst            route_nodes    traffic    packets   true_jitter_orig   pred_jitter
-------------------------------------------------------------------------------------------
    0   0   1                       0->1    350.733      0.353      0.006351      0.005763
    1   0   2                       0->2    111.101      0.112      0.007337      0.005551
    2   0   3                       0->3    258.200      0.258      0.008333      0.007329
    3   0   4                    0->3->4    493.234      0.494      0.013466      0.012697
    4   0   5 *                  0->2->5    588.126      0.583      0.012336      0.009266
    5   0   6                 0->3->4->6    372.817      0.374      0.041521      0.035324
    6   0   7                    0->1->7    240.129      0.241      0.025163      0.020202
    7   0   8                    0->3->8    258.277      0.256      0.017962      0.015727
    8   0   9                 0->3->8->9    147.789      0.149      0.038706      0.027301
    9   0  10                0->1->7->10    108.129      0.108      0.033688      0.028457
   10   0  11                0->3->8->11    345.508      0.347      0.042127      0.033662
   11   0  12                0->2->5->12    505.095      0.506      0.045224      0.040449
   12   0  13                0->2->5->13    255.027      0.257      0.031247      0.022959
   13   1   0                       1->0    162.849      0.161      0.007773      0.005895
   14   1   2                       1->2     82.633      0.083      0.005487      0.003725
   15   1   3                    1->0->3    122.067      0.124      0.026534      0.021381
   16   1   4                 1->0->3->4    509.072      0.508      0.033565      0.029189
   17   1   5                    1->2->5    617.634      0.616      0.010855      0.009049
   18   1   6                    1->7->6    191.583      0.192      0.030769      0.025588
   19   1   7                       1->7    160.025      0.160      0.008693      0.007908
   20   1   8                 1->0->3->8    402.704      0.400      0.040936      0.035446
   21   1   9                1->7->10->9    132.946      0.135      0.033819      0.028370
   22   1  10                   1->7->10    303.605      0.305      0.011725      0.010919
   23   1  11               1->7->10->11    134.045      0.136      0.031507      0.026865
   24   1  12                1->2->5->12    148.972      0.151      0.031680      0.024063
   25   1  13               1->7->10->13    433.557      0.432      0.044993      0.034909
   26   2   0                       2->0    190.995      0.192      0.008840      0.007509
   27   2   1                       2->1    565.903      0.560      0.008054      0.008423
   28   2   3                    2->0->3     78.591      0.079      0.032189      0.027402
   29   2   4                    2->5->4    189.344      0.191      0.003569      0.003041
   30   2   5                       2->5    114.723      0.114      0.001036      0.001018
   31   2   6                 2->1->7->6    300.381      0.303      0.054370      0.044006
   32   2   7                    2->1->7    292.969      0.291      0.027821      0.022567
   33   2   8                 2->0->3->8    547.243      0.548      0.040147      0.034485
   34   2   9                2->5->12->9    177.298      0.178      0.035309      0.037681
   35   2  10               2->5->13->10    627.557      0.630      0.060170      0.042770
   36   2  11               2->5->12->11    202.620      0.203      0.030925      0.025159
   37   2  12                   2->5->12    277.315      0.277      0.023593      0.025083
   38   2  13                   2->5->13     77.592      0.079      0.016846      0.015335
   39   3   0                       3->0    121.744      0.121      0.011274      0.008206
   40   3   1                    3->0->1    565.382      0.565      0.026752      0.021050
   41   3   2                    3->0->2    211.780      0.208      0.023968      0.018183
   42   3   4                       3->4    179.525      0.179      0.001051      0.001109
   43   3   5                    3->4->5    396.187      0.398      0.003596      0.003348
   44   3   6                    3->4->6    498.710      0.501      0.011709      0.011951
   45   3   7                 3->4->6->7    263.760      0.266      0.037507      0.037608
   46   3   8                       3->8    461.886      0.461      0.001054      0.001224
   47   3   9                    3->8->9    341.854      0.343      0.010965      0.011213
   48   3  10               3->8->11->10    558.500      0.558      0.039208      0.031440
   49   3  11                   3->8->11    244.100      0.241      0.014552      0.014145
   50   3  12                3->4->5->12    359.492      0.359      0.031102      0.031513
   51   3  13                3->4->5->13    312.158      0.312      0.024819      0.020973
   52   4   0                    4->3->0    523.663      0.524      0.015830      0.011378
   53   4   1                 4->5->2->1    627.854      0.624      0.022485      0.017733
   54   4   2                    4->5->2    548.045      0.553      0.003958      0.003315
   55   4   3                       4->3    534.491      0.536      0.000958      0.000870
   56   4   5                       4->5    457.098      0.456      0.001224      0.001177
   57   4   6                       4->6    615.021      0.612      0.009336      0.008406
   58   4   7                    4->6->7    292.752      0.291      0.028890      0.029994
   59   4   8                    4->3->8    461.596      0.460      0.003522      0.003848
   60   4   9                4->5->12->9    160.252      0.160      0.035869      0.034853
   61   4  10               4->5->13->10    266.594      0.265      0.036428      0.036796
   62   4  11               4->5->12->11    129.441      0.128      0.042310      0.042220
   63   4  12                   4->5->12    196.546      0.198      0.024389      0.025954
   64   4  13                   4->5->13    120.768      0.118      0.017299      0.016479
   65   5   0                    5->2->0    575.576      0.573      0.012240      0.010717
   66   5   1                    5->2->1    506.009      0.503      0.011753      0.009702
   67   5   2                       5->2    483.893      0.484      0.001398      0.001291
   68   5   3                    5->4->3    239.641      0.242      0.003454      0.002779
   69   5   4                       5->4    344.733      0.348      0.000969      0.000910
   70   5   6                    5->4->6    297.634      0.299      0.014044      0.013086
   71   5   7               5->13->10->7     79.292      0.078      0.036877      0.038055
   72   5   8                5->12->9->8    186.678      0.184      0.071945      0.063219
   73   5   9                   5->12->9    628.914      0.635      0.024562      0.020594
   74   5  10                  5->13->10    472.940      0.474      0.028222      0.029308
   75   5  11                  5->12->11    440.526      0.441      0.035669      0.030864
   76   5  12                      5->12    365.017      0.367      0.019116      0.020218
   77   5  13                      5->13    249.683      0.246      0.012492      0.010248
   78   6   0                 6->7->1->0    261.366      0.264      0.052633      0.045392
   79   6   1                    6->7->1    186.274      0.187      0.040415      0.039223
   80   6   2                 6->4->5->2    450.374      0.455      0.018237      0.016606
   81   6   3                    6->4->3    363.647      0.368      0.012096      0.011272
   82   6   4                       6->4    181.638      0.179      0.010019      0.010777
   83   6   5                    6->4->5    564.920      0.564      0.015563      0.015519
   84   6   7                       6->7    498.242      0.504      0.010353      0.011764
   85   6   8                 6->4->3->8    407.040      0.407      0.021196      0.017174
   86   6   9                6->7->10->9    669.780      0.667      0.044732      0.042652
   87   6  10                   6->7->10    156.872      0.157      0.015277      0.016408
   88   6  11               6->7->10->11    366.933      0.368      0.037960      0.036161
   89   6  12                6->4->5->12    373.253      0.374      0.047334      0.048224
   90   6  13               6->7->10->13    513.780      0.515      0.044569      0.041314
   91   7   0                    7->1->0    517.172      0.515      0.039727      0.032172
   92   7   1                       7->1    504.604      0.507      0.011431      0.013587
   93   7   2                    7->1->2     67.741      0.068      0.025896      0.021422
   94   7   3                 7->1->0->3    133.408      0.133      0.067329      0.056057
   95   7   4                    7->6->4    542.271      0.546      0.030409      0.028239
   96   7   5               7->10->13->5    467.751      0.471      0.045791      0.041857
   97   7   6                       7->6    361.011      0.363      0.010682      0.009522
   98   7   8                7->10->9->8    385.098      0.382      0.048083      0.045687
   99   7   9                   7->10->9    165.634      0.167      0.021901      0.017557
  100   7  10                      7->10    610.749      0.615      0.001078      0.001218
  101   7  11                  7->10->11    549.932      0.551      0.014487      0.013049
  102   7  12              7->10->11->12    447.954      0.449      0.038844      0.033386
  103   7  13                  7->10->13    349.534      0.347      0.022225      0.017831
  104   8   0                    8->3->0    463.880      0.463      0.015883      0.014013
  105   8   1                 8->3->0->1    241.105      0.240      0.031035      0.027563
  106   8   2                 8->3->0->2    296.739      0.298      0.036193      0.032591
  107   8   3                       8->3     78.352      0.080      0.001071      0.001029
  108   8   4                    8->3->4    155.211      0.155      0.003792      0.002960
  109   8   5               8->11->12->5    197.298      0.193      0.054408      0.045111
  110   8   6                 8->3->4->6    126.649      0.126      0.017946      0.018115
  111   8   7               8->11->10->7    186.801      0.190      0.040361      0.031399
  112   8   9                       8->9    605.263      0.607      0.006000      0.006594
  113   8  10                  8->11->10    341.915      0.341      0.026427      0.022143
  114   8  11                      8->11    136.252      0.135      0.009635      0.008025
  115   8  12                  8->11->12    643.663      0.642      0.030421      0.025590
  116   8  13              8->11->10->13     92.329      0.090      0.052337      0.044878
  117   9   0                 9->8->3->0    199.466      0.198      0.047199      0.043219
  118   9   1                9->10->7->1    465.408      0.467      0.046738      0.039784
  119   9   2                9->12->5->2    276.374      0.276      0.032656      0.030744
  120   9   3                    9->8->3    560.058      0.557      0.025958      0.021369
  121   9   4                 9->8->3->4    505.870      0.505      0.020795      0.020718
  122   9   5                   9->12->5    134.203      0.132      0.041123      0.039378
  123   9   6                9->10->7->6    314.226      0.314      0.036424      0.029068
  124   9   7                   9->10->7    210.290      0.214      0.011785      0.006203
  125   9   8                       9->8    514.261      0.514      0.022001      0.019285
  126   9  10                      9->10    320.785      0.319      0.008335      0.006798
  127   9  11                   9->8->11    315.008      0.314      0.030781      0.026382
  128   9  12                      9->12    102.054      0.102      0.005264      0.005616
  129   9  13                  9->10->13    500.274      0.505      0.034867      0.025576
  130  10   0                10->7->1->0    549.615      0.551      0.031339      0.024196
  131  10   1                   10->7->1    441.260      0.439      0.012856      0.009539
  132  10   2               10->13->5->2    314.626      0.318      0.045974      0.044321
  133  10   3                10->9->8->3    493.172      0.493      0.047980      0.043871
  134  10   4               10->13->5->4    224.161      0.227      0.033115      0.025489
  135  10   5                  10->13->5    561.609      0.564      0.037226      0.033755
  136  10   6                   10->7->6    640.012      0.639      0.012253      0.011659
  137  10   7                      10->7    190.659      0.193      0.001360      0.001112
  138  10   8                   10->9->8    278.678      0.282      0.037086      0.035695
  139  10   9                      10->9    171.741      0.171      0.016035      0.015204
  140  10  11                     10->11    476.917      0.473      0.007151      0.006139
  141  10  12                 10->11->12    110.029      0.112      0.030467      0.027755
  142  10  13                     10->13    617.159      0.615      0.016319      0.013787
  143  11   0                11->8->3->0    390.232      0.393      0.032722      0.029156
  144  11   1               11->10->7->1    375.134      0.372      0.036493      0.035900
  145  11   2               11->12->5->2    429.402      0.425      0.033609      0.033350
  146  11   3                   11->8->3    504.924      0.507      0.011036      0.008172
  147  11   4               11->12->5->4    372.025      0.373      0.052413      0.045022
  148  11   5                  11->12->5    549.660      0.545      0.045471      0.040671
  149  11   6               11->10->7->6    320.268      0.318      0.033156      0.030296
  150  11   7                  11->10->7    451.357      0.448      0.017703      0.013780
  151  11   8                      11->8    566.978      0.569      0.007151      0.004998
  152  11   9                  11->12->9     94.161      0.094      0.033004      0.034082
  153  11  10                     11->10    492.193      0.497      0.012077      0.011931
  154  11  12                     11->12    446.849      0.449      0.011302      0.012073
  155  11  13                 11->10->13    456.907      0.460      0.039921      0.029080
  156  12   0                12->5->2->0    274.872      0.272      0.049649      0.047093
  157  12   1                12->5->2->1    424.743      0.424      0.049262      0.048095
  158  12   2                   12->5->2    335.421      0.334      0.014214      0.014512
  159  12   3                12->9->8->3    361.318      0.363      0.040995      0.044703
  160  12   4                   12->5->4    613.395      0.614      0.028549      0.022784
  161  12   5                      12->5    377.906      0.379      0.009165      0.011364
  162  12   6                12->5->4->6    138.171      0.140      0.052012      0.046514
  163  12   7               12->9->10->7    118.213      0.119      0.040186      0.033585
  164  12   8                   12->9->8    281.547      0.277      0.040228      0.038834
  165  12   9                      12->9    569.182      0.570      0.008815      0.008568
  166  12  10                  12->9->10    574.781      0.571      0.031342      0.026981
  167  12  11                     12->11    571.793      0.575      0.006070      0.003558
  168  12  13                  12->5->13    406.946      0.408      0.026988      0.028423
  169  13   0                13->5->2->0    490.410      0.490      0.031789      0.026272
  170  13   1               13->10->7->1    468.552      0.464      0.071369      0.050819
  171  13   2                   13->5->2     78.226      0.080      0.014923      0.019310
  172  13   3                13->5->4->3    295.932      0.298      0.022563      0.022766
  173  13   4                   13->5->4    126.084      0.127      0.015994      0.016736
  174  13   5                      13->5    410.061      0.405      0.007516      0.007064
  175  13   6               13->10->7->6     83.415      0.084      0.063350      0.041582
  176  13   7                  13->10->7    324.093      0.325      0.015571      0.016496
  177  13   8               13->10->9->8    545.961      0.543      0.067558      0.062745
  178  13   9                  13->10->9    639.088      0.637      0.028920      0.027548
  179  13  10                     13->10    226.672      0.227      0.010565      0.011967
  180  13  11                 13->10->11    533.247      0.536      0.027937      0.025717
  181  13  12                  13->5->12    251.260      0.249      0.038648      0.043324
