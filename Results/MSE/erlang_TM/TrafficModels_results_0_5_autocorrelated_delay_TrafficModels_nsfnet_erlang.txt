2025-12-18 19:42:39.063673: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/spack/gilbreth-r9/apps/cuda/12.6.0-gcc-11.5.0-a7cv7sp/lib64:/apps/spack/gilbreth-r9/apps/libxml2/2.10.3-gcc-11.5.0-wroyiwb/lib:/apps/spack/gilbreth-r9/apps/libiconv/1.17-gcc-11.5.0-36njf44/lib:/apps/spack/gilbreth-r9/apps/libfabric/1.19.1-gcc-11.5.0-b5idhcg/lib64:/apps/spack/gilbreth-r9/apps/libfabric/1.19.1-gcc-11.5.0-b5idhcg/lib:/apps/spack/gilbreth-r9/apps/openmpi/4.1.6-gcc-11.5.0-w7wc45k/lib
2025-12-18 19:42:39.064192: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2025-12-18 19:42:41.495028: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2025-12-18 19:42:41.495101: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: gilbreth-fe03.rcac.purdue.edu
2025-12-18 19:42:41.495115: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: gilbreth-fe03.rcac.purdue.edu
2025-12-18 19:42:41.495212: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 565.57.1
2025-12-18 19:42:41.495242: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 565.57.1
2025-12-18 19:42:41.495251: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 565.57.1
2025-12-18 19:42:41.495537: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-12-18 19:42:42.522712: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
WARNING:tensorflow:Unresolved object in checkpoint: (root).link_embedding
WARNING:tensorflow:Unresolved object in checkpoint: (root).path_embedding
WARNING:tensorflow:Unresolved object in checkpoint: (root).aggr_mlp
WARNING:tensorflow:Unresolved object in checkpoint: (root).link_embedding.layer_with_weights-0
WARNING:tensorflow:Unresolved object in checkpoint: (root).link_embedding.layer_with_weights-1
WARNING:tensorflow:Unresolved object in checkpoint: (root).path_embedding.layer_with_weights-0
WARNING:tensorflow:Unresolved object in checkpoint: (root).path_embedding.layer_with_weights-1
WARNING:tensorflow:Unresolved object in checkpoint: (root).aggr_mlp.layer-0
WARNING:tensorflow:Unresolved object in checkpoint: (root).aggr_mlp.layer_with_weights-0
WARNING:tensorflow:Unresolved object in checkpoint: (root).aggr_mlp.layer-2
WARNING:tensorflow:Unresolved object in checkpoint: (root).aggr_mlp.layer_with_weights-1
WARNING:tensorflow:Unresolved object in checkpoint: (root).aggr_mlp.layer-4
WARNING:tensorflow:Unresolved object in checkpoint: (root).aggr_mlp.layer_with_weights-2
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate
WARNING:tensorflow:Unresolved object in checkpoint: (root).link_embedding.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).link_embedding.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).link_embedding.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).link_embedding.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).path_embedding.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).path_embedding.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).path_embedding.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).path_embedding.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).aggr_mlp.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).aggr_mlp.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).aggr_mlp.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).aggr_mlp.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).aggr_mlp.layer_with_weights-2.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).aggr_mlp.layer_with_weights-2.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).link_update.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).link_update.recurrent_kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).link_update.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).path_update.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).path_update.recurrent_kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).path_update.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).link_update.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).link_update.recurrent_kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).link_update.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).path_update.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).path_update.recurrent_kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).path_update.bias
WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.

=== ERLANG MODE B ===
Dataset directory : /scratch/gilbreth/mohame43/Routenet/RouteNet-Erlang/data/traffic_models/autocorrelated/train/nsfnet-autocorrelated
Experiment dir    : /scratch/gilbreth/mohame43/Routenet/RouteNet-Erlang/TrafficModels/Delay/autocorrelated
Metric / label    : delay
Sample index      : 0
Routes file       : /scratch/gilbreth/mohame43/Routenet/candidate_routes_0_5_train_nsfnet.txt

Loaded baseline sample: n_paths = 182, n_links = 42
BEST CHECKPOINT FOUND (Erlang): 198-3.23

====================================================================================================
[Erlang] Candidate 1: src=0, dst=5, path = 0->2->5
====================================================================================================
  idx src dst            route_nodes    traffic    packets   time_params[0:4] true_delay_orig   pred_delay
----------------------------------------------------------------------------------------------------------
    0   0   1                       0->1    950.996      0.951        [0. 0. 0. 0.]      0.203384      0.194493
    1   0   2                       0->2    756.477      0.754        [0. 0. 0. 0.]      0.148357      0.142163
    2   0   3                       0->3    869.635      0.870        [0. 0. 0. 0.]      0.309208      0.290962
    3   0   4                    0->3->4   1153.040      1.154        [0. 0. 0. 0.]      0.346392      0.338303
    4   0   5 *                  0->2->5    566.117      0.569        [0. 0. 0. 0.]      0.196491      0.192787
    5   0   6                 0->1->7->6    737.684      0.737        [0. 0. 0. 0.]      0.819106      0.788761
    6   0   7                    0->1->7   1191.700      1.192        [0. 0. 0. 0.]      0.622993      0.581944
    7   0   8                    0->3->8    226.220      0.227        [0. 0. 0. 0.]      0.369240      0.347190
    8   0   9                 0->3->8->9   1087.080      1.085        [0. 0. 0. 0.]      0.510340      0.499332
    9   0  10                0->1->7->10   1120.720      1.128        [0. 0. 0. 0.]      0.648824      0.639816
   10   0  11                0->3->8->11   1047.130      1.047        [0. 0. 0. 0.]      0.527418      0.500829
   11   0  12                0->2->5->12   1022.130      1.025        [0. 0. 0. 0.]      0.571856      0.526902
   12   0  13                0->2->5->13    860.969      0.862        [0. 0. 0. 0.]      0.615806      0.563497
   13   1   0                       1->0   1216.410      1.213        [0. 0. 0. 0.]      0.149838      0.142827
   14   1   2                       1->2    605.016      0.604        [0. 0. 0. 0.]      0.145251      0.141608
   15   1   3                    1->0->3   1023.580      1.019        [0. 0. 0. 0.]      0.470314      0.444041
   16   1   4                 1->0->3->4    779.641      0.782        [0. 0. 0. 0.]      0.530231      0.494692
   17   1   5                    1->2->5    837.155      0.838        [0. 0. 0. 0.]      0.189882      0.188150
   18   1   6                    1->7->6    842.462      0.838        [0. 0. 0. 0.]      0.586962      0.582344
   19   1   7                       1->7    909.031      0.912        [0. 0. 0. 0.]      0.405031      0.380935
   20   1   8                 1->0->3->8    629.704      0.631        [0. 0. 0. 0.]      0.517943      0.497524
   21   1   9                1->7->10->9    193.369      0.198        [0. 0. 0. 0.]      0.637140      0.617630
   22   1  10                   1->7->10    900.564      0.900        [0. 0. 0. 0.]      0.461364      0.437656
   23   1  11               1->7->10->11    276.972      0.275        [0. 0. 0. 0.]      0.611931      0.588380
   24   1  12                1->2->5->12    545.926      0.546        [0. 0. 0. 0.]      0.558119      0.527700
   25   1  13               1->7->10->13    632.698      0.632        [0. 0. 0. 0.]      0.621531      0.609723
   26   2   0                       2->0    242.107      0.245        [0. 0. 0. 0.]      0.117327      0.116470
   27   2   1                       2->1    315.499      0.314        [0. 0. 0. 0.]      0.115787      0.113908
   28   2   3                    2->0->3    415.798      0.418        [0. 0. 0. 0.]      0.429928      0.413933
   29   2   4                    2->5->4    414.973      0.410        [0. 0. 0. 0.]      0.095749      0.094444
   30   2   5                       2->5    256.580      0.255        [0. 0. 0. 0.]      0.049796      0.049482
   31   2   6                 2->1->7->6    286.609      0.286        [0. 0. 0. 0.]      0.732576      0.709672
   32   2   7                    2->1->7    548.554      0.552        [0. 0. 0. 0.]      0.507180      0.499494
   33   2   8                 2->0->3->8    239.071      0.239        [0. 0. 0. 0.]      0.479952      0.470060
   34   2   9                2->5->12->9    238.676      0.239        [0. 0. 0. 0.]      0.653113      0.616225
   35   2  10               2->5->13->10    668.616      0.666        [0. 0. 0. 0.]      0.915266      0.863699
   36   2  11               2->5->12->11    821.637      0.824        [0. 0. 0. 0.]      0.525604      0.498368
   37   2  12                   2->5->12    169.016      0.172        [0. 0. 0. 0.]      0.425665      0.387122
   38   2  13                   2->5->13    266.965      0.265        [0. 0. 0. 0.]      0.460321      0.421187
   39   3   0                       3->0    265.782      0.265        [0. 0. 0. 0.]      0.384658      0.350229
   40   3   1                    3->0->1    601.328      0.603        [0. 0. 0. 0.]      0.596505      0.546065
   41   3   2                    3->0->2    924.028      0.923        [0. 0. 0. 0.]      0.521732      0.486659
   42   3   4                       3->4    431.646      0.433        [0. 0. 0. 0.]      0.050872      0.050628
   43   3   5                    3->4->5    236.023      0.236        [0. 0. 0. 0.]      0.099403      0.098501
   44   3   6                    3->4->6   1194.290      1.196        [0. 0. 0. 0.]      0.208711      0.197504
   45   3   7                 3->0->1->7    896.603      0.893        [0. 0. 0. 0.]      1.009380      0.929532
   46   3   8                       3->8    956.946      0.952        [0. 0. 0. 0.]      0.051268      0.050818
   47   3   9                    3->8->9    342.076      0.339        [0. 0. 0. 0.]      0.216380      0.212214
   48   3  10                3->8->9->10    150.251      0.151        [0. 0. 0. 0.]      0.435551      0.431037
   49   3  11                   3->8->11    135.246      0.134        [0. 0. 0. 0.]      0.221098      0.215145
   50   3  12               3->8->11->12    441.364      0.438        [0. 0. 0. 0.]      0.356890      0.352753
   51   3  13                3->4->5->13   1232.330      1.236        [0. 0. 0. 0.]      0.508701      0.464756
   52   4   0                    4->3->0    791.535      0.791        [0. 0. 0. 0.]      0.431472      0.401260
   53   4   1                 4->3->0->1    780.928      0.786        [0. 0. 0. 0.]      0.649434      0.598493
   54   4   2                    4->5->2   1220.060      1.223        [0. 0. 0. 0.]      0.094885      0.092788
   55   4   3                       4->3   1109.560      1.101        [0. 0. 0. 0.]      0.052082      0.051429
   56   4   5                       4->5    473.103      0.470        [0. 0. 0. 0.]      0.047791      0.047034
   57   4   6                       4->6    586.372      0.584        [0. 0. 0. 0.]      0.152395      0.150448
   58   4   7                    4->6->7    798.424      0.796        [0. 0. 0. 0.]      0.296688      0.293635
   59   4   8                    4->3->8    920.226      0.924        [0. 0. 0. 0.]      0.102920      0.102346
   60   4   9                 4->3->8->9    707.241      0.706        [0. 0. 0. 0.]      0.259984      0.260752
   61   4  10               4->5->13->10    868.116      0.869        [0. 0. 0. 0.]      0.934984      0.858809
   62   4  11                4->3->8->11    825.132      0.826        [0. 0. 0. 0.]      0.274219      0.258711
   63   4  12                   4->5->12    481.624      0.476        [0. 0. 0. 0.]      0.404507      0.381426
   64   4  13                   4->5->13    460.771      0.461        [0. 0. 0. 0.]      0.450133      0.416793
   65   5   0                    5->2->0    620.598      0.621        [0. 0. 0. 0.]      0.166300      0.160822
   66   5   1                    5->2->1    467.680      0.471        [0. 0. 0. 0.]      0.162686      0.160926
   67   5   2                       5->2    809.369      0.807        [0. 0. 0. 0.]      0.048203      0.047339
   68   5   3                    5->4->3    877.157      0.879        [0. 0. 0. 0.]      0.096869      0.096048
   69   5   4                       5->4    621.810      0.624        [0. 0. 0. 0.]      0.044389      0.044078
   70   5   6                    5->4->6    547.246      0.550        [0. 0. 0. 0.]      0.202761      0.195919
   71   5   7               5->13->10->7   1153.200      1.154        [0. 0. 0. 0.]      0.938281      0.881878
   72   5   8                5->12->9->8    936.355      0.939        [0. 0. 0. 0.]      0.975812      0.885479
   73   5   9                   5->12->9   1189.470      1.185        [0. 0. 0. 0.]      0.590896      0.551082
   74   5  10                  5->13->10   1280.920      1.276        [0. 0. 0. 0.]      0.882646      0.800537
   75   5  11                  5->12->11   1125.100      1.126        [0. 0. 0. 0.]      0.483418      0.443966
   76   5  12                      5->12    834.535      0.836        [0. 0. 0. 0.]      0.363476      0.326175
   77   5  13                      5->13   1101.830      1.103        [0. 0. 0. 0.]      0.392708      0.362392
   78   6   0                 6->4->3->0    783.190      0.783        [0. 0. 0. 0.]      0.595632      0.557637
   79   6   1                    6->7->1    204.998      0.207        [0. 0. 0. 0.]      0.446035      0.431818
   80   6   2                 6->7->1->2    650.772      0.649        [0. 0. 0. 0.]      0.575553      0.564523
   81   6   3                    6->4->3   1128.410      1.125        [0. 0. 0. 0.]      0.200905      0.202228
   82   6   4                       6->4   1109.700      1.113        [0. 0. 0. 0.]      0.158766      0.147795
   83   6   5                    6->4->5    546.107      0.542        [0. 0. 0. 0.]      0.204111      0.200439
   84   6   7                       6->7    327.736      0.326        [0. 0. 0. 0.]      0.151666      0.148062
   85   6   8                 6->4->3->8    190.086      0.189        [0. 0. 0. 0.]      0.267323      0.265748
   86   6   9                6->7->10->9    151.687      0.151        [0. 0. 0. 0.]      0.394200      0.384691
   87   6  10                   6->7->10    382.617      0.384        [0. 0. 0. 0.]      0.208374      0.204714
   88   6  11               6->7->10->11   1193.960      1.195        [0. 0. 0. 0.]      0.343983      0.340073
   89   6  12                6->4->5->12    164.477      0.166        [0. 0. 0. 0.]      0.576343      0.545762
   90   6  13               6->7->10->13    845.147      0.843        [0. 0. 0. 0.]      0.381984      0.373605
   91   7   0                    7->1->0    402.073      0.401        [0. 0. 0. 0.]      0.434821      0.428460
   92   7   1                       7->1    836.404      0.834        [0. 0. 0. 0.]      0.287377      0.271154
   93   7   2                    7->1->2    806.385      0.806        [0. 0. 0. 0.]      0.426021      0.413479
   94   7   3                 7->1->0->3    484.897      0.487        [0. 0. 0. 0.]      0.769171      0.729831
   95   7   4                    7->6->4    371.303      0.368        [0. 0. 0. 0.]      0.371803      0.360053
   96   7   5               7->10->13->5   1212.300      1.213        [0. 0. 0. 0.]      0.396953      0.390789
   97   7   6                       7->6    694.965      0.694        [0. 0. 0. 0.]      0.207655      0.196180
   98   7   8                7->10->9->8    422.978      0.422        [0. 0. 0. 0.]      0.603221      0.563943
   99   7   9                   7->10->9    628.554      0.629        [0. 0. 0. 0.]      0.233805      0.224362
  100   7  10                      7->10    924.967      0.929        [0. 0. 0. 0.]      0.056150      0.055724
  101   7  11                  7->10->11    336.399      0.335        [0. 0. 0. 0.]      0.206906      0.203429
  102   7  12              7->10->11->12    204.025      0.206        [0. 0. 0. 0.]      0.348402      0.349793
  103   7  13                  7->10->13    641.413      0.649        [0. 0. 0. 0.]      0.235226      0.227754
  104   8   0                    8->3->0   1158.230      1.159        [0. 0. 0. 0.]      0.418182      0.400443
  105   8   1                 8->3->0->1    375.262      0.377        [0. 0. 0. 0.]      0.642387      0.607445
  106   8   2                 8->3->0->2    427.994      0.430        [0. 0. 0. 0.]      0.596915      0.552474
  107   8   3                       8->3   1002.060      1.000        [0. 0. 0. 0.]      0.053688      0.053219
  108   8   4                    8->3->4    566.517      0.570        [0. 0. 0. 0.]      0.104130      0.103898
  109   8   5               8->11->12->5   1083.920      1.083        [0. 0. 0. 0.]      0.545643      0.532721
  110   8   6                 8->3->4->6   1134.910      1.131        [0. 0. 0. 0.]      0.265816      0.251942
  111   8   7                8->9->10->7   1044.790      1.045        [0. 0. 0. 0.]      0.457562      0.433237
  112   8   9                       8->9   1228.210      1.225        [0. 0. 0. 0.]      0.153326      0.150299
  113   8  10                   8->9->10    374.248      0.375        [0. 0. 0. 0.]      0.372956      0.370501
  114   8  11                      8->11    969.183      0.969        [0. 0. 0. 0.]      0.161730      0.151302
  115   8  12                  8->11->12    372.232      0.373        [0. 0. 0. 0.]      0.303436      0.301072
  116   8  13               8->9->10->13    156.281      0.156        [0. 0. 0. 0.]      0.569360      0.554026
  117   9   0                 9->8->3->0    617.431      0.615        [0. 0. 0. 0.]      0.804657      0.738575
  118   9   1                9->10->7->1    659.700      0.661        [0. 0. 0. 0.]      0.591468      0.558325
  119   9   2                9->12->5->2    808.157      0.802        [0. 0. 0. 0.]      0.402895      0.400023
  120   9   3                    9->8->3    265.941      0.263        [0. 0. 0. 0.]      0.410097      0.388573
  121   9   4                 9->8->3->4    278.295      0.280        [0. 0. 0. 0.]      0.459123      0.438877
  122   9   5                   9->12->5    584.732      0.584        [0. 0. 0. 0.]      0.362038      0.353447
  123   9   6                9->10->7->6    339.453      0.338        [0. 0. 0. 0.]      0.513881      0.492287
  124   9   7                   9->10->7    690.367      0.689        [0. 0. 0. 0.]      0.289559      0.276756
  125   9   8                       9->8   1210.950      1.209        [0. 0. 0. 0.]      0.343586      0.328517
  126   9  10                      9->10    988.611      0.988        [0. 0. 0. 0.]      0.204689      0.203311
  127   9  11                   9->8->11    223.161      0.224        [0. 0. 0. 0.]      0.518689      0.496465
  128   9  12                      9->12    794.612      0.800        [0. 0. 0. 0.]      0.110829      0.111604
  129   9  13                  9->10->13    464.918      0.463        [0. 0. 0. 0.]      0.399220      0.382003
  130  10   0                10->7->1->0    241.395      0.243        [0. 0. 0. 0.]      0.534352      0.512069
  131  10   1                   10->7->1   1192.140      1.195        [0. 0. 0. 0.]      0.362022      0.346738
  132  10   2                10->7->1->2   1032.760      1.031        [0. 0. 0. 0.]      0.513249      0.487788
  133  10   3                10->9->8->3    801.689      0.802        [0. 0. 0. 0.]      0.598372      0.549983
  134  10   4                10->7->6->4    687.574      0.687        [0. 0. 0. 0.]      0.428356      0.429671
  135  10   5                  10->13->5   1143.740      1.138        [0. 0. 0. 0.]      0.349742      0.333063
  136  10   6                   10->7->6   1242.260      1.235        [0. 0. 0. 0.]      0.284037      0.269382
  137  10   7                      10->7   1219.180      1.219        [0. 0. 0. 0.]      0.070973      0.069229
  138  10   8                   10->9->8   1052.090      1.048        [0. 0. 0. 0.]      0.528335      0.498132
  139  10   9                      10->9   1128.340      1.133        [0. 0. 0. 0.]      0.173114      0.160748
  140  10  11                     10->11   1107.930      1.105        [0. 0. 0. 0.]      0.140096      0.137138
  141  10  12                 10->11->12    140.038      0.143        [0. 0. 0. 0.]      0.289612      0.292081
  142  10  13                     10->13    297.650      0.298        [0. 0. 0. 0.]      0.175732      0.174320
  143  11   0                11->8->3->0    676.348      0.679        [0. 0. 0. 0.]      0.547501      0.511755
  144  11   1               11->10->7->1    860.497      0.865        [0. 0. 0. 0.]      0.505216      0.481347
  145  11   2               11->12->5->2    287.491      0.285        [0. 0. 0. 0.]      0.439051      0.436497
  146  11   3                   11->8->3    345.232      0.347        [0. 0. 0. 0.]      0.166501      0.164073
  147  11   4                11->8->3->4    384.355      0.386        [0. 0. 0. 0.]      0.218111      0.214802
  148  11   5                  11->12->5   1119.160      1.120        [0. 0. 0. 0.]      0.370415      0.373610
  149  11   6               11->10->7->6   1236.920      1.233        [0. 0. 0. 0.]      0.402471      0.400487
  150  11   7                  11->10->7   1012.440      1.018        [0. 0. 0. 0.]      0.204824      0.197771
  151  11   8                      11->8    221.457      0.221        [0. 0. 0. 0.]      0.112917      0.110803
  152  11   9                  11->12->9    394.798      0.395        [0. 0. 0. 0.]      0.392680      0.371408
  153  11  10                     11->10    189.859      0.190        [0. 0. 0. 0.]      0.134786      0.133685
  154  11  12                     11->12    139.739      0.140        [0. 0. 0. 0.]      0.143734      0.143728
  155  11  13                 11->10->13    296.510      0.298        [0. 0. 0. 0.]      0.314376      0.307295
  156  12   0                12->5->2->0    232.149      0.231        [0. 0. 0. 0.]      0.417332      0.410122
  157  12   1                12->5->2->1    413.449      0.411        [0. 0. 0. 0.]      0.409168      0.404481
  158  12   2                   12->5->2    768.263      0.769        [0. 0. 0. 0.]      0.283797      0.283553
  159  12   3                12->9->8->3    965.157      0.963        [0. 0. 0. 0.]      0.661675      0.610142
  160  12   4                   12->5->4    278.762      0.279        [0. 0. 0. 0.]      0.293491      0.284265
  161  12   5                      12->5    476.085      0.475        [0. 0. 0. 0.]      0.247027      0.236529
  162  12   6                12->5->4->6    649.998      0.653        [0. 0. 0. 0.]      0.440716      0.435519
  163  12   7               12->9->10->7    924.688      0.929        [0. 0. 0. 0.]      0.536079      0.504368
  164  12   8                   12->9->8    566.047      0.566        [0. 0. 0. 0.]      0.609619      0.559227
  165  12   9                      12->9   1029.540      1.035        [0. 0. 0. 0.]      0.246100      0.219276
  166  12  10                  12->9->10    923.881      0.930        [0. 0. 0. 0.]      0.445450      0.431874
  167  12  11                     12->11   1019.540      1.022        [0. 0. 0. 0.]      0.121255      0.116201
  168  12  13                  12->5->13    560.061      0.558        [0. 0. 0. 0.]      0.645196      0.604991
  169  13   0                13->5->2->0    441.067      0.441        [0. 0. 0. 0.]      0.340665      0.333784
  170  13   1               13->10->7->1    354.674      0.353        [0. 0. 0. 0.]      0.847351      0.793446
  171  13   2                   13->5->2    178.976      0.179        [0. 0. 0. 0.]      0.222561      0.220160
  172  13   3                13->5->4->3    215.649      0.216        [0. 0. 0. 0.]      0.272948      0.270997
  173  13   4                   13->5->4    483.453      0.481        [0. 0. 0. 0.]      0.215152      0.211446
  174  13   5                      13->5   1240.700      1.232        [0. 0. 0. 0.]      0.165157      0.161215
  175  13   6               13->10->7->6    165.227      0.168        [0. 0. 0. 0.]      0.762503      0.726244
  176  13   7                  13->10->7    714.297      0.713        [0. 0. 0. 0.]      0.543210      0.509207
  177  13   8               13->10->9->8    909.047      0.912        [0. 0. 0. 0.]      1.011120      0.939783
  178  13   9                  13->10->9    277.418      0.276        [0. 0. 0. 0.]      0.654171      0.609059
  179  13  10                     13->10   1225.230      1.228        [0. 0. 0. 0.]      0.473416      0.431709
  180  13  11                 13->10->11   1167.790      1.168        [0. 0. 0. 0.]      0.611690      0.571679
  181  13  12                  13->5->12    656.660      0.654        [0. 0. 0. 0.]      0.519572      0.498690
