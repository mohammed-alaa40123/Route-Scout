2025-12-18 19:42:44.017311: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/spack/gilbreth-r9/apps/cuda/12.6.0-gcc-11.5.0-a7cv7sp/lib64:/apps/spack/gilbreth-r9/apps/libxml2/2.10.3-gcc-11.5.0-wroyiwb/lib:/apps/spack/gilbreth-r9/apps/libiconv/1.17-gcc-11.5.0-36njf44/lib:/apps/spack/gilbreth-r9/apps/libfabric/1.19.1-gcc-11.5.0-b5idhcg/lib64:/apps/spack/gilbreth-r9/apps/libfabric/1.19.1-gcc-11.5.0-b5idhcg/lib:/apps/spack/gilbreth-r9/apps/openmpi/4.1.6-gcc-11.5.0-w7wc45k/lib
2025-12-18 19:42:44.017976: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2025-12-18 19:42:48.153420: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2025-12-18 19:42:48.153492: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: gilbreth-fe03.rcac.purdue.edu
2025-12-18 19:42:48.153506: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: gilbreth-fe03.rcac.purdue.edu
2025-12-18 19:42:48.153601: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 565.57.1
2025-12-18 19:42:48.153635: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 565.57.1
2025-12-18 19:42:48.153645: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 565.57.1
2025-12-18 19:42:48.154015: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-12-18 19:42:49.170528: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
WARNING:tensorflow:Unresolved object in checkpoint: (root).link_embedding
WARNING:tensorflow:Unresolved object in checkpoint: (root).path_embedding
WARNING:tensorflow:Unresolved object in checkpoint: (root).aggr_mlp
WARNING:tensorflow:Unresolved object in checkpoint: (root).link_embedding.layer_with_weights-0
WARNING:tensorflow:Unresolved object in checkpoint: (root).link_embedding.layer_with_weights-1
WARNING:tensorflow:Unresolved object in checkpoint: (root).path_embedding.layer_with_weights-0
WARNING:tensorflow:Unresolved object in checkpoint: (root).path_embedding.layer_with_weights-1
WARNING:tensorflow:Unresolved object in checkpoint: (root).aggr_mlp.layer-0
WARNING:tensorflow:Unresolved object in checkpoint: (root).aggr_mlp.layer_with_weights-0
WARNING:tensorflow:Unresolved object in checkpoint: (root).aggr_mlp.layer-2
WARNING:tensorflow:Unresolved object in checkpoint: (root).aggr_mlp.layer_with_weights-1
WARNING:tensorflow:Unresolved object in checkpoint: (root).aggr_mlp.layer-4
WARNING:tensorflow:Unresolved object in checkpoint: (root).aggr_mlp.layer_with_weights-2
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate
WARNING:tensorflow:Unresolved object in checkpoint: (root).link_embedding.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).link_embedding.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).link_embedding.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).link_embedding.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).path_embedding.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).path_embedding.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).path_embedding.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).path_embedding.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).aggr_mlp.layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).aggr_mlp.layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).aggr_mlp.layer_with_weights-1.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).aggr_mlp.layer_with_weights-1.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).aggr_mlp.layer_with_weights-2.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).aggr_mlp.layer_with_weights-2.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).link_update.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).link_update.recurrent_kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).link_update.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).path_update.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).path_update.recurrent_kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).path_update.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).link_update.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).link_update.recurrent_kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).link_update.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).path_update.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).path_update.recurrent_kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).path_update.bias
WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.

=== ERLANG MODE B ===
Dataset directory : /scratch/gilbreth/mohame43/Routenet/RouteNet-Erlang/data/traffic_models/constant_bitrate/train/nsfnet-constant
Experiment dir    : /scratch/gilbreth/mohame43/Routenet/RouteNet-Erlang/TrafficModels/Delay/constant_bitrate
Metric / label    : delay
Sample index      : 0
Routes file       : /scratch/gilbreth/mohame43/Routenet/candidate_routes_0_5_train_nsfnet.txt

Loaded baseline sample: n_paths = 182, n_links = 42
BEST CHECKPOINT FOUND (Erlang): 176-4.67

====================================================================================================
[Erlang] Candidate 1: src=0, dst=5, path = 0->2->5
====================================================================================================
  idx src dst            route_nodes    traffic    packets   time_params[0:4] true_delay_orig   pred_delay
----------------------------------------------------------------------------------------------------------
    0   0   1                       0->1    949.542      0.950        [0. 1. 0. 0.]      0.146144      0.148665
    1   0   2                       0->2   1111.420      1.115        [0. 1. 0. 0.]      0.155508      0.157340
    2   0   3                       0->3    613.870      0.615        [0. 1. 0. 0.]      0.190929      0.196819
    3   0   4                    0->3->4    571.892      0.570        [0. 1. 0. 0.]      0.239484      0.245248
    4   0   5 *                  0->2->5   1157.350      1.154        [0. 1. 0. 0.]      0.211592      0.213513
    5   0   6                 0->1->7->6    442.165      0.441        [0. 1. 0. 0.]      0.554906      0.558173
    6   0   7                    0->1->7   1030.680      1.039        [0. 1. 0. 0.]      0.376470      0.378352
    7   0   8                    0->3->8    679.705      0.680        [0. 1. 0. 0.]      0.242976      0.249798
    8   0   9                 0->3->8->9   1198.560      1.193        [0. 1. 0. 0.]      0.404236      0.408543
    9   0  10                0->1->7->10    512.944      0.516        [0. 1. 0. 0.]      0.450961      0.454701
   10   0  11                0->3->8->11    152.685      0.151        [0. 1. 0. 0.]      0.386206      0.382153
   11   0  12                0->2->5->12   1229.780      1.229        [0. 1. 0. 0.]      0.759008      0.705498
   12   0  13                0->2->5->13   1126.930      1.129        [0. 1. 0. 0.]      0.422472      0.420846
   13   1   0                       1->0    678.625      0.673        [0. 1. 0. 0.]      0.158769      0.158818
   14   1   2                       1->2    660.203      0.659        [0. 1. 0. 0.]      0.130318      0.131626
   15   1   3                    1->0->3    905.272      0.909        [0. 1. 0. 0.]      0.346972      0.350111
   16   1   4                 1->0->3->4    866.526      0.864        [0. 1. 0. 0.]      0.396074      0.397351
   17   1   5                    1->2->5    745.517      0.746        [0. 1. 0. 0.]      0.186089      0.187561
   18   1   6                    1->7->6    727.705      0.728        [0. 1. 0. 0.]      0.384904      0.386767
   19   1   7                       1->7    663.961      0.673        [0. 1. 0. 0.]      0.228192      0.231404
   20   1   8                 1->0->3->8   1023.680      1.018        [0. 1. 0. 0.]      0.398624      0.400631
   21   1   9                1->7->10->9    378.449      0.377        [0. 1. 0. 0.]      0.512258      0.508993
   22   1  10                   1->7->10    147.261      0.149        [0. 1. 0. 0.]      0.308414      0.306662
   23   1  11               1->7->10->11    298.140      0.300        [0. 1. 0. 0.]      0.446547      0.445940
   24   1  12                1->2->5->12    838.382      0.837        [0. 1. 0. 0.]      0.733121      0.675998
   25   1  13               1->7->10->13   1167.260      1.166        [0. 1. 0. 0.]      0.471213      0.471440
   26   2   0                       2->0    540.936      0.541        [0. 1. 0. 0.]      0.116599      0.117273
   27   2   1                       2->1   1160.180      1.166        [0. 1. 0. 0.]      0.131430      0.133160
   28   2   3                    2->0->3    197.812      0.197        [0. 1. 0. 0.]      0.326657      0.329043
   29   2   4                    2->5->4    881.115      0.879        [0. 1. 0. 0.]      0.104104      0.105103
   30   2   5                       2->5    367.300      0.370        [0. 1. 0. 0.]      0.056862      0.057277
   31   2   6                 2->1->7->6    994.534      0.991        [0. 1. 0. 0.]      0.517942      0.520806
   32   2   7                    2->1->7    546.633      0.546        [0. 1. 0. 0.]      0.373866      0.375946
   33   2   8                 2->0->3->8    173.236      0.172        [0. 1. 0. 0.]      0.380946      0.383767
   34   2   9                2->5->12->9   1269.940      1.265        [0. 1. 0. 0.]      0.784347      0.722111
   35   2  10               2->5->13->10    881.231      0.879        [0. 1. 0. 0.]      0.557256      0.549025
   36   2  11               2->5->12->11    312.840      0.313        [0. 1. 0. 0.]      0.714713      0.656404
   37   2  12                   2->5->12    797.589      0.797        [0. 1. 0. 0.]      0.601057      0.544960
   38   2  13                   2->5->13    663.531      0.664        [0. 1. 0. 0.]      0.271157      0.270459
   39   3   0                       3->0    852.071      0.845        [0. 1. 0. 0.]      0.423559      0.398457
   40   3   1                    3->0->1    257.889      0.255        [0. 1. 0. 0.]      0.588138      0.568763
   41   3   2                    3->0->2    490.203      0.493        [0. 1. 0. 0.]      0.590972      0.564228
   42   3   4                       3->4    355.203      0.354        [0. 1. 0. 0.]      0.048135      0.048686
   43   3   5                    3->4->5    888.404      0.890        [0. 1. 0. 0.]      0.092642      0.094089
   44   3   6                    3->4->6    239.928      0.240        [0. 1. 0. 0.]      0.187270      0.190524
   45   3   7                 3->0->1->7    901.504      0.903        [0. 1. 0. 0.]      0.807300      0.784981
   46   3   8                       3->8   1248.700      1.251        [0. 1. 0. 0.]      0.051591      0.052237
   47   3   9                    3->8->9    712.845      0.714        [0. 1. 0. 0.]      0.226634      0.227884
   48   3  10                3->8->9->10    820.063      0.824        [0. 1. 0. 0.]      0.481770      0.479277
   49   3  11                   3->8->11    151.184      0.151        [0. 1. 0. 0.]      0.176407      0.176647
   50   3  12               3->8->11->12    449.680      0.453        [0. 1. 0. 0.]      0.333428      0.335094
   51   3  13                3->4->5->13    541.190      0.541        [0. 1. 0. 0.]      0.309828      0.309196
   52   4   0                    4->3->0    261.699      0.264        [0. 1. 0. 0.]      0.488508      0.463279
   53   4   1                 4->3->0->1   1139.990      1.141        [0. 1. 0. 0.]      0.621099      0.599151
   54   4   2                    4->5->2    201.482      0.202        [0. 1. 0. 0.]      0.095367      0.097399
   55   4   3                       4->3    778.223      0.778        [0. 1. 0. 0.]      0.050759      0.050993
   56   4   5                       4->5    434.068      0.433        [0. 1. 0. 0.]      0.046191      0.046541
   57   4   6                       4->6   1168.420      1.182        [0. 1. 0. 0.]      0.126062      0.129859
   58   4   7                    4->6->7    789.955      0.790        [0. 1. 0. 0.]      0.296838      0.300246
   59   4   8                    4->3->8    447.842      0.447        [0. 1. 0. 0.]      0.105026      0.106910
   60   4   9                 4->3->8->9    887.776      0.896        [0. 1. 0. 0.]      0.272438      0.275338
   61   4  10               4->5->13->10    876.435      0.872        [0. 1. 0. 0.]      0.546453      0.536679
   62   4  11                4->3->8->11    461.710      0.461        [0. 1. 0. 0.]      0.223577      0.222494
   63   4  12                   4->5->12    585.013      0.585        [0. 1. 0. 0.]      0.585876      0.535314
   64   4  13                   4->5->13    153.038      0.151        [0. 1. 0. 0.]      0.279967      0.271262
   65   5   0                    5->2->0    555.336      0.553        [0. 1. 0. 0.]      0.166418      0.167469
   66   5   1                    5->2->1    910.308      0.907        [0. 1. 0. 0.]      0.184730      0.185597
   67   5   2                       5->2    935.911      0.937        [0. 1. 0. 0.]      0.047856      0.048404
   68   5   3                    5->4->3   1233.790      1.235        [0. 1. 0. 0.]      0.097096      0.098879
   69   5   4                       5->4    990.818      0.998        [0. 1. 0. 0.]      0.047773      0.048381
   70   5   6                    5->4->6    809.741      0.809        [0. 1. 0. 0.]      0.180676      0.182740
   71   5   7               5->13->10->7    862.484      0.863        [0. 1. 0. 0.]      0.558038      0.550625
   72   5   8                5->12->9->8   1217.310      1.224        [0. 1. 0. 0.]      3.624830      3.570725
   73   5   9                   5->12->9    641.702      0.645        [0. 1. 0. 0.]      0.730642      0.672485
   74   5  10                  5->13->10    935.096      0.932        [0. 1. 0. 0.]      0.496869      0.490070
   75   5  11                  5->12->11   1011.710      1.010        [0. 1. 0. 0.]      0.638959      0.590602
   76   5  12                      5->12    578.928      0.584        [0. 1. 0. 0.]      0.539144      0.483426
   77   5  13                      5->13    418.305      0.415        [0. 1. 0. 0.]      0.217626      0.217036
   78   6   0                 6->4->3->0   1147.550      1.142        [0. 1. 0. 0.]      0.620205      0.589531
   79   6   1                    6->7->1    802.030      0.805        [0. 1. 0. 0.]      0.398707      0.397858
   80   6   2                 6->7->1->2    708.396      0.704        [0. 1. 0. 0.]      0.532900      0.529067
   81   6   3                    6->4->3    392.184      0.393        [0. 1. 0. 0.]      0.201604      0.202324
   82   6   4                       6->4    618.526      0.615        [0. 1. 0. 0.]      0.146141      0.147097
   83   6   5                    6->4->5    671.457      0.674        [0. 1. 0. 0.]      0.190663      0.192470
   84   6   7                       6->7    474.008      0.475        [0. 1. 0. 0.]      0.172123      0.172205
   85   6   8                 6->4->3->8    462.842      0.459        [0. 1. 0. 0.]      0.255372      0.255768
   86   6   9                6->7->10->9    839.997      0.839        [0. 1. 0. 0.]      0.427545      0.427524
   87   6  10                   6->7->10   1140.980      1.138        [0. 1. 0. 0.]      0.221189      0.222775
   88   6  11               6->7->10->11    469.713      0.471        [0. 1. 0. 0.]      0.376408      0.374188
   89   6  12                6->4->5->12    535.658      0.535        [0. 1. 0. 0.]      0.737978      0.684300
   90   6  13               6->7->10->13    788.622      0.788        [0. 1. 0. 0.]      0.418005      0.417342
   91   7   0                    7->1->0   1239.480      1.242        [0. 1. 0. 0.]      0.378552      0.376140
   92   7   1                       7->1    630.239      0.624        [0. 1. 0. 0.]      0.233094      0.233353
   93   7   2                    7->1->2    287.641      0.290        [0. 1. 0. 0.]      0.374956      0.375377
   94   7   3                 7->1->0->3    625.374      0.629        [0. 1. 0. 0.]      0.589410      0.586567
   95   7   4                    7->6->4    429.586      0.427        [0. 1. 0. 0.]      0.310283      0.312432
   96   7   5               7->10->13->5    943.944      0.937        [0. 1. 0. 0.]      0.380335      0.380348
   97   7   6                       7->6    243.209      0.242        [0. 1. 0. 0.]      0.165870      0.167155
   98   7   8                7->10->9->8    947.870      0.949        [0. 1. 0. 0.]      3.166310      3.156808
   99   7   9                   7->10->9   1095.790      1.095        [0. 1. 0. 0.]      0.256247      0.259412
  100   7  10                      7->10   1122.690      1.126        [0. 1. 0. 0.]      0.059619      0.059763
  101   7  11                  7->10->11    710.887      0.713        [0. 1. 0. 0.]      0.199404      0.201724
  102   7  12              7->10->11->12    273.633      0.270        [0. 1. 0. 0.]      0.379469      0.376079
  103   7  13                  7->10->13   1131.230      1.123        [0. 1. 0. 0.]      0.245789      0.248566
  104   8   0                    8->3->0    787.245      0.782        [0. 1. 0. 0.]      0.487011      0.461313
  105   8   1                 8->3->0->1    166.352      0.166        [0. 1. 0. 0.]      0.658514      0.631149
  106   8   2                 8->3->0->2    929.534      0.930        [0. 1. 0. 0.]      0.644651      0.613405
  107   8   3                       8->3    609.931      0.611        [0. 1. 0. 0.]      0.057701      0.057949
  108   8   4                    8->3->4   1122.620      1.124        [0. 1. 0. 0.]      0.103280      0.104926
  109   8   5               8->11->12->5    146.278      0.147        [0. 1. 0. 0.]      0.596042      0.590202
  110   8   6                 8->3->4->6    139.107      0.141        [0. 1. 0. 0.]      0.247089      0.252088
  111   8   7                8->9->10->7   1215.250      1.214        [0. 1. 0. 0.]      0.479518      0.478508
  112   8   9                       8->9    325.234      0.325        [0. 1. 0. 0.]      0.178443      0.178897
  113   8  10                   8->9->10    689.906      0.691        [0. 1. 0. 0.]      0.429203      0.426719
  114   8  11                      8->11    362.209      0.358        [0. 1. 0. 0.]      0.118961      0.117642
  115   8  12                  8->11->12    325.936      0.324        [0. 1. 0. 0.]      0.284454      0.283322
  116   8  13               8->9->10->13    383.231      0.385        [0. 1. 0. 0.]      0.632794      0.626506
  117   9   0                 9->8->3->0   1215.400      1.214        [0. 1. 0. 0.]      3.374400      3.384061
  118   9   1                9->10->7->1    940.771      0.938        [0. 1. 0. 0.]      0.543516      0.541192
  119   9   2                9->12->5->2   1033.070      1.029        [0. 1. 0. 0.]      0.458101      0.452330
  120   9   3                    9->8->3    840.878      0.843        [0. 1. 0. 0.]      2.941870      2.936713
  121   9   4                 9->8->3->4    929.140      0.932        [0. 1. 0. 0.]      2.986620      2.985572
  122   9   5                   9->12->5   1201.340      1.202        [0. 1. 0. 0.]      0.407430      0.404325
  123   9   6                9->10->7->6    903.033      0.909        [0. 1. 0. 0.]      0.463688      0.466604
  124   9   7                   9->10->7    797.782      0.806        [0. 1. 0. 0.]      0.309297      0.312279
  125   9   8                       9->8   1237.820      1.237        [0. 1. 0. 0.]      2.884090      2.872613
  126   9  10                      9->10    589.280      0.586        [0. 1. 0. 0.]      0.252419      0.254900
  127   9  11                   9->8->11    377.358      0.384        [0. 1. 0. 0.]      3.000090      2.984961
  128   9  12                      9->12   1221.430      1.214        [0. 1. 0. 0.]      0.119115      0.119670
  129   9  13                  9->10->13    650.608      0.648        [0. 1. 0. 0.]      0.439664      0.442046
  130  10   0                10->7->1->0    395.314      0.389        [0. 1. 0. 0.]      0.470519      0.465579
  131  10   1                   10->7->1    226.752      0.226        [0. 1. 0. 0.]      0.313356      0.312071
  132  10   2                10->7->1->2    627.338      0.627        [0. 1. 0. 0.]      0.429175      0.427645
  133  10   3                10->9->8->3   1146.300      1.141        [0. 1. 0. 0.]      3.154760      3.142866
  134  10   4                10->7->6->4    562.853      0.562        [0. 1. 0. 0.]      0.370939      0.371134
  135  10   5                  10->13->5    750.719      0.752        [0. 1. 0. 0.]      0.319405      0.323789
  136  10   6                   10->7->6    204.515      0.205        [0. 1. 0. 0.]      0.231463      0.234525
  137  10   7                      10->7    680.537      0.678        [0. 1. 0. 0.]      0.062088      0.061907
  138  10   8                   10->9->8    747.829      0.745        [0. 1. 0. 0.]      3.100050      3.092406
  139  10   9                      10->9    952.771      0.950        [0. 1. 0. 0.]      0.194017      0.197783
  140  10  11                     10->11    375.217      0.377        [0. 1. 0. 0.]      0.142252      0.144046
  141  10  12                 10->11->12   1176.050      1.170        [0. 1. 0. 0.]      0.284538      0.286784
  142  10  13                     10->13    150.096      0.152        [0. 1. 0. 0.]      0.201021      0.203033
  143  11   0                11->8->3->0   1142.910      1.142        [0. 1. 0. 0.]      0.610663      0.580282
  144  11   1               11->10->7->1    570.691      0.569        [0. 1. 0. 0.]      0.432915      0.432108
  145  11   2               11->12->5->2    497.101      0.501        [0. 1. 0. 0.]      0.499536      0.500304
  146  11   3                   11->8->3    845.055      0.851        [0. 1. 0. 0.]      0.180570      0.183740
  147  11   4                11->8->3->4    481.225      0.487        [0. 1. 0. 0.]      0.232276      0.236952
  148  11   5                  11->12->5   1057.590      1.046        [0. 1. 0. 0.]      0.440400      0.438838
  149  11   6               11->10->7->6    904.501      0.910        [0. 1. 0. 0.]      0.342019      0.347946
  150  11   7                  11->10->7    920.735      0.919        [0. 1. 0. 0.]      0.188591      0.192207
  151  11   8                      11->8   1036.820      1.035        [0. 1. 0. 0.]      0.121798      0.123464
  152  11   9                  11->12->9    962.699      0.968        [0. 1. 0. 0.]      0.335917      0.343350
  153  11  10                     11->10    484.168      0.489        [0. 1. 0. 0.]      0.130832      0.133913
  154  11  12                     11->12    719.198      0.724        [0. 1. 0. 0.]      0.156588      0.158705
  155  11  13                 11->10->13    942.247      0.946        [0. 1. 0. 0.]      0.309950      0.315924
  156  12   0                12->5->2->0    621.090      0.616        [0. 1. 0. 0.]      0.452672      0.446854
  157  12   1                12->5->2->1    875.735      0.871        [0. 1. 0. 0.]      0.469501      0.465042
  158  12   2                   12->5->2    442.380      0.447        [0. 1. 0. 0.]      0.336794      0.336948
  159  12   3                12->9->8->3   1056.900      1.057        [0. 1. 0. 0.]      3.136400      3.129041
  160  12   4                   12->5->4    264.988      0.263        [0. 1. 0. 0.]      0.344352      0.341302
  161  12   5                      12->5    444.746      0.446        [0. 1. 0. 0.]      0.289225      0.289572
  162  12   6                12->5->4->6    952.743      0.952        [0. 1. 0. 0.]      0.461359      0.457620
  163  12   7               12->9->10->7    662.672      0.661        [0. 1. 0. 0.]      0.504470      0.507941
  164  12   8                   12->9->8    337.426      0.339        [0. 1. 0. 0.]      3.095250      3.097031
  165  12   9                      12->9    317.721      0.318        [0. 1. 0. 0.]      0.196881      0.199807
  166  12  10                  12->9->10    537.669      0.537        [0. 1. 0. 0.]      0.448942      0.450417
  167  12  11                     12->11    166.636      0.163        [0. 1. 0. 0.]      0.113333      0.111248
  168  12  13                  12->5->13    984.699      0.989        [0. 1. 0. 0.]      0.489346      0.487785
  169  13   0                13->5->2->0    376.603      0.378        [0. 1. 0. 0.]      0.305886      0.307355
  170  13   1               13->10->7->1    762.435      0.763        [0. 1. 0. 0.]      0.578084      0.572982
  171  13   2                   13->5->2    180.005      0.180        [0. 1. 0. 0.]      0.189939      0.193358
  172  13   3                13->5->4->3    467.520      0.464        [0. 1. 0. 0.]      0.238861      0.239718
  173  13   4                   13->5->4    908.112      0.907        [0. 1. 0. 0.]      0.180093      0.182189
  174  13   5                      13->5    147.754      0.147        [0. 1. 0. 0.]      0.142964      0.143560
  175  13   6               13->10->7->6    137.677      0.138        [0. 1. 0. 0.]      0.530645      0.529392
  176  13   7                  13->10->7   1031.620      1.031        [0. 1. 0. 0.]      0.337509      0.338841
  177  13   8               13->10->9->8    941.845      0.937        [0. 1. 0. 0.]      3.383690      3.370234
  178  13   9                  13->10->9    200.906      0.201        [0. 1. 0. 0.]      0.508634      0.505230
  179  13  10                     13->10    794.132      0.795        [0. 1. 0. 0.]      0.279757      0.280400
  180  13  11                 13->10->11   1131.250      1.136        [0. 1. 0. 0.]      0.408325      0.409472
  181  13  12                  13->5->12    376.972      0.379        [0. 1. 0. 0.]      0.678045      0.622645
